{"question": "\nI am trying to examine the effect of weights on the performance of MWPM decoder on 5-repetition code as a simple example from Pymatching2. The idea is to vary the input weights by changing the error probability which while fixing the real error probability producing the syndrome:\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport pymatching\nimport matplotlib.pyplot as plt\n\nH = csc_matrix([[1, 1, 0, 0, 0],\n                [0, 1, 1, 0, 0],\n                [0, 0, 1, 1, 0],\n                [0, 0, 0, 1, 1]]) #Parity check matrix\nobservables = csc_matrix([[1, 0, 0, 0, 0]]) #logical error check matrix\n\n\n\n\nnoise_propability=np.array([0.09,0.1,0.05,0.1,0.11]) #real errors producing the syndromes\n\n\nerror_probability = 0.3 #assumption of error rates (i.e. weights)\nweights = np.ones(H.shape[1]) * np.log((1-error_probability)/error_probability)\nmatching = pymatching.Matching.from_check_matrix(H, weights=weights)\nnum_shots = 100000\nnum_errors = 0\nfor i in range(num_shots):\n    noise = (np.random.random(H.shape[1]) < noise_propability).astype(np.uint8)\n    syndrome = H@noise % 2\n    prediction, solution_weight = matching.decode(syndrome, return_weight=True)\n    predicted_observables = observables@prediction % 2\n    actual_observables = observables@noise % 2\n    num_errors += not np.array_equal(predicted_observables, actual_observables)\n\nLogical_error_rates = (num_errors/num_shots) \n\nNow, setting error_probability to 0.3 will yield Logical_error_rates= 0.00608\nwhile setting it equal to noise_probability (i.e. knowing exactly what error is most likely) yields 0.00615\nwhich is not very far away from the previous case! The logical error rate is actually a bit higher here but that's fine due to stochasticity.\nMy questions are:\n\ndoes this mean that input weights have no effect on the decoding process?\nIs there a Bayesian update of weights since we have the option return_weight=True?\n\nThank you!\n", "answers": ["\n\ndoes this mean that input weights have no effect on the decoding process?\n\nInput weights matter a lot. If you set all the weights to the same thing, creating an unweighted decoder, performance is substantially worse. I'm not sure if you'll see that with a d=5 rep code run for a single round, but in relevant cases it matters.\n\nIs there a Bayesian update of weights since we have the option return_weight=True\n\nThe total weight is unfortunately not very useful for predicting if a logical error has occurred. In your specific case of a code capacity rep code it does roughly correspond to the chance of an error, but as soon as you get to more complicated cases that relationship will wash out. In a distance 15 surface code  running for 1000 rounds, injecting 8 errors to create a logical error adds a tiny contribution to the overall weight; much less than the standard deviation of the distribution.\nWhat you instead want to do is to compute the weight of the next best topologically distinct matching (the complementary matching) and compute the difference in those weights. This is the \"complementary gap\". The gap is an excellent predictor. In your rep code case you're basically getting this for free because of the lack of degeneracy in paths errors can take. Here's an example plot I made using pymatching, sampling surface codes and computing the gap and seeing how often things failed conditioned on the gap. You can see it's basically a linear relationship on a log plot meaning it's an excellent predictor, and it predicts well down to extremely small logical error rates:\n\n"], "comments": ["Thanks a lot for the detailed answer! This makes sense. Do you have any idea why we don't have to provide weights for decode_batch using stim or if there is a way to do so?", "@Quantally When you get the pymatching.Matching from a stim circuit, the weights are being computed automatically from the probabilities of the error channels annotated into the circuit. The weights change naturally as a result of changing the strengths of the errors. The easiest way to assert direct control is often to convert the circuit into a detector error model and edit the probabilities in the dem."], "link": "https://quantumcomputing.stackexchange.com//questions/33779/pymatching-will-starting-with-arbitrary-weights-affect-the-mwpm-decoding-proces?r=SearchResults"}