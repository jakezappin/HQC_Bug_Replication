{"0": {"author": "Solomon", "date": "1687688666030", "content": "Hello,\nIn one PennyLane demo (transfer learning) the circuit is as follows:\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    RY_layer(q_input_features)\n\n    # Sequence of trainable variational layers\n    for k in range(q_depth):\n        entangling_layer(n_qubits)\n        RY_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\nIn the context of image classification using a quantum neural network (QNN), doesnt it make more sense to use qml.probs rather than qml.exp_vals for obtaining the outputs of the QNN? I think so because (1). The probabilities obtained from qml.probs represent the likelihood of the quantum system being in each computational basis state which aligns well with the probabilistic nature of classification tasks, where each class is assigned a probability or confidence score. By using qml.probs, I can interpret the QNN\u2019s output as a probability distribution over the different classes.\n2. Softmax activation: The probabilities obtained from qml.probs can be directly used as inputs to a softmax activation function, which is commonly applied in the final layer of a neural network for multi-class classification. The softmax function normalizes the probabilities and ensures they sum up to 1, providing a meaningful representation of class probabilities.\nKindly let me know where I am wrong here,\nThanks.", "link": "https://discuss.pennylane.ai//t/qml-exp-vals-vs-qml-probs/3095/1"}, "1": {"author": "isaacdevlugt", "date": "1687789114289", "content": "Hey @Solomon!\n\ndoesnt it make more sense to use qml.probs rather than qml.exp_vals for obtaining the outputs of the QNN?\n\nGreat question! In principle, both should work so long as the post-processing and loss function are such that the problem (classification) is \u201cproperly\u201d solved. The loss function here is CrossEntropyLoss. If you take a look through the documentation for it, it says:\n\nIt is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general).\n\nThen there\u2019s some math that shows why that\u2019s the case. To summarize, the concern that you raised here is taken care of in the loss function . I think this answers your second-listed question, as well. Let me know if that helps!", "link": "https://discuss.pennylane.ai//t/qml-exp-vals-vs-qml-probs/3095/2"}, "2": {"author": "Solomon", "date": "1687688666030", "content": "Hello,\nIn one PennyLane demo (transfer learning) the circuit is as follows:\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    RY_layer(q_input_features)\n\n    # Sequence of trainable variational layers\n    for k in range(q_depth):\n        entangling_layer(n_qubits)\n        RY_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\nIn the context of image classification using a quantum neural network (QNN), doesnt it make more sense to use qml.probs rather than qml.exp_vals for obtaining the outputs of the QNN? I think so because (1). The probabilities obtained from qml.probs represent the likelihood of the quantum system being in each computational basis state which aligns well with the probabilistic nature of classification tasks, where each class is assigned a probability or confidence score. By using qml.probs, I can interpret the QNN\u2019s output as a probability distribution over the different classes.\n2. Softmax activation: The probabilities obtained from qml.probs can be directly used as inputs to a softmax activation function, which is commonly applied in the final layer of a neural network for multi-class classification. The softmax function normalizes the probabilities and ensures they sum up to 1, providing a meaningful representation of class probabilities.\nKindly let me know where I am wrong here,\nThanks.", "link": "https://discuss.pennylane.ai//t/qml-exp-vals-vs-qml-probs/3095/3"}}