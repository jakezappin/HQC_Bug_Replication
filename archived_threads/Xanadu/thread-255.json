{"0": {"author": "gojiita_ku", "date": "1623144061130", "content": "Hi everyone. I\u2019m working on quantum neural networks for computer vision. I wonder if there is a better or more efficient way to implement quantum version of convolution rather than using a number of for loops like the one demonstrated in this tutorial 2? Josh suggested me to use Dask to parallelize QNode computations (Thanks @josh again! ). However, it seems I did not use it correctly because when I executed the demo codes provided in the documentation 6 of Dask, I found the computation got even much slower. Here is the code I run:\nimport dask\nimport numpy as np\ndef inc(x):\n    return x + 1\ndef double(x):\n    return x * 2\ndef add(x, y):\n    return x + y\ndata = np.random.rand((100))\nt0 = time.time()\noutput = []\nfor x in data:\n    a = dask.delayed(inc)(x)\n    b = dask.delayed(double)(x)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\ntotal = dask.delayed(sum)(output)\ntotal.compute() \nt1 = time.time()\nprint(\"Time: \", (t1 - t0) )\nt0 = time.time()\noutput = []\nfor x in data:\n    a = (inc)(x)\n    b = (double)(x)\n    c = (add)(a, b)\n    output.append(c)\ntotal = (sum)(output)\nt1 = time.time()\nprint(\"Time: \", (t1 - t0) )\nTime:  0.034467220306396484\nTime:  0.0003325939178466797\n\nCould anyone help me with it? Many thanks!", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/1"}, "1": {"author": "antalszava", "date": "1623202487886", "content": "Hi @gojiita_ku,\nWelcome! \nThe QNodeCollection objects could be helpful in parallelizing QNode evaluations. We have Dask support for a QNodeCollection by specifying the parallel=True option (see the Asynchronous evaluation in the related docs 13).\nIt is worth noting, however, that as the documentation mentions, this option will only be useful with some devices (e.g., can be beneficial with the QVM device from the Forest plugin, as used in this tutorial 5).\nLet us know how this goes! 2 Replies", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/2"}, "2": {"author": "gojiita_ku", "date": "1623206591911", "content": "Hi @antalszava,\nThanks for your answer!  I\u2019ll try QNodeCollection for my case and see if it would work.\nI\u2019m very excited to see Pennylane has been updated so quickly! I\u2019m currently only using Pennylane for QML.1", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/3"}, "3": {"author": "gojiita_ku", "date": "1623222502096", "content": "Hi @antalszava,\nI  checked the documentation 2 of QNodeCollection. It seems like all QNodes within a QNodeCollection  must have the same input.  But if we use QNodeCollection in the case of quantum convolution, all QNodes would have different inputs (e.g. different patches in the two-dimensional image). So that means QNodeCollection would not work for this case?  I\u2019m not sure if I got a correct interpretation of the class QNodeCollection.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/4"}, "4": {"author": "antalszava", "date": "1623245917868", "content": "Hi @gojiita_ku,\nIndeed, that\u2019s right. A QNodeCollection assumes that the inputs are the same for all the QNodes.\n\nCould potentially all QNode inputs be concatenated into a single object, which is than passed to the QNodeCollection? Indexing into the input could then be a way for distributing the input parameters. Something along the lines of\n\nimport pennylane as qml\nimport numpy as np\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ1(par):\n    qml.RY(par[0], wires=0)  # <---- 1. parameter\n    return qml.expval(qml.PauliZ(0))\n\n@qml.qnode(dev)\ndef circ2(par):\n    qml.RX(par[1], wires=0)  # <---- 2. parameter\n    return qml.expval(qml.PauliZ(0))\n\nqnodes = qml.QNodeCollection([circ1, circ2])\n\npar1 = np.array([0.1234])\npar2 = np.array([0.4323])\n\npars = np.concatenate([par1, par2]) # <---- Concatenating input parameters\n\nqnodes(pars)\n\nThere is a historic reason for why a QNodeCollection behaves like this, mainly circuits with similar structures were considered when creating it.\n\nThe Dask logic in QNodeCollection 4 is as follows:\n\nfor q in self.qnodes:\n    results.append(dask.delayed(q)(*args, **kwargs))\n\nreturn dask.compute(*results, scheduler=_scheduler)\n\nHere, _scheduler is a Dask scheduler to use, can be for example \"threads\".\nAlternatively, this could also help with a custom solution to using Dask with multiple qnodes. The input parameters for each qnode could then be zipped with the qnode itself (just an idea for a potential approach):\nfor args, q in zip(arg_lists, qnodes):\n    results.append(dask.delayed(q)(*args, **kwargs))\n\nwhere arg_lists would be an ordered list of arguments for each qnode and qnodes are the QNodes to evaluate.\nHope some of this is helpful, let us know how it goes! ", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/5"}, "5": {"author": "gojiita_ku", "date": "1623247632850", "content": "Hi @antalszava,\nThank you so much! I\u2019ll try both of your suggestions and get back to you soon. ", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/6"}, "6": {"author": "antalszava", "date": "1623277713209", "content": "Sounds good! 1", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/7"}, "7": {"author": "gojiita_ku", "date": "1625478606416", "content": "Hi @antalszava,\nI\u2019m so sorry for getting back to you so late\u2026 I was quite busy with my work for the past few weeks .\nI tried the first approach you suggested and here is the code\ndef test(simulator,parallel):\n    dev = qml.device(simulator, wires=2)\n    t0 = time.time()\n    pars = np.random.rand(128)\n    qnodes = qml.QNodeCollection()\n    for i in range(128):\n        @qml.qnode(dev,interface='torch')\n        def circ(inputs,weights,i=i):\n            qml.RY(weights, wires=0)\n            qml.RY(inputs[i], wires=0)  \n            return qml.expval(qml.PauliZ(0))\n        qnodes.append(circ)\n\n    res = qnodes(pars,0.1,parallel=parallel)\n    t1 = time.time()\n    print(\"Running time\", (t1 - t0) \n\n\n\n311966\u00d7300 29.4 KB\n\nI was a bit confused by the result as the running time is even higher when I set parallel = True. Also,  it is recommended by the documentation to use external simulator for asynchronous mode. So I used qiskit simulator like qiskit.aer, but received an error shown below:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-11-76b5e0198504> in <module>\n----> 1 test('qiskit.aer',False)\n\n<ipython-input-8-be9b5cfd8a9d> in test(simulator, parallel)\n      1 def test(simulator,parallel):\n----> 2     dev = qml.device(simulator, wires=2)\n      3     t0 = time.time()\n      4     pars = np.random.rand(128)\n      5     qnodes = qml.QNodeCollection()\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/__init__.py in device(name, *args, **kwargs)\n    246 \n    247         # loads the device class\n--> 248         plugin_device_class = plugin_devices[name].load()\n    249 \n    250         if Version(version()) not in Spec(plugin_device_class.pennylane_requires):\n\n~/anaconda3/lib/python3.7/site-packages/pkg_resources/__init__.py in load(self, require, *args, **kwargs)\n   2432         if require:\n   2433             self.require(*args, **kwargs)\n-> 2434         return self.resolve()\n   2435 \n   2436     def resolve(self):\n\n~/anaconda3/lib/python3.7/site-packages/pkg_resources/__init__.py in resolve(self)\n   2438         Resolve the entry point from its module and attrs.\n   2439         \"\"\"\n-> 2440         module = __import__(self.module_name, fromlist=['__name__'], level=0)\n   2441         try:\n   2442             return functools.reduce(getattr, self.attrs, module)\n\n~/anaconda3/lib/python3.7/site-packages/pennylane_qiskit/__init__.py in <module>\n     17 from .aer import AerDevice\n     18 from .basic_aer import BasicAerDevice\n---> 19 from .converter import load, load_qasm, load_qasm_from_file\n     20 from .ibmq import IBMQDevice\n\n~/anaconda3/lib/python3.7/site-packages/pennylane_qiskit/converter.py in <module>\n     33 \n     34 \n---> 35 def _check_parameter_bound(param: Parameter, var_ref_map: Dict[Parameter, qml.variable.Variable]):\n     36     \"\"\"Utility function determining if a certain parameter in a QuantumCircuit has\n     37     been bound.\n\nAttributeError: module 'pennylane' has no attribute 'variable'\n\nCould you please help me with these problems? Many thanks!", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/8"}, "8": {"author": "antalszava", "date": "1625492895349", "content": "Hi @gojiita_ku,\nNo worries, hope things are going well! \nNice! Indeed, for default.qubit setting parallel=True will likely not have the desired effect. We\u2019ve experienced better performance in particular with the QVM device and welcome all insight on further observations! \nAs for the error, could you make sure that you have the latest released version of both PennyLane and PennyLane-Qiskit? Both should be version 0.16.0. The version number can be checked by looking at the output of qml.about() and the packages can be upgraded by e.g., pip install pennylane --upgrade.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/9"}, "9": {"author": "gojiita_ku", "date": "1625562818512", "content": "Hi @antalszava,\nThanks for your reply!\nFollowing your advice, I upgraded PennyLane-Qiskit to version 0.16.0. Now my code works for qiskit simulator. Thanks for that! But the running time is  still higher when I set  parallel = True. Then I run the code on the QVM device, as I remember you mentioned you\u2019ve experienced better performance in particular with the QVM device. But I receive an error when I set  parallel = True which I could not understand\u2026 Here is the code:\ndef test(simulator,parallel):\n    dev = qml.device(simulator,device='4q-pyqvm')\n    t0 = time.time()\n    pars = np.random.rand(12)\n    qnodes = qml.QNodeCollection()\n    for i in range(12):\n        @qml.qnode(dev,interface='torch')\n        def circ(inputs,weights,i=i):\n            qml.RY(weights, wires=0)\n            qml.RY(inputs[i], wires=0)  \n            return qml.expval(qml.PauliZ(0))\n        qnodes.append(circ)\n\n    res = qnodes(pars,0.1,parallel=parallel)\n    t1 = time.time()\n    print(\"Running time\", (t1 - t0) )\ntest('forest.qvm',True)\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-4-996259bf4fe9> in <module>\n----> 1 test('forest.qvm',True)\n\n<ipython-input-2-de01ad33e3df> in test(simulator, parallel)\n     12         qnodes.append(circ)\n     13 \n---> 14     res = qnodes(pars,0.1,parallel=parallel)\n     15     t1 = time.time()\n     16     print(\"Running time\", (t1 - t0) )\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/collections/qnode_collection.py in __call__(self, *args, **kwargs)\n    274 \n    275     def __call__(self, *args, **kwargs):\n--> 276         results = self.evaluate(args, kwargs)\n    277         return self.convert_results(results, self.interface)\n    278 \n\n~/anaconda3/lib/python3.7/site-packages/pennylane/collections/qnode_collection.py in evaluate(self, args, kwargs)\n    227                 results.append(dask.delayed(q)(*args, **kwargs))\n    228 \n--> 229             return dask.compute(*results, scheduler=_scheduler)\n    230 \n    231         for q in self.qnodes:\n\n~/anaconda3/lib/python3.7/site-packages/dask/base.py in compute(*args, **kwargs)\n    395     keys = [x.__dask_keys__() for x in collections]\n    396     postcomputes = [x.__dask_postcompute__() for x in collections]\n--> 397     results = schedule(dsk, keys, **kwargs)\n    398     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n    399 \n\n~/anaconda3/lib/python3.7/site-packages/dask/threaded.py in get(dsk, result, cache, num_workers, **kwargs)\n     74     results = get_async(pool.apply_async, len(pool._pool), dsk, result,\n     75                         cache=cache, get_id=_thread_get_id,\n---> 76                         pack_exception=pack_exception, **kwargs)\n     77 \n     78     # Cleanup pools associated to dead threads\n\n~/anaconda3/lib/python3.7/site-packages/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\n    499                         _execute_task(task, data)  # Re-execute locally\n    500                     else:\n--> 501                         raise_exception(exc, tb)\n    502                 res, worker_id = loads(res_info)\n    503                 state['cache'][key] = res\n\n~/anaconda3/lib/python3.7/site-packages/dask/compatibility.py in reraise(exc, tb)\n    110         if exc.__traceback__ is not tb:\n    111             raise exc.with_traceback(tb)\n--> 112         raise exc\n    113 \n    114 else:\n\n~/anaconda3/lib/python3.7/site-packages/dask/local.py in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\n    270     try:\n    271         task, data = loads(task_info)\n--> 272         result = _execute_task(task, data)\n    273         id = get_id()\n    274         result = dumps((result, id))\n\n~/anaconda3/lib/python3.7/site-packages/dask/local.py in _execute_task(arg, cache, dsk)\n    251         func, args = arg[0], arg[1:]\n    252         args2 = [_execute_task(a, cache) for a in args]\n--> 253         return func(*args2)\n    254     elif not ishashable(arg):\n    255         return arg\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/qnode.py in __call__(self, *args, **kwargs)\n    596 \n    597         # execute the tape\n--> 598         res = self.qtape.execute(device=self.device)\n    599 \n    600         # if shots was changed\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/tape/tape.py in execute(self, device, params)\n   1321             params = self.get_parameters()\n   1322 \n-> 1323         return self._execute(params, device=device)\n   1324 \n   1325     def execute_device(self, params, device):\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/interfaces/torch.py in _execute(self, params, **kwargs)\n    266     def _execute(self, params, **kwargs):\n    267         kwargs[\"tape\"] = self\n--> 268         res = _TorchInterface.apply(kwargs, *params)\n    269         return res\n    270 \n\n~/anaconda3/lib/python3.7/site-packages/pennylane/interfaces/torch.py in forward(ctx, input_kwargs, *input_)\n     71         # evaluate the tape\n     72         tape.set_parameters(ctx.all_params_unwrapped, trainable_only=False)\n---> 73         res = tape.execute_device(ctx.args, device)\n     74         tape.set_parameters(ctx.all_params, trainable_only=False)\n     75 \n\n~/anaconda3/lib/python3.7/site-packages/pennylane/tape/tape.py in execute_device(self, params, device)\n   1352 \n   1353         if isinstance(device, qml.QubitDevice):\n-> 1354             res = device.execute(self)\n   1355         else:\n   1356             res = device.execute(self.operations, self.observables, {})\n\n~/anaconda3/lib/python3.7/site-packages/pennylane_forest/qvm.py in execute(self, circuit, **kwargs)\n    156             self._circuit_hash = circuit.graph.hash\n    157 \n--> 158         return super().execute(circuit, **kwargs)\n    159 \n    160     def apply(self, operations, **kwargs):\n\n~/anaconda3/lib/python3.7/site-packages/pennylane/_qubit_device.py in execute(self, circuit, **kwargs)\n    194         # generate computational basis samples\n    195         if self.shots is not None or circuit.is_sampled:\n--> 196             self._samples = self.generate_samples()\n    197 \n    198         multiple_sampled_jobs = circuit.is_sampled and self._has_partitioned_shots()\n\n~/anaconda3/lib/python3.7/site-packages/pennylane_forest/qvm.py in generate_samples(self)\n    230     def generate_samples(self):\n    231         if \"pyqvm\" in self.qc.name:\n--> 232             return self.qc.run(self.prog, memory_map=self._parameter_map)\n    233 \n    234         if self.circuit_hash is None:\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/api/_error_reporting.py in wrapper(*args, **kwargs)\n    249             global_error_context.log[key] = pre_entry\n    250 \n--> 251         val = func(*args, **kwargs)\n    252 \n    253         # poke the return value of that call in\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/api/_quantum_computer.py in run(self, executable, memory_map)\n    136             for region_name, values_list in memory_map.items():\n    137                 self.qam.write_memory(region_name=region_name, value=values_list)\n--> 138         return self.qam.run().wait().read_memory(region_name=\"ro\")\n    139 \n    140     @_record_call\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/pyqvm.py in run(self)\n    265         for _ in range(self.program.num_shots):\n    266             self.wf_simulator.reset()\n--> 267             self._execute_program()\n    268             for name in self.ram.keys():\n    269                 self._memory_results.setdefault(name, list())\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/pyqvm.py in _execute_program(self)\n    482         halted = len(self.program) == 0\n    483         while not halted:\n--> 484             halted = self.transition()\n    485 \n    486         return self\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/pyqvm.py in transition(self)\n    311         \"\"\"\n    312         assert self.program is not None\n--> 313         instruction = self.program[self.program_counter]\n    314 \n    315         if isinstance(instruction, Gate):\n\n~/anaconda3/lib/python3.7/site-packages/pyquil/quil.py in __getitem__(self, index)\n    893             Program(self.instructions[index])\n    894             if isinstance(index, slice)\n--> 895             else self.instructions[index]\n    896         )\n    897 \n\nIndexError: list index out of range\n\n", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/10"}, "10": {"author": "josh", "date": "1625564779326", "content": "Hi @gojiita_ku, from the following part of the error message,\n~/anaconda3/lib/python3.7/site-packages/pyquil/pyqvm.py in transition(self)\n\nit looks like you are using the pyQVM (that is, the Python implementation of the QVM). In the past, we saw the best parallel speedup with the standard QVM - this is a simulator that is written in Lisp, and runs in a separate process to the Python program.\nPerhaps you could try running it again, this time using the QVM? The following demo shows how to do this:\n\n\npennylane.ai\n\n\n\nPage not found \u2014 PennyLane\nA Python library for quantum machine learning, automatic differentiation, and optimization of hybrid quantum-classical computations. Use multiple hardware devices, alongside TensorFlow or PyTorch, in a single computation.\n\n\n\n\n\n", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/11"}, "11": {"author": "gojiita_ku", "date": "1625585900514", "content": "Hi @josh,\nThank you for your reply!\nYes,  you are right. I was using the device 4q-pyqvm . So I walked through the demo you recommended and switched to the device 4q-qvm . I also downloaded and installed the Forest SDK . I set up local server for the QVM and quilc on my laptop following the documentation. But after I run the following code on the terminal:\nimport pennylane as qml\nimport numpy as np\nimport time\nimport torch\n\n\ndef test(simulator,parallel):\n    dev = qml.device(simulator,device='4q-qvm')\n    t0 = time.time()\n    pars = np.random.rand(12)\n    qnodes = qml.QNodeCollection()\n    for i in range(12):\n        @qml.qnode(dev,interface='torch')\n        def circ(inputs,weights,i=i):\n            qml.RY(weights, wires=0)\n            qml.RY(inputs[i], wires=0)\n            return qml.expval(qml.PauliZ(0))\n        qnodes.append(circ)\n\n    res = qnodes(pars,0.1,parallel=parallel)\n    t1 = time.time()\n    print(\"Running time\", (t1 - t0) )\n\n\ntest('forest.qvm',True)\n\n\nI received an error:\nSegmentation fault: 11\n\nWas I missing anything important?\nAlso, I\u2019m wondering that even if we can get a speed up on the QVM device in the parallel mode, the running time would be still higher than the one on the default.qubit simulator. I noticed that running the same code in the sequential mode takes only 0.039s on  the default.qubit simulator while taking 1.479s on the  QVM device\u2026\nActually, my goal is to train a hybrid quantum-classical neural network which involves executing  B\\times128 \\times128   circuits where B is batch size (e.g. 4).  Both forward and backward pass need to be computed for this large number of circuits. So I really want to know if we could reduce the running time to a level which is acceptable for production by employing the QNodeCollection method (particularly in the parallel mode)?", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/12"}, "12": {"author": "josh", "date": "1625649667726", "content": "Hi @gojiita_ku,\n\nWas I missing anything important?\n\nFrom what I can see, your code looks good! Which makes the segmentation fault more puzzling \nUnfortunately, the segmentation fault indicates it might be a memory issue in the QVM itself? The only thing I can suggest is perhaps opening up an issue on the QVM GitHub issue.1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/13"}, "13": {"author": "josh", "date": "1625650098292", "content": "\nI noticed that running the same code in the sequential mode takes only 0.039s on the default.qubit simulator while taking 1.479s on the QVM device\u2026\n\nOne thing to keep in mind is that the supported methods of computing quantum gradients can differ between devices, which can affect total optimization time \nFor example, default.qubit defaults to backpropagation, which adds some overhead to the forwards pass, but the backwards pass can be done in constant time.\nIn contrast, when using an external device like the QVM, only parameter-shift is supported. This means that the forward pass, while faster than default.qubit, the backwards pass may be slower, since it scales with the number of parameters in the circuit!\nYou can check out the backprop demo for more details if you are interested!1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/14"}, "14": {"author": "gojiita_ku", "date": "1625754616888", "content": "Hi @josh,\n\nUnfortunately, the segmentation fault indicates it might be a memory issue in the QVM itself? The only thing I can suggest is perhaps opening up an issue on the QVM GitHub issue.\n\nOk. I will open up an issue on the QVM GitHub issue.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/15"}, "15": {"author": "gojiita_ku", "date": "1625756137973", "content": "\nFor example,  default.qubit  defaults to backpropagation, which adds some overhead to the forwards pass, but the backwards pass can be done in constant time.\nIn contrast, when using an external device like the QVM, only parameter-shift is supported. This means that the forward pass, while faster than  default.qubit , the backwards pass may be  slower , since it scales with the number of parameters in the circuit!\n\nI got what you mean. I need to consider running times for both forward and backwards pass. Thanks for your explanation! But what I want to show is that the default.qubit  has much better performance in terms of running time than the QVM device even just for the forward pass. It takes only  0.039s  to run the above code (which only computes the forward pass) on the  default.qubit  simulator while taking  1.479s  on the QVM device. As you mentioned, the backwards pass may be slower on the QVM device when it scales with the number of parameters in the circuit  since only parameter-shift is supported. So I wonder if the  default.qubit would be the optimal choice for optimizing the total computation time.\nI got another question. Could QNodeCollection  objects be also helpful in parallelizing QNode backwards pass evaluations (e.g. calculation of parameterized circuits gradients) ?", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/16"}, "16": {"author": "josh", "date": "1625760254862", "content": "\nIt takes only 0.039s to run the above code (which only computes the forward pass) on the default.qubit simulator while taking 1.479s on the QVM device.\n\nThis is really interesting! Unfortunately I can\u2019t comment too much, since I am unaware as to how QVM works internally. It could be the case that it has more optimal performance at higher qubit number, or greater depth circuits, for example.\n\nI got another question. Could QNodeCollection objects be also helpful in parallelizing QNode backwards pass evaluations (e.g. calculation of parameterized circuits gradients) ?\n\nUnfortunately not \u2013 QNodeCollection only parallelizes multiple forward passes.\nHowever, we recently added support for PennyLane devices to perform batch execution. At the moment, one device supports it \u2014 the PennyLane-Braket device. It uses this batch execution capability to batch execute all the parameter-shift gradients at once.\nFor more info, there is a nice write up in this demonstration: Computing gradients in parallel with Amazon Braket | PennyLane Demos 21", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/17"}, "17": {"author": "gojiita_ku", "date": "1626014879759", "content": "Hi @josh,\n\n\n\n josh:\n\nHowever, we recently added support for PennyLane devices to perform batch execution. At the moment, one device supports it \u2014 the PennyLane-Braket device. It uses this batch execution capability to batch execute all the parameter-shift gradients at once.\n\n\nThanks for your advice! Actually, I participated in QHack 2021 and used AWS credits to get access to AWS bracket service. I trained a hybrid quantum-classical model using SV1 simulator at the parallel model but found the training was much slower than the local simulator (e.g. default.quibit). I found later from the documentation that  the remote device has more advantage at computing large circuits rather than small circuits, considering the transition time between the local user and the remote server. In my case, the circuit to compute is always very small (e.g. circuit with four qubits). So I\u2019m guessing maybe I had better use local simulators even though they do not support batch execution .\nAnyway, I think I should share an example code I\u2019m currently working on so that you or anyone else could give me further supervision on how to reduce the running time .1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/18"}, "18": {"author": "josh", "date": "1626061736667", "content": "\n\n\n gojiita_ku:\n\nAnyway, I think I should share an example code I\u2019m currently working on so that you or anyone else could give me further supervision on how to reduce the running time .\n\n\nNo worries @gojiita_ku! Yes, that would definitely be helpful.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/19"}, "19": {"author": "gojiita_ku", "date": "1628050888235", "content": "Hi, @josh @antalszava\nSorry for my late reply!\nBasically, I am working on an image segmentation task based on quantum convolution. The original model is very complicated,  so I show you only the code for the quantum part which I feel most difficult to optimize . The function of the code is to perform a quantum version of  1\\times1  convolution on a feature map with the shape of (batch size, channel, height, width),  namely each value  from each channel at the same spatial position is fed into a quantum circuit every time. In my case, the input feature map should be in the shape of (batch size, 4, 128, 128) where the value of batch size can be flexible. I run the code with batch size of 2 and it took about 15  minutes in my laptop. The size of dataset in my task is very large, so I guess it would take a very very long time to train the model\u2026 So could you please help me with this code? Many thanks!\nimport pennylane as qml\nimport torch\nimport torch.nn as nn\nimport time\n\nn_qubits = 4\ndev = qml.device('default.qubit', wires=n_qubits)\n\nclass QuanvLayer(nn.Module):\n    def __init__(self, batch_size, height, width, channel):\n        super(QuanvLayer, self).__init__()\n        self.q_params = nn.Parameter(torch.randn(2, channel))\n        qnodes = qml.QNodeCollection()\n        for i in range(batch_size*height*width):\n            @qml.qnode(dev, interface=\"torch\",method='adjoint',mutable=False)\n            def circuit(inputs,weights,i=i):\n    \n                for j in range(channel):\n                    qml.RY(inputs[i][j], wires=j)\n         \n                return [qml.expval(qml.PauliZ(wires=j)) for j in range(4)]\n            qnodes.append(circuit)\n\n        self.qnodes = qnodes\n\n\n    def forward(self, x):\n        b,c,h,w = x.shape\n        x = x.permute(1,0,2,3)  # (c,b,h,w)\n        x = torch.flatten(x,start_dim=1) # (c,b*h*w)\n        x = x.permute(1,0) # (b*h*w,c)\n\n        x = self.qnodes(x,self.q_params,parallel=True)  #(b*h*w,c)\n\n        x = x.reshape((b,h,w,c)) # (b,h,w,c)\n\n        x = x.permute(0,3,2,1) # (b,c,w,h)\n        \n        x = x.permute(0,1,3,2) # (b,c,h,w)\n        \n        return x\n\n\n\nbatch_size = 2\nheight = 128\nwidth = 128\nchannel = 4\nt0 = time.time()\nx = torch.randn(batch_size, channel,height,width)\nx = torch.tensor(x,requires_grad=True)\nloss = QuanvLayer(batch_size,height,width, channel)(x).sum()\nloss.backward()\nt1 = time.time()\nprint(t1-t0)\n                                                  \n1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/20"}, "20": {"author": "gojiita_ku", "date": "1625585900514", "content": "Hi @josh,\nThank you for your reply!\nYes,  you are right. I was using the device 4q-pyqvm . So I walked through the demo you recommended and switched to the device 4q-qvm . I also downloaded and installed the Forest SDK . I set up local server for the QVM and quilc on my laptop following the documentation. But after I run the following code on the terminal:\nimport pennylane as qml\nimport numpy as np\nimport time\nimport torch\n\n\ndef test(simulator,parallel):\n    dev = qml.device(simulator,device='4q-qvm')\n    t0 = time.time()\n    pars = np.random.rand(12)\n    qnodes = qml.QNodeCollection()\n    for i in range(12):\n        @qml.qnode(dev,interface='torch')\n        def circ(inputs,weights,i=i):\n            qml.RY(weights, wires=0)\n            qml.RY(inputs[i], wires=0)\n            return qml.expval(qml.PauliZ(0))\n        qnodes.append(circ)\n\n    res = qnodes(pars,0.1,parallel=parallel)\n    t1 = time.time()\n    print(\"Running time\", (t1 - t0) )\n\n\ntest('forest.qvm',True)\n\n\nI received an error:\nSegmentation fault: 11\n\nWas I missing anything important?\nAlso, I\u2019m wondering that even if we can get a speed up on the QVM device in the parallel mode, the running time would be still higher than the one on the default.qubit simulator. I noticed that running the same code in the sequential mode takes only 0.039s on  the default.qubit simulator while taking 1.479s on the  QVM device\u2026\nActually, my goal is to train a hybrid quantum-classical neural network which involves executing  B\\times128 \\times128 B\u00d7128\u00d7128  circuits where BB is batch size (e.g. 4).  Both forward and backward pass need to be computed for this large number of circuits. So I really want to know if we could reduce the running time to a level which is acceptable for production by employing the QNodeCollection method (particularly in the parallel mode)?", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/21"}, "21": {"author": "gojiita_ku", "date": "1628050888235", "content": "Hi, @josh @antalszava\nSorry for my late reply!\nBasically, I am working on an image segmentation task based on quantum convolution. The original model is very complicated,  so I show you only the code for the quantum part which I feel most difficult to optimize . The function of the code is to perform a quantum version of  1\\times11\u00d71  convolution on a feature map with the shape of (batch size, channel, height, width),  namely each value  from each channel at the same spatial position is fed into a quantum circuit every time. In my case, the input feature map should be in the shape of (batch size, 4, 128, 128) where the value of batch size can be flexible. I run the code with batch size of 2 and it took about 15  minutes in my laptop. The size of dataset in my task is very large, so I guess it would take a very very long time to train the model\u2026 So could you please help me with this code? Many thanks!\nimport pennylane as qml\nimport torch\nimport torch.nn as nn\nimport time\n\nn_qubits = 4\ndev = qml.device('default.qubit', wires=n_qubits)\n\nclass QuanvLayer(nn.Module):\n    def __init__(self, batch_size, height, width, channel):\n        super(QuanvLayer, self).__init__()\n        self.q_params = nn.Parameter(torch.randn(2, channel))\n        qnodes = qml.QNodeCollection()\n        for i in range(batch_size*height*width):\n            @qml.qnode(dev, interface=\"torch\",method='adjoint',mutable=False)\n            def circuit(inputs,weights,i=i):\n    \n                for j in range(channel):\n                    qml.RY(inputs[i][j], wires=j)\n         \n                return [qml.expval(qml.PauliZ(wires=j)) for j in range(4)]\n            qnodes.append(circuit)\n\n        self.qnodes = qnodes\n\n\n    def forward(self, x):\n        b,c,h,w = x.shape\n        x = x.permute(1,0,2,3)  # (c,b,h,w)\n        x = torch.flatten(x,start_dim=1) # (c,b*h*w)\n        x = x.permute(1,0) # (b*h*w,c)\n\n        x = self.qnodes(x,self.q_params,parallel=True)  #(b*h*w,c)\n\n        x = x.reshape((b,h,w,c)) # (b,h,w,c)\n\n        x = x.permute(0,3,2,1) # (b,c,w,h)\n        \n        x = x.permute(0,1,3,2) # (b,c,h,w)\n        \n        return x\n\n\n\nbatch_size = 2\nheight = 128\nwidth = 128\nchannel = 4\nt0 = time.time()\nx = torch.randn(batch_size, channel,height,width)\nx = torch.tensor(x,requires_grad=True)\nloss = QuanvLayer(batch_size,height,width, channel)(x).sum()\nloss.backward()\nt1 = time.time()\nprint(t1-t0)\n                                                  \n1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/22"}, "22": {"author": "antalszava", "date": "1628081641011", "content": "Hi @gojiita_ku,\nNo worries!\nIt seems, that in this specific case the following could be adjusted:\n@qml.qnode(dev, interface=\"torch\",diff_method='adjoint',mutable=False).\nI.e., we\u2019d like to have diff_method='adjoint' be passed to qml.qnode, instead of method='adjoint'.\nIf we\u2019re not passing diff_method='adjoint', then the unknown keyword is ignored and we\u2019re using the parameter-shift rule with the Torch interface.\nUsing the adjoint method resulted in a runtime of about ~110 seconds for me locally.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/23"}, "23": {"author": "gojiita_ku", "date": "1628152030936", "content": "\nIt seems, that in this specific case the following could be adjusted:\n@qml.qnode(dev, interface=\"torch\",diff_method='adjoint',mutable=False) .\nI.e., we\u2019d like to have  diff_method='adjoint'  be passed to  qml.qnode , instead of  method='adjoint' .\nIf we\u2019re not passing  diff_method='adjoint' , then the unknown keyword is ignored and we\u2019re using the parameter-shift rule with the Torch interface.\nUsing the adjoint method resulted in a runtime of about ~110 seconds for me locally.\n\nHi @antalszava,\nThanks so much for pointing out the error I made! Yes, after replacing method='adjoint' with diff_method='adjoint', the new code also resulted in a runtime of  about ~ 120 seconds in my local environment! This is an obvious speedup!\nHowever, compared to classical image segmentation models,  the quantum part still takes much more time (270s vs 0.16s for batch size of 2 in my case). So may I ask two more questions below?\n\n\nIs there any other method to further reduce the run time of this quantum version of convolution? For example, could we exploit vectorization for pennylane?\n\n\nIf we use device as default.qubit.tf,  intall tensorflow with gpu support and rebuild the model using tensorflow, would this quantum convolution be executed on gpu?\n\n", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/24"}, "24": {"author": "Romain_Moyard", "date": "1628192112417", "content": "Hi @gojiita_ku,\nYour questions are welcome!\n\nI am looking into it and I will come back to you tomorrow.\nYes you are right, you can use default.qubit.tf with diff_method=backprop it supports classical back propagation on GPU.\n1", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/25"}, "25": {"author": "Romain_Moyard", "date": "1628256965019", "content": "Hey @gojiita_ku,\nFor the first question, I spoke with my colleague @jmarrazola  and unfortunately we do no think that there is a straightforward way of further further speeding up the execution of the present code. Indeed at the moment simulating quantum circuits is harder than implementing neural networks.\nFor vectorization techniques it would probably need to happen at the level of how the device operates, which would require changing most of the code base.\nFeel free to ask any other questions!", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/26"}, "26": {"author": "gojiita_ku", "date": "1628520555491", "content": "Hi @Romain_Moyard,\nThanks so much for your reply! I\u2019m asking this question because I would like to make this type of hybrid quantum-classical models leap out of the lab and be more practical in certain real-world scenarios (e.g. medical imaging). Personally, I find the biggest obstacle I am facing is how to optimize the computation time of quantum parts of the model. Anyway, thanks for all of your supports! I believe this issue can be resolved soon in the future with some new features of Pennylane. \nBy the way, may I further confirm one thing? Even though default.qubit.tf supports back-propagation method like TensorFlow, the computation of quantum circuits is still not based on vectorization, right? I mean quantum circuits in this case are still executed sequentially in for loops.", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/27"}, "27": {"author": "jmarrazola", "date": "1628525163231", "content": "Hi @gojiita_ku,\nThat\u2019s right, devices in pennylane don\u2019t perform a vectorized simulation of quantum circuits. Instead, the approach is to create a tape representing the circuit and subsequently apply each operation in the tape to an initial state vector. This follows essentially the same logic required to execute circuits on hardware.\nYour suggestion of a fully-vectorized simulator is intriguing. In principle this should be possible, but would require a large change to how simulators are designed and built. It\u2019s also not clear that the gain would be significant \nLet us know if there\u2019s anything else we can do to help!\nBest,\nJuan Miguel", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/28"}, "28": {"author": "gojiita_ku", "date": "1628568479111", "content": "Hi @jmarrazola,\nThank you so much for confirming my doubts! I\u2019ll let you know if I got more questions about Pennylane. 1", "link": "https://discuss.pennylane.ai//t/how-to-use-dask-to-parallelize-qnode-computations/1101/29"}}