{"0": {"author": "ToMago", "date": "1659098304336", "content": "Hi Pennylane-Community!\nI recently implemented a circuit for a Quantum Autoencoder.\nUsing a classical optimizer like Adam, everything works great, but I would like to use Quantum Natual Gradient Descent and I am running into some problems trying to implement that.\nMy circuit is a QNode, which takes the trainable parameters and some input data as arguments and return an expectation value:\n@qml.qnode(dev1)\ndef circuit(params, data):\n    // data encoding and parametrized circuit ...\n    return qml.expval(qml.PauliZ(TOTAL_QBITS-1))\n\nTo train the model I defined a cost function for a single data sample:\ndef cost(params, single_sample):\n     return (1 - circuit(params, single_sample)) ** 2\n\nNow I would like to iterate over all training samples and optimizes the cost with the QNGoptimizer. All the examples I found had the data for the QNode hard coded into the circuit but of course I want to optimize for a large dataset and I can\u2019t seem to make the QNGoptimizer work with a QNode with two arguments.\nopt = qml.QNGOptimizer(learning_rate)\n\nfor it in range(epochs):\n    for j, sample in enumerate(x_train):\n        metric_fn = qml.metric_tensor(circuit, approx=\"block-diag\")\n        params, _ = opt.step(cost, params, sample,  metric_tensor_fn=metric_fn)\n\n    loss = cost(params)\n\n    print(f\"Epoch: {it} | Loss: {loss} |\")\n\nI checked the output of the metric function metric_fn for some arbitrary input of weights and a data sample and it returns a tuple of two metric tensors, one for the weights and one for the input data. This tuple can\u2019t be used by the opt.step to optimize.\nAny suggestions how I can fix this?\nA second question would be If it is possible to extend this training to batches.\nThanks!\nGreetings\nTom\n\n\n Solved by josh in post #8 \n\n\n                Hi @ToMago, I believe I have a solution, which entails using creating a lambda  function so that cost and metric_fn only accept params as input: \nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev)\ndef circuit(params, data):\n    qml.\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/1"}, "1": {"author": "isaacdevlugt", "date": "1659105544907", "content": "Hey @ToMago! Looking at the documentation for the QNG optimizer 2, it\u2019s tough to say why your code isn\u2019t working without seeing the exact error readout. Can you provide that?", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/2"}, "2": {"author": "ToMago", "date": "1659107211560", "content": "Thanks for your answer!\nOf course I can post the full error trace, however I found it to be a bit misleading:\n\n\nError trace\nTypeError                                 Traceback (most recent call last)\nInput In [72], in <cell line: 3>()\n      6     metric_fn = qml.metric_tensor(circuit, approx=\"block-diag\")\n      7     print(metric_fn(np.array([[1,2,3,4]]),sample))\n----> 8     params, _ = opt.step(cost_sample, params, sample, metric_tensor_fn=metric_fn)\n      9     print(j, end=\"\\r\")\n     11 loss = cost(params)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/optimize/qng.py:269, in QNGOptimizer.step(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\n    245 def step(\n    246     self, qnode, *args, grad_fn=None, recompute_tensor=True, metric_tensor_fn=None, **kwargs\n    247 ):\n    248     \"\"\"Update the parameter array :math:`x` with one step of the optimizer.\n    249 \n    250     Args:\n   (...)\n    267         array: the new variable values :math:`x^{(t+1)}`\n    268     \"\"\"\n--> 269     new_args, _ = self.step_and_cost(\n    270         qnode,\n    271         *args,\n    272         grad_fn=grad_fn,\n    273         recompute_tensor=recompute_tensor,\n    274         metric_tensor_fn=metric_tensor_fn,\n    275         **kwargs,\n    276     )\n    277     return new_args\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/optimize/qng.py:212, in QNGOptimizer.step_and_cost(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\n    210 shape = qml.math.shape(_metric_tensor)\n    211 size = qml.math.prod(shape[: len(shape) // 2])\n--> 212 self.metric_tensor = qml.math.reshape(_metric_tensor, (size, size))\n    213 # Add regularization\n    214 self.metric_tensor = self.metric_tensor + self.lam * qml.math.eye(\n    215     size, like=_metric_tensor\n    216 )\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/autoray/autoray.py:85, in do(fn, like, *args, **kwargs)\n     82 else:\n     83     backend = infer_backend(like)\n---> 85 return get_lib_fn(backend, fn)(*args, **kwargs)\n\nFile <__array_function__ internals>:180, in reshape(*args, **kwargs)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/numpy/core/fromnumeric.py:298, in reshape(a, newshape, order)\n    198 @array_function_dispatch(_reshape_dispatcher)\n    199 def reshape(a, newshape, order='C'):\n    200     \"\"\"\n    201     Gives a new shape to an array without changing its data.\n    202 \n   (...)\n    296            [5, 6]])\n    297     \"\"\"\n--> 298     return _wrapfunc(a, 'reshape', newshape, order=order)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/numpy/core/fromnumeric.py:54, in _wrapfunc(obj, method, *args, **kwds)\n     52 bound = getattr(obj, method, None)\n     53 if bound is None:\n---> 54     return _wrapit(obj, method, *args, **kwds)\n     56 try:\n     57     return bound(*args, **kwds)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43, in _wrapit(obj, method, *args, **kwds)\n     41 except AttributeError:\n     42     wrap = None\n---> 43 result = getattr(asarray(obj), method)(*args, **kwds)\n     44 if wrap:\n     45     if not isinstance(result, mu.ndarray):\n\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\n\n\nIn this case the error is thrown because the metric_fn function returns a tuple of two metric tensors, one for the weights, one for the data and of course the reshaping in the QNG class does not work on a tuple.\nI tried to kind of fix this but never got anywhere, so I suppose the QNG optimizer only works on functions with a single argument?\nBut how to use it in a real application with a larger dataset and a custom loss function?", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/3"}, "3": {"author": "josh", "date": "1659120478545", "content": "Hi @ToMago! When you create the data, have you tried setting requires_grad=False?\ndata = np.array(data, requires_grad=False)\n2", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/4"}, "4": {"author": "ToMago", "date": "1659122416973", "content": "Thanks @josh!\nThis does indeed fix the first issue, when I call metric_fn with\ndata = np.array(data, requires_grad=False)\nit only gives one metric tensor and not a tuple. Yay!\nHowever I still get an error:\n\n\nError Trace\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [74], in <cell line: 3>()\n      6     metric_fn = qml.metric_tensor(circuit, approx=\"block-diag\")\n      7     print(metric_fn(np.array([[1,2,3,4]]),np.array(sample,requires_grad=False)))\n----> 8     params, _ = opt.step(cost_sample, params, np.array(sample,requires_grad=False), metric_tensor_fn=metric_fn)\n      9     print(j, end=\"\\r\")\n     11 loss = cost(params)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/optimize/qng.py:269, in QNGOptimizer.step(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\n    245 def step(\n    246     self, qnode, *args, grad_fn=None, recompute_tensor=True, metric_tensor_fn=None, **kwargs\n    247 ):\n    248     \"\"\"Update the parameter array :math:`x` with one step of the optimizer.\n    249 \n    250     Args:\n   (...)\n    267         array: the new variable values :math:`x^{(t+1)}`\n    268     \"\"\"\n--> 269     new_args, _ = self.step_and_cost(\n    270         qnode,\n    271         *args,\n    272         grad_fn=grad_fn,\n    273         recompute_tensor=recompute_tensor,\n    274         metric_tensor_fn=metric_tensor_fn,\n    275         **kwargs,\n    276     )\n    277     return new_args\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/optimize/qng.py:219, in QNGOptimizer.step_and_cost(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\n    214     self.metric_tensor = self.metric_tensor + self.lam * qml.math.eye(\n    215         size, like=_metric_tensor\n    216     )\n    218 g, forward = self.compute_grad(qnode, args, kwargs, grad_fn=grad_fn)\n--> 219 new_args = np.array(self.apply_grad(g, args), requires_grad=True)\n    221 if forward is None:\n    222     forward = qnode(*args, **kwargs)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/optimize/qng.py:293, in QNGOptimizer.apply_grad(self, grad, args)\n    291 grad_flat = np.array(list(_flatten(grad)))\n    292 x_flat = np.array(list(_flatten(args)))\n--> 293 x_new_flat = x_flat - self.stepsize * np.linalg.solve(self.metric_tensor, grad_flat)\n    294 return unflatten(x_new_flat, args)\n\nFile ~/.conda/envs/tfq/lib/python3.9/site-packages/pennylane/numpy/tensor.py:155, in tensor.__array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n    151 args = [i.unwrap() if hasattr(i, \"unwrap\") else i for i in inputs]\n    153 # call the ndarray.__array_ufunc__ method to compute the result\n    154 # of the vectorized ufunc\n--> 155 res = super().__array_ufunc__(ufunc, method, *args, **kwargs)\n    157 if ufunc.nout == 1:\n    158     res = (res,)\n\nValueError: operands could not be broadcast together with shapes (24,) (20,) \n\n\nI only changed\nparams, _ = opt.step(cost, params, np.array(sample, requires_grad=False),  metric_tensor_fn=metric_fn)\n\nfrom my above code.\nI guess the optimizer still sees the input data as parameters and calculates gradients for them so that the dimension with the metric tensor does not match anymore?\nFor clarity i should mention, the shape of the parameters is (5,4) and the datapoints are 4 dimensional leading to the dimension mismatch of 20 and 24.1", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/5"}, "5": {"author": "isaacdevlugt", "date": "1659129217521", "content": "Glad @josh was able to help you! Is it possible that you need to change the problematic line to\nparams = opt.step(cost, params, np.array(sample, requires_grad=False),  metric_tensor_fn=metric_fn) \nThe function opt.step returns an array (your new parameters), not a tuple.", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/6"}, "6": {"author": "ToMago", "date": "1659167953454", "content": "Yes, that is probably true now that the data has no grad, however that does not resolve the error with the two different dimensions in apply_grad.", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/7"}, "7": {"author": "josh", "date": "1659301323564", "content": "Hi @ToMago, I believe I have a solution, which entails using creating a lambda  function so that cost and metric_fn only accept params as input:\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev)\ndef circuit(params, data):\n    qml.AngleEmbedding(data, wires=[0, 1, 2])\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2])\n    return qml.expval(qml.PauliZ(2))\n\ndata = np.random.random([3], requires_grad=False)\nparams = np.random.random(qml.StronglyEntanglingLayers.shape(3, 3), requires_grad=True)\n\ndef cost(params, single_sample):\n     return (1 - circuit(params, single_sample)) ** 2\n\nopt = qml.QNGOptimizer()\n\nfor it in range(10):\n    cost_fn = lambda p: cost(p, data)\n    metric_fn = lambda p: qml.metric_tensor(circuit, approx=\"block-diag\")(p, data)\n\n    params, loss = opt.step_and_cost(cost_fn, params,  metric_tensor_fn=metric_fn)\n\n    print(f\"Epoch: {it} | Loss: {loss} |\")\n\nEpoch: 0 | Loss: 0.5582732050743116 |\nEpoch: 1 | Loss: 0.41371200375696354 |\nEpoch: 2 | Loss: 0.3163450001396077 |\nEpoch: 3 | Loss: 0.2494155603676695 |\nEpoch: 4 | Loss: 0.2020055226900949 |\nEpoch: 5 | Loss: 0.16737305015180642 |\nEpoch: 6 | Loss: 0.1413480948325721 |\nEpoch: 7 | Loss: 0.12129520018671695 |\nEpoch: 8 | Loss: 0.10550239221136215 |\nEpoch: 9 | Loss: 0.09282592043979121 |\n\nLet me know if that ends up working for you! [Note: for simplicity, I slightly modified your example to remove the minibatches]Solution1", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/8"}, "8": {"author": "ToMago", "date": "1659351220222", "content": "Yes this way it works!\nThanks so much, this is super helpful!\nSo would it be possible to implement a version with minibatches and the QNG optimizer ?\nI\u2019m not sure how this would be done since the Fubiny-Study metric can only be calculated for a single sample of input data, so if I use the update rule for QNG\nw:= w - \\eta g^{-1} \\nabla_w\\mathcal{L}_i(w)\nwhere \\mathcal{L}_i is the loss for a single sample and replace it with \\mathcal{L}_i \\rightarrow \\frac{1}{n}\\sum_{i=0}^n \\mathcal{L}_i, how do I update the weights ?\nSorry, I\u2019m not too familiar with natural gradient descent, do I also average the metric tensors for the different data points?", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/9"}, "9": {"author": "isaacdevlugt", "date": "1659477194369", "content": "@josh Correct me if I\u2019m wrong here, but we\u2019d just need to modify the cost function to accommodate batches as follows:\nbatch_size = 10\ndata = [np.random.random([3], requires_grad=False) for _ in range(batch_size)]\nparams = np.random.random(qml.StronglyEntanglingLayers.shape(3, 3), requires_grad=True)\n\ndef cost(params, batch):\n     return np.sum((1 - circuit(params, single_sample)) ** 2 for single_sample in batch) / batch_size\n\n(this would be for a \u201cmean\u201d reduction method of the cost function). Then, you loop over all of your batches, perform the parameter update in the same way, and that constitutes one \u201citeration\u201d.", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/10"}, "10": {"author": "ToMago", "date": "1659351220222", "content": "Yes this way it works!\nThanks so much, this is super helpful!\nSo would it be possible to implement a version with minibatches and the QNG optimizer ?\nI\u2019m not sure how this would be done since the Fubiny-Study metric can only be calculated for a single sample of input data, so if I use the update rule for QNG\nw:= w - \\eta g^{-1} \\nabla_w\\mathcal{L}_i(w)w:=w\u2212\u03b7g\u22121\u2207wLi(w)\nwhere \\mathcal{L}_iLi is the loss for a single sample and replace it with \\mathcal{L}_i \\rightarrow \\frac{1}{n}\\sum_{i=0}^n \\mathcal{L}_iLi\u21921n\u2211ni=0Li, how do I update the weights ?\nSorry, I\u2019m not too familiar with natural gradient descent, do I also average the metric tensors for the different data points?", "link": "https://discuss.pennylane.ai//t/training-with-qng-optimizer-on-circuit-with-data-argument/2067/11"}}