{"0": {"author": "QuantumMan", "date": "1684642271181", "content": "HI,\nI am trying to scale the number of qubits for my experiments using Quantum GANs \u2014 PennyLane documentation 2 . But when I run the script with num_qubits of more than 5, then the script fails with error -\nTraceback (most recent call last):\nFile \u201cqgan.py\u201d, line 225, in \noutD_fake = discriminator(fake_data.detach()).view(-1)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u201d, line 1501, in _call_impl\nreturn forward_call(*args, **kwargs)\nFile \u201cqgan.py\u201d, line 93, in forward\nreturn self.model(x)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u201d, line 1501, in _call_impl\nreturn forward_call(*args, **kwargs)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u201d, line 217, in forward\ninput = module(input)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u201d, line 1501, in _call_impl\nreturn forward_call(*args, **kwargs)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u201d, line 114, in forward\nreturn F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x128 and 64x64)\nAny thoughts on how we can make the script more generic, and pass the qubits as parameters? Is there any other channel, where we can ask these kind of questions and get quick answers?\nThanks", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/1"}, "1": {"author": "isaacdevlugt", "date": "1684850830267", "content": "Hey @QuantumMan,\nI believe you\u2019re getting an error when you change the number of qubits here because the image sizes are 8 by 8 (8*8 = 64). There are a total of 5+1 qubits being used, which means 2^6 = 64 is the size of the quantum circuit state in the computational basis. The output of the circuit is qml.probs(wires=list(range(n_qubits))) which will output the proper dimension 1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/2"}, "2": {"author": "mass_of_15", "date": "1684857595092", "content": "Hi @QuantumMan,\nSo to generate an image of 64 x 64, I must use 12 qubits (11 data qubits and 1 ancillary qubit). How many qubits can PennyLane support? Also, can I use it to generate RBG images instead of grayscale images?", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/3"}, "3": {"author": "isaacdevlugt", "date": "1684857920596", "content": "Pennylane simulators are able to support up to 32 qubits ", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/4"}, "4": {"author": "mass_of_15", "date": "1684858280425", "content": "Can QGAN generate RBG images instead of grayscale images? Will it be able to handle so many qubits? Is there a way around it?", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/5"}, "5": {"author": "isaacdevlugt", "date": "1684859205329", "content": "Yep! In principle, any generative model that was made for grayscale images can be used for RGB images with suitable adjustments . Usually all this means is that the output of the model is multinomial/continuous instead of binary.", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/6"}, "6": {"author": "QuantumMan", "date": "1684907555366", "content": "\n\n\n isaacdevlugt:\n\nThe output of the circuit\n\n\nThanks @isaacdevlugt  - sorry, i am still newbie to quantum and ML, so what should i be changing in the code to support 10 qubits or scale to 32 qubits? Thanks.", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/7"}, "7": {"author": "isaacdevlugt", "date": "1684939167288", "content": "No worries at all! A GAN, effectively, has two parts:\n\nA generator that generates data similar to what\u2019s in your training dataset but is fake\n\nA discriminator that discerns whether the generator\u2019s fake data is real or not.\n\nWe want to train the discriminator to be good at telling us what is fake or not, and we want to train the generator to make it even better at making fake data seem real.\nIn this demo, the generator is quantum. So, the data that the quantum generator makes must have the same dimensions as the real data. The real data is an 8x8 image (8x8 = 64 units), so the quantum generator must generate 8x8 images, as well.\nThere\u2019s a few layers of abstraction that I\u2019m glossing over because the circuit is using a patching method. But, a quantum circuit can generate 8x8=64-length data by outputting a probability vector that is of length 2^6 = 64, where the \u201c6\u201d is the number of qubits. If you increase the number of qubits, you are generating an image that isn\u2019t the right dimension.\nHope that makes sense!", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/8"}, "8": {"author": "isaacdevlugt", "date": "1684939429985", "content": "\nso what should i be changing in the code to support 10 qubits or scale to 32 qubits?\n\nYou\u2019d have to make some serious changes to the quantum generator for this in order for the dimensions to work out. There\u2019s many ways to make it work, but ultimately anything should (in theory) work so long as your quantum generator can generate a 64-length vector that can be suitably interpreted as an image (e.g., a probability vector).\nI think there\u2019s a more important thing to highlight here in that simply increasing the number of qubits doesn\u2019t always guarantee better performance, which is what I assume you\u2019re after. There\u2019s a lot of nuance and unknowns in machine learning, let alone quantum machine learning!", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/9"}, "9": {"author": "QuantumMan", "date": "1684948059288", "content": "That helps. I do have a GPU VM, where i am trying to run some QGAN application where i can scale the Qubits and see how the resource utlization is varying.  Do you have any other simple QGAN example i can use for above purpose?", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/10"}, "10": {"author": "isaacdevlugt", "date": "1684952602372", "content": "AH!  There is a way to simply increase the number of qubits. Sorry! Let me backpedal.\nEverything I said about the dimensions needing to match (i.e., the generator needs to generate 8x8 = 64 unit features) is still correct. However, the patch method\u2019s architecture can be moulded to your liking. The nuances I was glossing over are actually quite important .\nGiven N_G sub-generators that make up the entire generator, N qubits, and N_A ancillary qubits, the size of each sub-generator\u2019s output (patch_size) is 2^{N - N_A}. So, when all of the sub-generators are recombined / concatenated together, that cumulative output needs to be 8x8.\nIn the tutorial, we have N_G = 4, N = 5, and N_A = 1. Each sub-generator will create a feature whose length is 2^{5 - 1} = 2^4 = 16. 4 groups of 16 gives us 64 \u2014 the dimensionality we need!\nHowever, I could easily do N_G = 4, N = 6, and N_A = 2 and the math still checks out (you can try this). You could even do N_G = 16, N = 5, and N_A = 3.\nHere\u2019s the magic formula you can use: 2^{N - N_A} = \\frac{64}{N_G}. If that equation holds, then those values of N_G, N, and N_A will work.\nSorry about that! Hopefully that clears things up, but please let me know if you\u2019re still confused!1", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/11"}, "11": {"author": "QuantumMan", "date": "1684989199192", "content": "Thank you , that actually helped to scale the qubits. I see the total time to run the whole application increased, but the Nvidia GPU utilization seems stagnated around 12% for 5, 10, 15 n_qubits. I am using \u2018lightning.gpu\u2019  plugin. Any thoughts on how to increase the gpu utlization?1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/12"}, "12": {"author": "QuantumMan", "date": "1684991227310", "content": "Do you think the descriminators aren\u2019t scaled as the generators are scaling, so would that be a problem. I see as the qubits scaled, the total runtime of the program is increasing.", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/13"}, "13": {"author": "isaacdevlugt", "date": "1685019827489", "content": "\nAny thoughts on how to increase the gpu utlization?\n\nMy thoughts initially are that the calculation will use what it needs to use and nothing more.\n\nDo you think the descriminators aren\u2019t scaled as the generators are scaling, so would that be a problem. I see as the qubits scaled, the total runtime of the program is increasing.\n\nI\u2019m not sure I understand your question here. But, if you want to scale up the number of trainable parameters in the discriminator, you\u2019d need to tweak the inner dimensions of the classical layers in Discriminator:\nclass Discriminator(nn.Module):\n    \"\"\"Fully connected classical discriminator\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            # Inputs to first hidden layer (num_input_features -> 64)\n            nn.Linear(image_size * image_size, 64),\n            nn.ReLU(),\n            # First hidden layer (64 -> 16)\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            # Second hidden layer (16 -> output)\n            nn.Linear(16, 1),\n            nn.Sigmoid(),\n        )\n\nRegarding an increase in runtime as the number of qubits increases, that is to be expected .1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/14"}, "14": {"author": "QuantumMan", "date": "1685026405625", "content": "\n\n\n isaacdevlugt:\n\nif you want to scale up the number of trainable parameters in the discriminator, you\u2019d need to tweak the inner dimensions of the classical layers in Discriminator:\n\n\n\n\nI do understand as the number of generators/number of qubits/ancillary qubits increase, we do have more runtime. But that increase in runtime is caused by more computation, which i believe should show up in the increase of GPU utilization(lightning.gpu) , but i see it as flat line. Thats what i was wondering. Even CPU/Memory seems normal. SO i am wondering number of generators aren\u2019t either being leveraging whole GPU and is blocked by soemthing else.", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/15"}, "15": {"author": "isaacdevlugt", "date": "1685029013875", "content": "The increase in run time won\u2019t necessarily correlate to an increase in GPU usage across all regimes. For circuits with 5-15 qubits like what you have, lightning.gpu is probably consumed by computational overheads that you simply are not exceeding due to the small circuit size. For 20+ qubits and deep circuits, you should probably see that 12% number increase. You can also try using adjoint instead of parameter-shift in those regimes, as well. That will probably help!\nIf you\u2019re sticking to regimes with <20 qubits, lightning gpu isn\u2019t the best.", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/16"}, "16": {"author": "QuantumMan", "date": "1685051326714", "content": "Thanks @isaacdevlugt - Can you explain what plugin should i be using. I am hoping we have GPU VM , so using lighting.gpu and also whats the computational overhead between 5-15%.  The above script doesn\u2019t seem to support adjoint diff_method.\nAlso when i use 30 qubits, 26 anciallary qubits. the script stuck\u2026 Any idea why?\ntime python3 qgan.py\n^C^C^C\n^C^C^C^C^CTraceback (most recent call last):\nFile \u201cqgan.py\u201d, line 220, in \nfake_data = generator(noise)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u201d, line 1501, in _call_impl\nreturn forward_call(*args, **kwargs)\nFile \u201cqgan.py\u201d, line 176, in forward\nq_out = partial_measure(elem, params).float().unsqueeze(0)\nFile \u201cqgan.py\u201d, line 135, in partial_measure\nprobs = quantum_circuit(noise, weights)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/qnode.py\u201d, line 889, in call\nres = qml.execute(\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/interfaces/execution.py\u201d, line 729, in execute\nres = _execute(\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/interfaces/torch.py\u201d, line 258, in execute\nreturn ExecuteTapes.apply(kwargs, *parameters)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/torch/autograd/function.py\u201d, line 506, in apply\nreturn super().apply(*args, **kwargs)  # type: ignore[misc]\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/interfaces/torch.py\u201d, line 87, in forward\nres, ctx.jacs = ctx.execute_fn(ctx.tapes, **ctx.gradient_kwargs)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/interfaces/execution.py\u201d, line 205, in wrapper\nres = fn(execution_tapes.values(), **kwargs)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/interfaces/execution.py\u201d, line 131, in fn\nreturn original_fn(tapes, **kwargs)\nFile \u201c/usr/lib/python3.8/contextlib.py\u201d, line 75, in inner\nreturn func(*args, **kwds)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/_qubit_device.py\u201d, line 591, in batch_execute\nres = self.execute(circuit)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/_qubit_device.py\u201d, line 381, in execute\nresults = self.statistics(circuit=circuit)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane_lightning_gpu/lightning_gpu.py\u201d, line 408, in statistics\nreturn super().statistics(circuit, shot_range, bin_size)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane/_qubit_device.py\u201d, line 757, in statistics\nself.probability(wires=obs.wires, shot_range=shot_range, bin_size=bin_size)\nFile \u201c/home/exouser/.local/lib/python3.8/site-packages/pennylane_lightning_gpu/lightning_gpu.py\u201d, line 769, in probability\nself._gpu_state.Probability(device_wires)\nKeyboardInterrupt\nreal\t7m23.809s", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/17"}, "17": {"author": "isaacdevlugt", "date": "1685106964381", "content": "\nCan you explain what plugin should i be using\n\nI think using lightning.qubit in tandem with Pytorch / JAX is a good idea!\n\nAlso when i use 30 qubits, 26 anciallary qubits. the script stuck\u2026 Any idea why?\n\nIt\u2019s tough to say, but it\u2019s most likely the fact that it\u2019s an extremely large calculation that you\u2019re wanting to do so it takes a while!", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/18"}, "18": {"author": "Eleonora_Panini", "date": "1699352500910", "content": "Hi @isaacdevlugt, I adapted the quantum GAN demo for 64x64 images RGB. I used 13 qubits, 2 ancillary, 2 layers depth and 6 sub generators. The input to discriminator is 64x64x3, while the input of the generator is batch size x n.qubits. The training was really slow but it completed using alsoGPU and lightning.qubit or .gpu. Although after 800 epochs the output images are a black square with some coloured points and the discriminator loss reached a really low value.\nimage547\u00d7582 20.9 KB\nSo the problem is not how many epochs training  but i think we need to modify something in the quantum generator. The output images have 4 dimensions(batch size, 64,64,3). Can you help me in order to make the model work for RGB images?\nThe code is the file \u2018qgan\u2019 on github.\n\n\nGitHub\n\n\n\nGitHub - Elyon7jkd/qGAN\nContribute to Elyon7jkd/qGAN development by creating an account on GitHub.\n\n\n\n\n\n\nThank you", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/19"}, "19": {"author": "isaacdevlugt", "date": "1699484262503", "content": "Hey @Eleonora_Panini,\nI briefly looked at your repository and it seems like your code works for RGB images in the sense that you don\u2019t encounter any syntax or data-handling errors. I can help you figure out how to use and fix errors you may encounter in pennylane and plugins / interfaces for it, but getting your code to work better is a question for you to solve \nIf your code isn\u2019t performing like you expect it to, then I suggest that you scale down your model\u2019s size, the data, etc., to make debugging / prototyping that much easier. It\u2019s so much nicer to get a smaller code base to work properly and scale it up after the fact. Let me know if that helps at all!1 Reply", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/20"}, "20": {"author": "Eleonora_Panini", "date": "1699352500910", "content": "Hi @isaacdevlugt, I adapted the quantum GAN demo for 64x64 images RGB. I used 13 qubits, 2 ancillary, 2 layers depth and 6 sub generators. The input to discriminator is 64x64x3, while the input of the generator is batch size x n.qubits. The training was really slow but it completed using alsoGPU and lightning.qubit or .gpu. Although after 800 epochs the output images are a black square with some coloured points and the discriminator loss reached a really low value.\nimage547\u00d7582 20.9 KB\nSo the problem is not how many epochs training  but i think we need to modify something in the quantum generator. The output images have 4 dimensions(batch size, 64,64,3). Can you help me in order to make the model work for RGB images?\nThe code is the file \u2018qgan\u2019 on github.\n\n\nGitHub\n\n\n\nGitHub - Elyon7jkd/qGAN\nContribute to Elyon7jkd/qGAN development by creating an account on GitHub.\n\n\n\n\n\n\nThank you", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/21"}, "21": {"author": "isaacdevlugt", "date": "1699646667537", "content": "Hey @Eleonora_Panini! The images aren\u2019t showing. But it might be that you\u2019re in a barren plateau! Those pop up a lot in quantum machine learning. Might be worth reading up on them and seeing if there are any tips and tricks that you can apply to your problem ", "link": "https://discuss.pennylane.ai//t/how-to-scale-the-qgan-example-to-more-qubits/2958/22"}}