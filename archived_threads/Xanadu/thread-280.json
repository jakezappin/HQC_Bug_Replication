{"0": {"author": "dancbeaulieu", "date": "1628692348166", "content": "Hello,\nI created a QNLP transfer learning program using Pytorch and DistilBERT, and it runs correctly when I run it classically on a local simulator. However, I cannot get it to run on either IBMQ or AWS Braket. I have been trying to diagnose and fix this issue for over a month, and just can\u2019t figure out what is incorrect. The issue is that when I try to run on IBMQ, it doesn\u2019t run on the IBMQ network and just runs locally, despite what I set the IBMQ device to. It runs, creates results but no log on the backend of the service. As for when I run this code on AWS Braket, it holds forever on the training phase (line starting runner.train()) and doesn\u2019t show up in the Braket \u201ctasks\u201d log. It doesn\u2019t work on quantum hardware or in the simulator.\nHere is a link to the data: https://github.com/dancbeaulieu/publicshare/blob/main/sdg_classification.zip 10\nHere is my code:\n#!/usr/bin/env python\ncoding: utf-8\npath_to_data= \u2018data/sdg_classification\u2019\ntrain_filename= \u2018train_set_sdg_1_7_8_12_13_toy.csv\u2019\nvalidation_filename= \u2018val_set_sdg_1_7_8_12_13_toy.csv\u2019\ntest_filename= \u2018eval_set_sdg_1_7_8_12_13_curated_journals_toy.csv\u2019\ntext_field_name= \u2018title_keywords_abstract\u2019\nlabel_field_name= \u2018sdg_id\u2019\npath_to_test_pred_scores= \u2018data/output/pred.txt\u2019\n#model:\nmodel_name=\u2018distilbert-base-uncased\u2019    # pretrained model from Transformers\nmax_seq_length= 256                    # depends on your available GPU memory (in combination with batch size)\nnum_classes= 5\nn_in = 5 #input into quantum layer\n#training:\nlearn_rate= 3e-5                       # learning rate is typically ~1e-5 for transformers\nnum_epochs= 2                         # smth around 2-6 epochs is typically fine when finetuning transformers\naccum_steps= 4                         # one optimization step for that many backward passes\nbatch_size= 8                         # depends on your available GPU memory (in combination with max seq length)\nlog_dir=\u2018logdir\u2019                        # for training logs and tensorboard visualizations\nfp16_params= None                      # fp16 support\nq_depth = 2                 # Depth of the quantum circuit (number of variational layers)\nn_qubits = 2                # Number of qubits\nstep = 0.0004               # Learning rate\nimport time\n#general:\nseed=17                               # random seed for reproducibility\ngamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\nq_delta = 0.01              # Initial spread of random quantum weights\nstart_time = time.time()    # Start of the computation timer\ntrain_samples = 100\ntest_samples = 50\n#data_dir = \u201ctexts/.arxiv/\u201d\nvalidation_split = .2\nshuffle_dataset = True\nimport logging\nfrom pathlib import Path\nfrom typing import List, Mapping, Tuple\nPyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n#import torchvision\n#from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer\n#from catalyst.utils import set_global_seed\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom pathlib import Path\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\nimport numpy as np\nimport torch\nimport yaml\n#from catalyst.dl import SupervisedRunner\nimport pennylane as qml\nimport copy\nimport qiskit\nfrom qiskit import IBMQ\n#Catalyst version 21.X breaks all 20.0 imports\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import (\nAccuracyCallback,\nCheckpointCallback,\nInferCallback,\nOptimizerCallback,\n)\nfrom catalyst.utils import prepare_cudnn, set_global_seed\n#FOR CONNECTING TO IBMQ, REPLACED HUB, GROUP & PROJECT WITH FAKE INFO\n#token = \u2018FAKE TOKEN\u2019\n#IBMQ.save_account(token)\n#IBMQ.load_account()\n#provider = IBMQ.get_provider(hub=\u2018HUB\u2019, group=\u2018GROUP\u2019, project=\u2018PROJECT\u2019)\n#dev = qml.device(\u201cqiskit.ibmq\u201d, wires=n_qubits, backend=\u201cibmq_bogota\u201d, provider=provider)\n#dev = qml.device(\u201cdefault.qubit\u201d, wires=n_qubits)\n#device = torch.device(\u201ccuda:0\u201d if torch.cuda.is_available() else \u201ccpu\u201d)\nPlease enter the S3 bucket you created during onboarding\n(or any other S3 bucket starting with \u2018amazon-braket-\u2019 in your account) in the code below\n#CHANGED BUCKET NAME TO PROTECT MY INFORMATION\nmy_bucket = \u201cbraket-bucket-fake\u201d # the name of the bucket\nmy_prefix = \u201cbraket\u201d # the name of the folder in the bucket\ns3_folder = (my_bucket, my_prefix)\n#Pennylane Hardware config\n#device_arn = \u201carn:aws:braket:::device/quantum-simulator/amazon/sv1\u201d\n#device_arn = \u201carn:aws:braket:::device/qpu/ionq/ionQdevice\u201d\ndevice_arn = \u201carn:aws:braket:::device/qpu/rigetti/Aspen-9\u201d\ndev_remote = qml.device(\n\u201cbraket.aws.qubit\u201d,\ndevice_arn=device_arn,\nwires=n_qubits,\ns3_destination_folder=s3_folder,\nparallel=True,\n)\ndevice = torch.device(\u201ccuda:0\u201d if torch.cuda.is_available() else \u201ccpu\u201d)\ndef get_project_root() -> Path:\nreturn Path(file).parent.parent\n#data\nclass TextClassificationDataset(Dataset):\n\u201c\u201d\"\nWrapper around Torch Dataset to perform text classification\n\u201c\u201d\"\ndef __init__(\n    self,\n    texts: List[str],\n    labels: List[str] = None,\n    label_dict: Mapping[str, int] = None,\n    max_seq_length: int = 512,\n    model_name: str = \"distilbert-base-uncased\",\n):\n    \"\"\"\n    Args:\n        texts (List[str]): a list with texts to classify or to train the\n            classifier on\n        labels List[str]: a list with classification labels (optional)\n        label_dict (dict): a dictionary mapping class names to class ids,\n            to be passed to the validation data (optional)\n        max_seq_length (int): maximal sequence length in tokens,\n            texts will be stripped to this length\n        model_name (str): transformer model name, needed to perform\n            appropriate tokenization\n\n    \"\"\"\n\n    self.texts = texts\n    self.labels = labels\n    self.label_dict = label_dict\n    self.max_seq_length = max_seq_length\n\n    if self.label_dict is None and labels is not None:\n        # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n        # using this instead of `sklearn.preprocessing.LabelEncoder`\n        # no easily handle unknown target values\n        self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n\n    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n    # suppresses tokenizer warnings\n    logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n\n    # special tokens for transformers\n    # in the simplest case a [CLS] token is added in the beginning\n    # and [SEP] token is added in the end of a piece of text\n    # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n    self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n    self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n    self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n\ndef __len__(self) -> int:\n    \"\"\"\n    Returns:\n        int: length of the dataset\n    \"\"\"\n    return len(self.texts)\n\ndef __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n    \"\"\"Gets element of the dataset\n\n    Args:\n        index (int): index of the element in the dataset\n    Returns:\n        Single element by index\n    \"\"\"\n\n    # encoding the text\n    x = self.texts[index]\n\n    # a dictionary with `input_ids` and `attention_mask` as keys\n    output_dict = self.tokenizer.encode_plus(\n        x,\n        add_special_tokens=True,\n        padding=\"max_length\",\n        max_length=self.max_seq_length,\n        return_tensors=\"pt\",\n        truncation=True,\n        return_attention_mask=True,\n    )\n\n    # for Catalyst, there needs to be a key called features\n    output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n    del output_dict[\"input_ids\"]\n\n    # encoding target\n    if self.labels is not None:\n        y = self.labels[index]\n        y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n        output_dict[\"targets\"] = y_encoded\n\n    return output_dict\n\ndef read_data(params: dict) -> Tuple[dict, dict]:\n\u201c\u201d\"\nA custom function that reads data from CSV files, creates PyTorch datasets and\ndata loaders. The output is provided to be easily used with Catalyst\n:param params: a dictionary read from the config.yml file\n:return: a tuple with 2 dictionaries\n\"\"\"\n# reading CSV files to Pandas dataframes\ntrain_df = pd.read_csv(\n    Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"train_filename\"]\n)\nvalid_df = pd.read_csv(\n    Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"validation_filename\"]\n)\ntest_df = pd.read_csv(\n    Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"test_filename\"]\n)\n\n# creating PyTorch Datasets\ntrain_dataset = TextClassificationDataset(\n    texts=train_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n    labels=train_df[params[\"data\"][\"label_field_name\"]].values,\n    max_seq_length=params[\"model\"][\"max_seq_length\"],\n    model_name=params[\"model\"][\"model_name\"],\n)\n\nvalid_dataset = TextClassificationDataset(\n    texts=valid_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n    labels=valid_df[params[\"data\"][\"label_field_name\"]].values,\n    max_seq_length=params[\"model\"][\"max_seq_length\"],\n    model_name=params[\"model\"][\"model_name\"],\n)\n\ntest_dataset = TextClassificationDataset(\n    texts=test_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n    labels=test_df[params[\"data\"][\"label_field_name\"]].values,\n    max_seq_length=params[\"model\"][\"max_seq_length\"],\n    model_name=params[\"model\"][\"model_name\"],\n)\n\nset_global_seed(params[\"general\"][\"seed\"])\n\n# creating PyTorch data loaders and placing them in dictionaries (for Catalyst)\ntrain_val_loaders = {\n    \"train\": DataLoader(\n        dataset=train_dataset,\n        batch_size=params[\"training\"][\"batch_size\"],\n        shuffle=True,\n    ),\n    \"valid\": DataLoader(\n        dataset=valid_dataset,\n        batch_size=params[\"training\"][\"batch_size\"],\n        shuffle=False,\n    ),\n}\n\ntest_loaders = {\n    \"test\": DataLoader(\n        dataset=test_dataset,\n        batch_size=params[\"training\"][\"batch_size\"],\n        shuffle=False,\n    )\n}\n\nreturn train_val_loaders, test_loaders, train_dataset, valid_dataset, test_dataset\n\nclass BertForSequenceClassification(nn.Module):\n\u201c\u201d\"\nSimplified version of the same class by HuggingFace.\nSee transformers/modeling_distilbert.py in the transformers repository.\n\u201c\u201d\"\ndef __init__(\n    self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3\n):\n    \"\"\"\n    Args:\n        pretrained_model_name (str): HuggingFace model name.\n            See transformers/modeling_auto.py\n        num_classes (int): the number of class labels\n            in the classification task\n    \"\"\"\n    super().__init__()\n\n    config = AutoConfig.from_pretrained(\n        pretrained_model_name, num_labels=num_classes\n    )\n\n    self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n    #for param in self.parameters():\n    #    param.requires_grad = False\n    self.classifier = nn.Linear(config.hidden_size, num_classes)\n    self.dropout = nn.Dropout(dropout)\n\ndef forward(self, features, attention_mask=None, head_mask=None):\n    \"\"\"Compute class probabilities for the input sequence.\n\n    Args:\n        features (torch.Tensor): ids of each token,\n            size ([bs, seq_length]\n        attention_mask (torch.Tensor): binary tensor, used to select\n            tokens which are used to compute attention scores\n            in the self-attention heads, size [bs, seq_length]\n        head_mask (torch.Tensor): 1.0 in head_mask indicates that\n            we keep the head, size: [num_heads]\n            or [num_hidden_layers x num_heads]\n    Returns:\n        PyTorch Tensor with predicted class scores\n    \"\"\"\n    assert attention_mask is not None, \"attention mask is none\"\n\n    # taking BERTModel output\n    # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n    bert_output = self.model(\n        input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n    )\n    # we only need the hidden state here and don't need\n    # transformer output, so index 0\n    seq_output = bert_output[0]  # (bs, seq_len, dim)\n    # mean pooling, i.e. getting average representation of all tokens\n    pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n    pooled_output = self.dropout(pooled_output)  # (bs, dim)\n    scores = self.classifier(pooled_output)  # (bs, num_classes)\n\n    return scores\n\nwith open(str(\u201cconfig.yml\u201d)) as f:\nparams = yaml.load(f, Loader=yaml.FullLoader)\ndef H_layer(nqubits):\n\u201c\u201d\u201cLayer of single-qubit Hadamard gates.\n\u201c\u201d\u201d\nfor idx in range(nqubits):\nqml.Hadamard(wires=idx)\ndef RY_layer(w):\n\u201c\u201d\u201cLayer of parametrized qubit rotations around the y axis.\n\u201c\u201d\u201d\nfor idx, element in enumerate(w):\n#print(\u2018w in RY_layer\u2019, type(w))\nqml.RY(element, wires=idx)\ndef RX_layer(w):\n\u201c\u201d\u201cLayer of parametrized qubit rotations around the X axis.\n\u201c\u201d\u201d\nfor idx, element in enumerate(w):\n#print(\u2018w in RY_layer\u2019, type(w))\nqml.RX(element, wires=idx)\ndef RZ_layer(w):\n\u201c\u201d\u201cLayer of parametrized qubit rotations around the Z axis.\n\u201c\u201d\u201d\nfor idx, element in enumerate(w):\n#print(\u2018w in RY_layer\u2019, type(w))\nqml.RZ(element, wires=idx)\ndef entangling_layer(nqubits):\n\u201c\u201d\u201cLayer of CNOTs followed by another shifted layer of CNOT.\n\u201c\u201d\u201d\n# In other words it should apply something like :\n# CNOT  CNOT  CNOT  CNOT\u2026  CNOT\n#   CNOT  CNOT  CNOT\u2026  CNOT\nfor i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,\u2026N-2\nqml.CNOT(wires=[i, i + 1])\nfor i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,\u2026N-3\nqml.CNOT(wires=[i, i + 1])\ndef entangling_layer_alt(nqubits):\n\u201c\u201d\u201cLayer of CNOTs followed by another shifted layer of CNOT.\n\u201c\u201d\u201d\n# In other words it should apply something like :\n# CNOT  CNOT  CNOT  CNOT\u2026  CNOT\n#   CNOT  CNOT  CNOT\u2026  CNOT\nfor i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,\u2026N-2\nqml.CNOT(wires=[i, i + 1])\nfor i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,\u2026N-3\nqml.CNOT(wires=[i, i + 1])\n#Original version of quantum circuit used in Mari et al 2019\n@qml.qnode(dev_remote, interface=\u201ctorch\u201d)\ndef quantum_net(q_input_features, q_weights_flat):\n\u201c\u201d\"\nThe variational quantum circuit.\n\u201c\u201d\"\n# Reshape weights\nq_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n# Start from state |+> , unbiased w.r.t. |0> and |1>\nH_layer(n_qubits)\n\n# Embed features in the quantum node\nRY_layer(q_input_features)\n\n# Sequence of trainable variational layers\nfor k in range(q_depth):\n    entangling_layer(n_qubits)\n    RY_layer(q_weights[k])\n\n# Expectation values in the Z basis\nexp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\nreturn tuple(exp_vals)\n\nread and process data\ntrain_val_loaders, test_loaders, train_dataset, valid_dataset, test_dataset = read_data(params)\nreproducibility\nset_global_seed(seed)\nprepare_cudnn(deterministic=True)\nclass DressedQuantumNet(nn.Module):\n\u201c\u201d\"\nTorch module implementing the dressed quantum net.\n\u201c\u201d\"\ndef __init__(self):\n    \"\"\"\n    Definition of the *dressed* layout.\n    \"\"\"\n\n    super().__init__()\n    self.pre_net = nn.Linear(num_classes, n_qubits)\n    self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))       \n    self.post_net = nn.Linear(n_qubits, 2)\n\ndef forward(self, input_features):\n    \"\"\"\n    Defining how tensors are supposed to move through the *dressed* quantum\n    net.\n    \"\"\"\n\n    # obtain the input features for the quantum circuit\n    # by reducing the feature dimension from 512 to 4\n    pre_out = self.pre_net(input_features)\n    q_in = torch.tanh(pre_out) * np.pi / 2.0\n\n    # Apply the quantum circuit to each element of the batch and append to q_out\n    q_out = torch.Tensor(0, n_qubits)\n    q_out = q_out.to(device)\n    for elem in q_in:\n        q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0)\n        #quantum_net(elem, self.q_params).float().unsqueeze(0)\n        q_out = torch.cat((q_out, q_out_elem))\n\n    # return the two-dimensional prediction from the postprocessing layer\n    return self.post_net(q_out)\n\nclass BertForSequenceClassification_qnlp(nn.Module):\n\u201c\u201d\"\nSimplified version of the same class by HuggingFace.\nSee transformers/modeling_distilbert.py in the transformers repository.\n\u201c\u201d\"\ndef __init__(\n    self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3\n):\n    \"\"\"\n    Args:\n        pretrained_model_name (str): HuggingFace model name.\n            See transformers/modeling_auto.py\n        num_classes (int): the number of class labels\n            in the classification task\n    \"\"\"\n    super().__init__()\n\n    config = AutoConfig.from_pretrained(\n        pretrained_model_name, num_labels=num_classes\n    )\n\n    self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n\n    #for param in self.parameters():\n    #    param.requires_grad = False \n\n    self.classifier = nn.Linear(config.hidden_size, num_classes)\n    self.linear = DressedQuantumNet()\n    self.dropout = nn.Dropout(dropout)\n\ndef forward(self, features, attention_mask=None, head_mask=None):\n    \"\"\"Compute class probabilities for the input sequence.\n\n    Args:\n        features (torch.Tensor): ids of each token,\n            size ([bs, seq_length]\n        attention_mask (torch.Tensor): binary tensor, used to select\n            tokens which are used to compute attention scores\n            in the self-attention heads, size [bs, seq_length]\n        head_mask (torch.Tensor): 1.0 in head_mask indicates that\n            we keep the head, size: [num_heads]\n            or [num_hidden_layers x num_heads]\n    Returns:\n        PyTorch Tensor with predicted class scores\n    \"\"\"\n    assert attention_mask is not None, \"attention mask is none\"\n\n    # taking BERTModel output\n    # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n    bert_output = self.model(\n        input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n    )\n    # we only need the hidden state here and don't need\n    # transformer output, so index 0\n    seq_output = bert_output[0]  # (bs, seq_len, dim)\n    # mean pooling, i.e. getting average representation of all tokens\n    pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n    pooled_output = self.dropout(pooled_output)  # (bs, dim)\n    scores = self.classifier(pooled_output)  # (bs, num_classes)\n\n    return scores\n\nmodel_dressnet = BertForSequenceClassification_qnlp(\npretrained_model_name=model_name ,\nnum_classes=num_classes\n)\nUse CUDA or CPU according to the \u201cdevice\u201d object.\nmodel_dressnet = model_dressnet.to(device)\nspecify criterion for the multi-class classification task, optimizer and scheduler\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(\nmodel_dressnet.parameters(), lr=float(learn_rate)\n)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\nhere we specify that we pass masks to the runner. So model\u2019s forward method will be called with\nthese arguments passed to it.\nrunner = SupervisedRunner(input_key=(\u201cfeatures\u201d, \u201cattention_mask\u201d))\n#quantum_net_alt, original alternate version based on randomized quantum circuit\nfinally, training the model with Catalyst\nrunner.train(\n#input_key=(\u201cfeatures\u201d, \u201cattention_mask\u201d),\nmodel=model_dressnet,\ncriterion=criterion,\noptimizer=optimizer,\nscheduler=scheduler,\nloaders=train_val_loaders,\ncallbacks=[\nAccuracyCallback(num_classes=num_classes),\nOptimizerCallback(accumulation_steps=accum_steps),\n],\nlogdir=log_dir,\nnum_epochs=num_epochs,\nverbose=True\n)", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/1"}, "1": {"author": "christina", "date": "1628772644971", "content": "Edit: correcting for typo.  Sorry for that\u2026\nThanks for your question.\nHave you been able to run any individual simple test circuits on either of the platforms?\nBefore getting the full workflow evaluated, I often find it helpful to make sure I have everything set up correctly with a qnode like\n@qml.qnode(dev)\ndef circuit(x):\n    qml.RX(x,wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ncircuit(0.1)\n\nThen I know for sure its the remote communication that\u2019s the problem and not anything in a long chunk of code.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/2"}, "2": {"author": "dancbeaulieu", "date": "1628793768716", "content": "Hello! I had to modify the code to get it to work, had to make it \u201c@qml.qnode(dev)\u201d then it worked. Gave me the result \u201ctensor(0.99500417, requires_grad=True)\u201d. So it looks like the problem is the connection to quantum hardware in the code. I can also connect to the IBMQ system \u201cibmq_casablanca\u201d and run the code successfully. So I can run on my local system and on IBMQ quantum hardware.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/3"}, "3": {"author": "gojiita_ku", "date": "1628824116774", "content": "Hi @dancbeaulieu,\nYour work looks fantastic! So did you also compare the performance of  this quantum Bert model with the original Bert model? For example, did this quantum model achieve a better performance in terms of classification accuracy and F1 score on the same dataset?1", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/4"}, "4": {"author": "dancbeaulieu", "date": "1629205736172", "content": "I can\u2019t get the model to work on any quantum hardware or simulator. Only on my local system. I need assistance with this and help figuring out why it doesn\u2019t run on quantum computers using pennylane.2 Replies", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/5"}, "5": {"author": "Maria_Schuld", "date": "1629266652590", "content": "Hey @dancbeaulieu,\nIf you want help figuring out how to run your code on quantum hardware, the easiest will be if you open a new thread with a minimum working example of what does not work. It may not be related to the transfer learning idea at all!\nMaking an MWE (i.e. reproducing the error you see with just a few lines of code) is crucial - it will speed up our reply, make it more targeted, and in 90% of the cases creating the MWE makes you solve the issue  (at least that is my experience).1 Reply", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/6"}, "6": {"author": "_risto", "date": "1631420179998", "content": "Hi @dancbeaulieu\nI could not get it run on IBMQ lab. Did you manage to run it?", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/7"}, "7": {"author": "CatalinaAlbornoz", "date": "1631581684958", "content": "Hi @_risto!\nCould you share your code and error message so that we can take a look into it?\nThanks!", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/8"}, "8": {"author": "_risto", "date": "1631640597410", "content": "Hi @CatalinaAlbornoz\nWhen I run the https://pennylane.ai/qml/demos/tutorial_quantum_transfer_learning.html 5 in IBMQ lab and when importing data: data dir = os.path.abspath(rPATH), I get: FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/PATH.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/9"}, "9": {"author": "Diego", "date": "1631654435464", "content": "Hi @_risto,\nDoes the code work when running locally?\nCould you verify that data_dir points to the folder hymenoptera_data?", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/10"}, "10": {"author": "_risto", "date": "1631654888748", "content": "Hi @Diego\nWhen I run the code on the notebook from my terminal it works.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/11"}, "11": {"author": "CatalinaAlbornoz", "date": "1631738705550", "content": "Hi @_risto, I\u2019m not being able to reproduce your error. Are you sure that you have all of your images in the right folder? If your notebook is in the IBMQ lab outside any folder, you can then create the hymenoptera_data folder (with all of its contents) and change the line data_dir = \"../_data/hymenoptera_data\" to data_dir = \"./hymenoptera_data\"\nPlease let me know if this works for you!\n\n32%20PM1704\u00d7668 89.9 KB\n", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/12"}, "12": {"author": "_risto", "date": "1631739083135", "content": "Hi @CatalinaAlbornoz\nWhen importing pennylane and qml I get:\nName: PennyLane\nVersion: 0.17.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: GitHub - PennyLaneAI/pennylane: PennyLane is a cross-platform Python library for differentiable programming of quantum computers. Train a quantum computer the same way as a neural network. 1\nAuthor: None\nAuthor-email: None\nLicense: Apache License 2.0\nLocation: /opt/conda/lib/python3.8/site-packages \u2192 I can\u2019t find this folder on my laptop.\nThis is why I get:\n\nimage1294\u00d782 8.16 KB\n\nDo I need to install IBM MQ?", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/13"}, "13": {"author": "CatalinaAlbornoz", "date": "1631739901449", "content": "Hi @_risto ! I think I might have misunderstood the question. Are you trying run this notebook in the IBMQ Lab platform? Or locally on your computer but using an IBM device?\nDon\u2019t worry about finding the Python folder in your laptop, I think the problem is somewhere else. Could you send the full error message?", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/14"}, "14": {"author": "_risto", "date": "1631896185244", "content": "Hi @CatalinaAlbornoz\nIf I run it locally (so to call the IBMQ as device) or on IBMQ lab both times the hymenoptera data can not be found. The difference is, that when I do it through the later, I noticed that the file being executed is located at  /opt/conda/lib/python3.8/site-packages", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/15"}, "15": {"author": "CatalinaAlbornoz", "date": "1631923863865", "content": "Hi @_risto, that\u2019s very strange. Could you send the location of your notebook, and the location of the data folder?", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/16"}, "16": {"author": "dancbeaulieu", "date": "1633463388314", "content": "Thanks, I took some time to make a new program and create a minimum example as clean as possible with an aim to eliminate unnecessary packages. Opening a new discussion.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/17"}, "17": {"author": "CatalinaAlbornoz", "date": "1633476376476", "content": "Thank you @dancbeaulieu!", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/18"}, "18": {"author": "_risto", "date": "1637105764964", "content": "Hi @CatalinaAlbornoz\nI went again to run the QTL tutorial, step by step, but I get:\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper___cat)\nThis has not happened before. I have updated pennylane and pip.", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/19"}, "19": {"author": "CatalinaAlbornoz", "date": "1637166011464", "content": "Hi @_risto, the last line before Dataset Loading is where we configure the Pytorch device. This is how it should look. device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nIs this how it looks for you? If it does then it\u2019s probably a problem with torch.\nI would suggest that you try to uninstall and reinstall like it\u2019s shown here. I\u2019m running it in my cpu and it\u2019s working.\nPlease let me know if this works for you!", "link": "https://discuss.pennylane.ai//t/quantum-nlp-transfer-learning/1270/20"}}