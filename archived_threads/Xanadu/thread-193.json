{"0": {"author": "Vineesha", "date": "1586635719528", "content": "\n\n\nPhysical Review Letters \u2013 1 Feb 19\n\n\nQuantum Machine Learning in Feature Hilbert Spaces 5\nA basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this...\n\n\n\n\n\nI am trying to implement the variational quantum classifier(explicit approach- Figs. 4 and 5) as described in the paper using Pennylane. The circuit architecture is provided in the supplementary information.\nI have coded everything and it seems okay to me. However, it doesn\u2019t work and I get the UserWarning: Output seems independent of input.\nCan there be a possible error in encoding the inputs and circuit parameters?\n@Maria_Schuld\nCode snippet:\ndev_fock = qml.device(\"strawberryfields.fock\", wires=2, cutoff_dim=3)\n\ndef layer(W):\n    qml.Beamsplitter(W[0,0], W[1,0], wires=[0,1])\n    qml.Displacement(1.0,W[0,1],wires=0)\n    qml.Displacement(1.0,W[1,1],wires=1)\n    qml.QuadraticPhase(W[0,2],wires=0)\n    qml.QuadraticPhase(W[1,2],wires=1)\n    qml.CubicPhase(W[0,3],wires=0)\n    qml.CubicPhase(W[1,3],wires=1)\n\n@qml.qnode(dev_fock)\ndef circuit_p0(weights, x):\n    qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n    for W in weights:\n        layer(W)\n    return expval(qml.FockStateProjector(np.array([2,0]), wires=[0,1]))\n\n@qml.qnode(dev_fock)\ndef circuit_p1(weights, x):\n    qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n    for W in weights:\n        layer(W)\n    return expval(qml.FockStateProjector(np.array([0,2]), wires=[0,1]))\n\ndef variational_classifier(var, x):\n    weights=var[0]\n    bias=var[1]\n    p0= circuit_p0(weights,x)+bias\n    p1= circuit_p1(weights,x)+bias\n    prob_y0= p0/(p0+p1)\n    prob_y1= p1/(p0+p1)\n    if prob_y0> prob_y1:\n        ans=-1\n    else:\n        ans=1\n    return ans\n\ndef square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n    loss = loss / len(labels)\n    return loss\n\ndef accuracy(labels, predictions):\n    acc = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            acc = acc + 1\n    acc = acc / len(labels)\n    return acc\n\ndef cost(var, X, Y):\n    predictions = [variational_classifier(var, x=x) for x in X]\n    return square_loss(Y, predictions)\n\nX, Y= sklearn.datasets.make_moons(n_samples=200, shuffle=True, noise=0.1, random_state=None)\nY = Y * 2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n\nnp.random.seed(0)\nnum_qubits = 2\nnum_layers = 4\nvar_init = (np.random.randn(num_layers, num_qubits,4),0.0)\n\nbatch_size = 5\nopt = AdamOptimizer(0.01, beta1=0.9, beta2=0.99)\nvar = var_init\nsqloss=np.zeros(50)\nfor it in range(50):\n    batch_index = np.random.randint(0, len(X), (batch_size,))\n    X_batch = X[batch_index]\n    Y_batch = Y[batch_index]\n    var = opt.step(lambda v: cost(v, X_batch, Y_batch), var)\n    predictions = [variational_classifier(var, x) for x in X]\n    acc = accuracy(Y, predictions)\n    sqloss[it]= square_loss(Y,predictions)\n    print(\"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(it + 1, cost(var, X, Y), acc))\n", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/1"}, "1": {"author": "Maria_Schuld", "date": "1586796276645", "content": "Dear Vineesha,\nWelcome to the forum and thanks for your question!\nWhen I run your code I get a different error from yours.\nI wonder if you could please shorten your code to a minimum working example which only contains the lines necessary to reproduce the error? That would make it a lot easier for me to try and help 1", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/2"}, "2": {"author": "Vineesha", "date": "1589938779565", "content": "Dear Maria Schuld,\nThank you for your reply.\nI was able to fix the errors.\nHowever, the training is not very good. I get the results as shown below for 50 training examples generated using X, Y= sklearn.datasets.make_circles(n_samples=50, shuffle=True, noise=0.1, random_state=None)\nI use Adam optimizer with learning rate 0.005 with square-loss cost function and batch-size=5.\nHow can I improve my training performance?\n\nvar_class_results890\u00d7314 34.9 KB\n2 Replies", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/3"}, "3": {"author": "Maria_Schuld", "date": "1589952235421", "content": "That is the central research question when doing machine learning \nYou will have to experiment with different optimization settings and models, and if none works you might want think about theoretical limitations that prevent your model from training. It\u2019s hard to give you a general answer here\u20261", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/4"}, "4": {"author": "valeria", "date": "1594652531352", "content": "Hi Vineesha,\nI am trying to implement the classifier described in the cited article and I\u2019m getting the same warning you mentioned: UserWarning: Output seems independent of input.\nMy code is actually similar to yours\u2026 I can\u2019t really understand where I\u2019m doing wrong and how this warning can affect my results.\nHow did you manage to solve this issue?\nThanks in advance!", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/5"}, "5": {"author": "sjahangiri", "date": "1594659653715", "content": "Hi @valeria. Thanks for the question. Could you please run qml.about() and tell me the output and also give me the full list of your imports?", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/6"}, "6": {"author": "valeria", "date": "1594751678489", "content": "Hi @sjahangiri,\nThanks for your answer! After running qml.about(), I get the following output:\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip. Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue. To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\nName: PennyLane\nVersion: 0.8.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: None\nAuthor-email: None\nLicense: Apache License 2.0\nLocation: c:\\users\\valeria\\anaconda3\\lib\\site-packages\nRequires: scipy, networkx, appdirs, autograd, numpy, toml, semantic-version\nRequired-by: PennyLane-SF, PennyLane-qiskit, PennyLane-Cirq\nPlatform info:           Windows-8.1-6.3.9600-SP0\nPython version:          3.7.3\nNumpy version:           1.18.2\nScipy version:           1.2.1\nInstalled devices:\n- default.gaussian (PennyLane-0.8.1)\n- default.qubit (PennyLane-0.8.1)\n- default.tensor (PennyLane-0.8.1)\n- default.tensor.tf (PennyLane-0.8.1)\n- strawberryfields.fock (PennyLane-SF-0.8.0)\n- strawberryfields.gaussian (PennyLane-SF-0.8.0)\n- qiskit.aer (PennyLane-qiskit-0.8.2)\n- qiskit.basicaer (PennyLane-qiskit-0.8.2)\n- qiskit.ibmq (PennyLane-qiskit-0.8.2)\n- cirq.simulator (PennyLane-Cirq-0.8.0)\n\nMoreover, these are the packages I imported in my notebook:\nfrom pennylane import numpy as np\nimport pennylane as qml\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n%matplotlib inline\nThanks in advance for the help ", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/7"}, "7": {"author": "sjahangiri", "date": "1594753630082", "content": "Thanks @valeria. I get a different error when I run the code copied above. Could you please copy a minimum working example of your code that gives that warning?", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/8"}, "8": {"author": "valeria", "date": "1594910596664", "content": "Hi @sjahangiri. The following is the needed code to get the mentioned warning.\nfrom pennylane import numpy as np\nimport pennylane as qml\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n%matplotlib inline\n\nnp.random.seed(0)\neta = 0.01\nbatch_size = 5\nn_steps = 500\nn_layers = 4\nTheta = (0.05*np.random.rand(n_layers,2,4),0.01)\nfraction_train = 0.75\ndev = qml.device(\"strawberryfields.fock\", wires=2, cutoff_dim=3, analytic=False)\n\ndef layer(l,params):\n    qml.Beamsplitter(params[l,0,0],params[l,1,0],wires=[0,1])\n    qml.Displacement(params[l,0,1],0.0,wires=0)\n    qml.Displacement(params[l,1,1],0.0,wires=1)\n    qml.QuadraticPhase(params[l,0,2],wires=0)\n    qml.QuadraticPhase(params[l,1,2],wires=1)\n    qml.CubicPhase(params[l,0,3],wires=0)\n    qml.CubicPhase(params[l,1,3],wires=1)\n\n@qml.qnode(dev)\ndef circuit_o0(params, x):\n    qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n    for l in range(n_layers):\n        layer(l,params)\n    return qml.expval(qml.FockStateProjector(np.array([2,0]), wires=[0,1]))\n\n@qml.qnode(dev)\ndef circuit_o1(params, x):\n    qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n    for l in range(n_layers):\n        layer(l,params)\n    return qml.expval(qml.FockStateProjector(np.array([0,2]), wires=[0,1]))\n\ndef variational_classifier(Theta, x):\n    params = Theta[0]\n    bias = Theta[1]\n    o0 = circuit_o0(params,x) + bias\n    o1 = circuit_o1(params,x) + bias\n    p0 = o0/(o0+o1)\n    p1 = o1/(o0+o1)\n    if p0 > p1:\n        label = 0\n    else:\n        label = 1\n    return label\n\ndef square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n    loss = loss / len(labels)\n    return loss\n\ndef accuracy(labels, predictions):\n    acc = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            acc = acc + 1\n    acc = acc / len(labels)\n    return acc\n\ndef cost(Theta, X, Y):\n    predictions = np.array([variational_classifier(Theta, x=x) for x in X])\n    return square_loss(Y, predictions)\n\nX, Y = make_moons(n_samples=200, shuffle=True, noise=0.1, random_state=None)\nnum_data = len(Y)\nnum_train = int(fraction_train * num_data)\nindex = np.random.permutation(range(num_data))\nfeats_train = X[index[:num_train]]\nY_train = Y[index[:num_train]]\nfeats_val = X[index[num_train:]]\nY_val = Y[index[num_train:]]\nX_train = X[index[:num_train]]\nX_val = X[index[num_train:]]\nopt = qml.AdamOptimizer()\n\nvar = Theta\na = []\nc = []\nl = []\ncnt = 0\n\nfor it in range(n_steps):\n    batch_index = np.random.randint(0, num_train, (batch_size,))\n    feats_train_batch = feats_train[batch_index]\n    Y_train_batch = Y_train[batch_index]\n    var = opt.step(lambda v: cost(v, feats_train_batch, Y_train_batch), var)\n    predictions_train = np.array([variational_classifier(var, x) for x in feats_train])\n    predictions_val = np.array([variational_classifier(var, x) for x in feats_val])\n    acc_train = accuracy(Y_train, predictions_train)\n    acc_val = accuracy(Y_val, predictions_val)\n    c.append(cost(var, X, Y))\n    a.append(acc_val)\n    l.append(square_loss(Y_val, predictions_val))\n    if it>0 and abs(a[it] - a[it-1])<1e-3:\n        cnt = cnt+1\n    else:\n        cnt = 0\n    if cnt >= 20 and it>=100:\n        break\n    print(\n        \"Iter: {:5d} | Loss: {:0.7f} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n        \"\".format(it + 1, square_loss(Y_val, predictions_val), cost(var, X, Y), acc_train, acc_val)\n    )1", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/9"}, "9": {"author": "sjahangiri", "date": "1594935249627", "content": "Hi @valeria,\nThe automatic differentiation framework (in this case, the autograd.numpy module) needs to see every step in the computation as differentiable. Your conditional check and assignment of integer in variational_classifier means that this particular step is non-differentiable. To avoid the warning you can train on the label probabilities, not the labels, by modifying variational_classifier as:\ndef variational_classifier(Theta, x):\n    params = Theta[0]\n    bias = Theta[1]\n    o0 = circuit_o0(params,x) + bias\n    o1 = circuit_o1(params,x) + bias\n    p0 = o0/(o0+o1)\n    return p0, 1-p0\n\nIn this case, square_loss and cost should be updated as well:\ndef square_loss(labels, prediction_probs):\n    loss = 0\n    for l, p in zip(labels, prediction_probs):\n        prediction = 0.0 * p[0] + 1.0 * p[1]  # 0*p[0] actually unnecessary, but kept for clarity\n        loss = loss + (l - prediction) ** 2\n    loss = loss / len(labels)\n    return loss\n\ndef cost(Theta, X, Y):\n    prediction_probs = np.array([variational_classifier(Theta, x=x) for x in X])  # only name changed here, to make more clear\n    return square_loss(Y, prediction_probs)\n\nThere\u2019s an illustrative related example here 7 for making the training loss a function of the label probabilities (this example uses logistic loss instead of square loss though).\nPlease let us know if you have any further questions.3", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/10"}, "10": {"author": "valeria", "date": "1595002670319", "content": "Thank you so much @sjahangiri! Your explanation was very clear and the warning disappeared - your help was so precious \nCheers,\nValeria1", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/11"}, "11": {"author": "Vineesha", "date": "1600271894891", "content": "Hi @valeria I did a similar correction to my variational_classifier function, I am returning only normalized p0 and adjusting square loss function accordingly:\ndef variational_classifier(weights, x=None):\n     p0= circuit_p0(weights,x=x)\n     p1= circuit_p1(weights,x=x)\n     normalization= p0+ p1+ 1e-10\n     output= p0/normalization\n     return output1", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/12"}, "12": {"author": "hilarlia", "date": "1616885817192", "content": "Hi @valeria @Vineesha,\nI am using the same code but getting a different error:\nCannot differentiate with respect to parameter(s) {44}\nI tried sjahangiri\u2019s recommendation but it doesn\u2019t work. How did you manage this? My code is below:\nimport pennylane as qml\nfrom pennylane import expval\nfrom pennylane import numpy as np\nfrom pennylane import AdamOptimizer\nimport sklearn.datasets\nfrom sklearn.datasets import make_moons\n%matplotlib inline    \n\ndev = qml.device(\"strawberryfields.fock\", wires=2, cutoff_dim=3, analytic=False)\n\n    np.random.seed(0)\n    batch_size = 5\n    n_steps = 500\n    n_layers = 4\n    Theta = 0.05*np.random.rand(n_layers,2,4)\n    fraction_train = 0.75\n    lambda_reg = 0.1\n\n    def layer(params,l): # The variational circuit layer\n        qml.Beamsplitter(params[l,0,0],params[l,1,0],wires=[0,1])\n        qml.Displacement(params[l,0,1],0.0,wires=0)\n        qml.Displacement(params[l,1,1],0.0,wires=1)\n        qml.QuadraticPhase(params[l,0,2],wires=0)\n        qml.QuadraticPhase(params[l,1,2],wires=1)\n        qml.CubicPhase(params[l,0,3],wires=0)\n        qml.CubicPhase(params[l,1,3],wires=1)\n\n    @qml.qnode(dev)\n    def circuit_o0(Theta, x): # Circuit to Calculate p(2,0)\n        qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n        for l in range(n_layers):\n            layer(Theta,l=l)\n        return qml.expval(op=qml.FockStateProjector(np.array([2,0]), wires=[0,1]))\n\n    @qml.qnode(dev)\n    def circuit_o1(params,x): # Circuit for Calculate p(0,2)\n        qml.templates.embeddings.SqueezingEmbedding(x,wires=[0,1],method='phase',c=1.5)\n        for l in range(n_layers):\n            layer(params,l=l)\n        return qml.expval(op=qml.FockStateProjector(np.array([0,2]), wires=[0,1]))\n\n    def variational_classifier(Theta, x): # Determine the probability: p(2,0)/(p(0,2)+p(2,0))\n        #params = Theta[0]\n        #bias = Theta[1]\n        o0 = circuit_o0(Theta,x=x) + 0.01\n        o1 = circuit_o1(Theta,x=x) + 0.01\n        p0 = o0/(o0+o1)\n        return p0, 1-p0\n\n    def square_loss(labels,prediction_probs): # Calculate square loss\n        loss = 0\n        print(prediction_probs)\n        for l, p in zip(labels, prediction_probs):\n            prediction = 0.0 * p[0] + 1.0 * p[1]\n            loss = loss - (l - p) ** 2\n        loss = loss / len(labels)\n        print(loss)\n        return loss\n\n    def accuracy(labels, predictions): # Calculate Accuracy\n        acc = 0\n        for l, p in zip(labels, predictions):\n            if abs(l - p) < 1e-5:\n                acc = acc + 1\n        acc = acc / len(labels)\n        return acc\n\n    def cost(Theta, X=None, Y=None): # Calculate cost using square loss\n        prediction_probs = np.array([variational_classifier(Theta, x=x) for x in X])\n        return square_loss(Y,prediction_probs)\n\n\n\n    # Extract dataset\n    X, Y = make_moons(n_samples=200, shuffle=True, noise=0.1, random_state=None)\n\n    num_data = len(Y) # Number of data\n\n    num_train = int(fraction_train * num_data) # Number of training samples\n\n    index = np.random.permutation(range(num_data)) # Permutation for shuffling\n    feats_train = X[index[:num_train]] # Features for training\n    Y_train = Y[index[:num_train]] # Labels of training features\n    feats_val = X[index[num_train:]] # Test features\n    Y_val = Y[index[num_train:]] # Labels of test features\n\n    opt = qml.AdamOptimizer() # I will use Adam Optimizer\n\n    var = Theta\n    a = []\n    c = []\n    l = []\n    cnt = 0\n\n    for it in range(n_steps):\n        \n        # Randomly select the batches\n        batch_index = np.random.randint(0, num_train, (batch_size,))\n        feats_train_batch = feats_train[batch_index]\n        Y_train_batch = Y_train[batch_index]\n        \n        # Optimize the theta\n        var = opt.step(lambda v: cost(v, X=feats_train_batch, Y=Y_train_batch), var)\n        \n        # Predict train and test samples\n        predictions_train = np.array([variational_classifier(var, x) for x in feats_train])\n        predictions_val = np.array([variational_classifier(var, x) for x in feats_val])\n        \n        # Calculate accuracies\n        acc_train = accuracy(Y_train, predictions_train)\n        acc_val = accuracy(Y_val, predictions_val)\n        \n        # Store cost, accuracy and square loss values\n        c.append(cost(var, X, Y))\n        a.append(acc_val)\n        l.append(square_loss(Y_val, predictions_val))\n        \n        # Break the loop after reaching convergence\n        if it>0 and abs(a[it] - a[it-1])<1e-3:\n            cnt = cnt+1\n        else:\n            cnt = 0\n        if cnt >= 20 and it>=100:\n            break\n            \n        # Print results\n        print(\n            \"Iter: {:5d} | Loss: {:0.7f} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n            \"\".format(it + 1, square_loss(Y_val, predictions_val), cost(var, X, Y), acc_train, acc_val)\n        )", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/13"}, "13": {"author": "nathan", "date": "1617037114878", "content": "Hi @hilarlia,\nWelcome to the community \nYour question was directed at other users, so I\u2019ll leave space for them to provide their thoughts, but I was looking into your code and was unable to reproduce the error you specified (I ended up with a different error).\nBoth PennyLane and Strawberry Fields are updated regularly with potentially breaking changes, and this thread is several months old. To help with solving any issues you are seeing, could you please post the output of qml.about() here?", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/14"}, "14": {"author": "hilarlia", "date": "1617039924837", "content": "Hi @nathan,\nNow I noticed I send the code here different. The line in the square loss function should be:\n loss = loss + (l - prediction) ** 2\nNow it gives that error.\nName: PennyLane Version: 0.14.1 Summary: PennyLane is a Python quantum machine learning library by Xanadu Inc. Home-page: https://github.com/XanaduAI/pennylane Author: None Author-email: None License: Apache License 2.0 Location: c:\\users\\ufkug\\.conda\\envs\\env\\lib\\site-packages Requires: toml, numpy, semantic-version, scipy, networkx, autograd, appdirs Required-by: PennyLane-SF Platform info:           Windows-10-10.0.19041-SP0 Python version:          3.8.3 Numpy version:           1.18.5 Scipy version:           1.5.1 Installed devices:\ndefault.gaussian (PennyLane-0.14.1)\ndefault.mixed (PennyLane-0.14.1)\ndefault.qubit (PennyLane-0.14.1)\ndefault.qubit.autograd (PennyLane-0.14.1)\ndefault.qubit.jax (PennyLane-0.14.1)\ndefault.qubit.tf (PennyLane-0.14.1)\ndefault.tensor (PennyLane-0.14.1)\ndefault.tensor.tf (PennyLane-0.14.1)\nstrawberryfields.fock (PennyLane-SF-0.14.0)\nstrawberryfields.gaussian (PennyLane-SF-0.14.0)\nstrawberryfields.gbs (PennyLane-SF-0.14.0)\nstrawberryfields.remote (PennyLane-SF-0.14.0)\nstrawberryfields.tf (PennyLane-SF-0.14.0)", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/15"}, "15": {"author": "nathan", "date": "1617116799075", "content": "Hi @hilarlia,\nThanks for the clarification. I was able to determine that the cause of the error message was the usage of FockStateProjector in your model. Specifically, this measurement operation takes an array parameter to indicate which Fock states to compute the expectation value of. By default, PennyLane assumes all array parameters should be differentiated, unless directly stated otherwise (this is similar to what TensorFlow does; in contrast PyTorch requires you to explicitly declare every trainable parameter manually).\nIn this case, you actually don\u2019t want to differentiate those specific arrays, since doing so doesn\u2019t make sense.\nYou will be able to get your code to run with the following modification:\nprojector20 = np.array([2, 0], requires_grad=False)\nprojector02 = np.array([0, 2], requires_grad=False)\n\nalong with updating your circuits to instead end with:\nreturn qml.expval(op=qml.FockStateProjector(projector20, wires=[0,1]))\n\nand\nreturn qml.expval(op=qml.FockStateProjector(projector02, wires=[0,1]))\n\nas applicable.\nNote that the reason the previous posters did not have this issue is that they were using an older version of PennyLane at the time.", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/16"}, "16": {"author": "Satanik_Mitra", "date": "1617189577633", "content": "Hi @Maria_Schuld @Vineesha   , I am a beginner in in the quantum machine learning and was going through the paper QML in feature hilbert space. At one point I am little confused, it will be great if either of you help in clarifying.\nIn the paper at page #7 the statement \u201cSince this probability depends on the displacement and squeezing intensity, it is better to define two probabilities, say p(n1 = 2; n2 = 0) and p(n1 = 0; n2 = 2), as\na one-hot encoded output vector (o0; o1).\u201d\nHere my doubt is why we are considering 2. Is it representing probability or photon count. I am bit confused. if you kindly give a brief explanation to it.\nThanks and Regards", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/17"}, "17": {"author": "nathan", "date": "1617205601493", "content": "Hi @Satanik_Mitra, welcome to the forum!\nSince it seems your question is independent of the above discussion, could you please create a new topic specifically for it? This will make it easier for future readers to find it 1", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/18"}, "18": {"author": "hilarlia", "date": "1617400115843", "content": "Thank you @nathan, it was really helpful, I don\u2019t get the error anymore! Sorry for late response, I forgot to answer at the moment.", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/19"}, "19": {"author": "hilarlia", "date": "1618184394618", "content": "Hi @Maria_Schuld,\nI was trying to reproduce the results in your paper, so I have a couple of questions since the accuracy always oscillates between interval 0.6-0.8. First, how do you implement l2 regularization in binary classification? Is it adding the Frobenius norm^2 of theta multiplied with a penalty term to the square loss? If so, could I ask what was the penalty term you set? The second question is how did you implement the optimizer? Is it TensorFlow\u2019s Stochastic Gradient Descent? How did you set the adaptive learning rate?\nBy the way sorry for bothering those questions. If answering those questions violate some kind of confidentiality, feel free to ignore my post.1 Reply", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/20"}, "20": {"author": "Maria_Schuld", "date": "1618248001422", "content": "Hey @hilarlia!\nYour questions are all good, I often regret that we didn\u2019t publish our code back then -that will hopefully never happen again.\nI had a look into the simulation, and yes, we used an l2 regulariser with penalty coefficient 0.01, as well as a standard square loss. Furthermore, we used PennyLane\u2019s AdagradOptimizer with initial step 0.01, batch size 5 and four layers.\nHope this helps? The experiment was not necessarily trying to propose a good classifier, but more a demonstration that the principle works, so I wouldn\u2019t put too much effort into reproducing it\u2026", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/21"}, "21": {"author": "hilarlia", "date": "1619977768740", "content": "Hi @Maria_Schuld, thank you very much for your earlier response. I want to reproduce some papers to see those working examples. I tried the according to what you\u2019ve mentioned, however I couldn\u2019t get good results. It was oscillating around 0.6. Could you point out where I am making the mistake?\ndev = qml.device(\"strawberryfields.fock\", wires=2, cutoff_dim=3, analytic=False)\n\nnp.random.seed(0)\nbatch_size = 5\nn_steps = 5000\nn_layers = 4\neta = 0.02\nTheta = (0.05*np.random.rand(n_layers,2,4),eta)\nfraction_train = 0.75\ndef layer(params,l): # The variational circuit layer\nqml.Beamsplitter(params[l,0,0],params[l,1,0],wires=[0,1])\nqml.Displacement(params[l,0,1],0.0,wires=0)\nqml.Displacement(params[l,1,1],0.0,wires=1)\nqml.QuadraticPhase(params[l,0,2],wires=0)\nqml.QuadraticPhase(params[l,1,2],wires=1)\nqml.CubicPhase(params[l,0,3],wires=0)\nqml.CubicPhase(params[l,1,3],wires=1)\n\n@qml.qnode(dev)\ndef circuit_o0(params, x): # Circuit to Calculate p(2,0)\nqml.templates.embeddings.SqueezingEmbedding(x,wires= \n[0,1],method='phase',c=1.5)\nfor l in range(n_layers):\n    layer(params,l)\nreturn\nqml.expval(qml.FockStateProjector(np.array([2,0]),requires_grad=Fal\nse), wires = [0,1]))\n\n@qml.qnode(dev)\ndef circuit_o1(params,x): # Circuit for Calculate p(0,2)\n    qml.templates.embeddings.SqueezingEmbedding(x,wires= \n[0,1],method='phase',c=1.5)\nfor l in range(n_layers):\n    layer(params,l)\nreturn\n\n\n\n\ndef variational_classifier(Theta, x): # Determine the probability: p(2,0)/(p(0,2)+p(2,0))\n    params = Theta[0]\n    bias = Theta[1]\n    o0 = circuit_o0(params,x) + bias\n    o1 = circuit_o1(params,x) + bias\n    p0 = o0/(o0+o1)\n    label = 0\n    if p0<0.5:\n        label = 1\n    norm = np.linalg.norm(Theta[0][0][0],ord=2)**2 + \n    np.linalg.norm(Theta[0][0][1],ord=2)**2 + np.linalg.norm(Theta[0][1][0],ord=2)**2 + np.linalg.norm(Theta[0][1][1],ord=2)**2 + np.linalg.norm(Theta[0][2][0],ord=2)**2 + np.linalg.norm(Theta[0][2][1],ord=2)**2 + np.linalg.norm(Theta[0][3][0],ord=2)**2 + np.linalg.norm(Theta[0][3][1],ord=2)**2\n    return p0, 1-p0,label, norm\n\ndef square_loss(labels,prediction_probs): # Calculate square loss\n    loss = 0\n    reg = 0.01\n    for l, p in zip(labels, prediction_probs):\n    loss = loss + (l-p[2])**2\n    loss = loss / len(labels)\n    loss = loss + p[3]*reg\n    return loss\n\ndef accuracy(labels, predictions): # Calculate Accuracy\n    acc = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p[2]) < 1e-5:\n            acc = acc + 1\n    acc = acc / len(labels)\n    return acc\n\ndef cost(Theta, X, Y): # Calculate cost using square loss\n    prediction_probs = np.array([variational_classifier(Theta, x=x) for x in X])\n    return square_loss(Y,prediction_probs)\n\n\n\n# Extract dataset\nX, Y = make_moons(n_samples=200, shuffle=True, noise=0.1, \nrandom_state=None)\n\nnum_data = len(Y) # Number of data\n\nnum_train = int(fraction_train * num_data) # Number of training samples\n\nindex = np.random.permutation(range(num_data)) # Permutation for shuffling\nfeats_train = X[index[:num_train]] # Features for training\nY_train = Y[index[:num_train]] # Labels of training features\nfeats_val = X[index[num_train:]] # Test features\nY_val = Y[index[num_train:]] # Labels of test features\n\nopt = qml.AdagradOptimizer() # I will use QNG Optimizer\nvar = Theta\na = []\nc = []\nl = []\ncnt = 0\n\nfor it in range(n_steps):\n    \n# Randomly select the batches\nbatch_index = np.random.randint(0, num_train, (batch_size,))\nfeats_train_batch = feats_train[batch_index]\nY_train_batch = Y_train[batch_index]\n    \n# Optimize the theta\nvar = opt.step(lambda v: cost(v, feats_train_batch, Y_train_batch), var)\n    \n# Predict train and test samples\npredictions_train = np.array([variational_classifier(var, x) for x in feats_train])\npredictions_val = np.array([variational_classifier(var, x) for x in feats_val])\n\n# Calculate accuracies\nacc_train = accuracy(Y_train, predictions_train)\nacc_val = accuracy(Y_val, predictions_val)\n        \n# Store cost, accuracy and square loss values\nc.append(cost(var, X, Y))\na.append(acc_val)\nl.append(square_loss(Y_val, predictions_val))\n        \n# Break the loop after reaching convergence\nif it>0 and abs(a[it] - a[it-1])<1e-3:\n    cnt = cnt+1\nelse:\n    cnt = 0\nif cnt >= 20 and it>=100:\n    break\n        \n# Print results\nprint(\n    \"Iter: {:5d} | Loss: {:0.7f} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n     \"\".format(it + 1, square_loss(Y_val, predictions_val), cost(var, X, Y), acc_train, acc_val)\n)", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/22"}, "22": {"author": "Maria_Schuld", "date": "1620047900703", "content": "Hey @hilarlia,\nAs far as I can see, the main settings are the same as in our experiment. But I am not sure I understand the regulariser, which is the third entry of the prediction p[3]. We used the inner product of the trainable variables weights (arranged as a vector) with itself, l2regularization = np.sum(np.inner(weights, weights)) [np is PennyLane\u2019s numpy version].\nI am not sure if this would make a difference?\nUsually, training QNNs is quite tricky, but I remember that in this case things were rather stable - since it is not so much different to a Gaussian kernel\u2026", "link": "https://discuss.pennylane.ai//t/quantum-machine-learning-in-feature-hilbert-spaces/374/23"}}