{"0": {"author": "Eleonora_Panini", "date": "1697658262963", "content": "This is the link of my notebook code, it is a quantum GAN for images:\n\n\ncolab.research.google.com\n\n\n\nGoogle Colaboratory 1\n\n\n\n\n\nThis is the error on google colab: I tried to set the max value of recursion limit and if I increase it over 10000, the session will crash.\nRecursionError                            Traceback (most recent call last)\n in <cell line: 1>()\n----> 1 discriminator = Discriminator().to(device)\n2 generator = PatchQuantumGenerator(n_generators).to(device)\n3\n4 # Binary cross entropy\n5 criterion = nn.BCELoss()\n2 frames\n\u2026 last 1 frames repeated, from the frame below \u2026\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n795     def _apply(self, fn):\n796         for module in self.children():\n \u2192 797             module._apply(fn)\n798\n799         def compute_should_use_set_data(tensor, tensor_applied):\nRecursionError: maximum recursion depth exceeded while calling a Python object", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1697672345739", "content": "Hi @Eleonora_Panini ,\nThank you for your question.\nI see you started from our demo on Quantum GANs 2 and modified some things. I would recommend trying to run the demo as-is and letting us know if you also run into this issue.\nFrom the error traceback I can see that the error arises from Torch. If the demo works but your code doesn\u2019t then I would recommend making a smaller version of this problem and testing to see if you get the same error.\nIf after reducing your dataset and minimizing the changes in the code with respect to the demo, you still have the same error please share your code again with us specifying every line that you changed with respect to the demo and we\u2019ll do our best to help you.\nI hope this helps you debug your problem.", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/2"}, "2": {"author": "Eleonora_Panini", "date": "1697721116970", "content": "This is the full code, I tried to reduce the dataset from 200 images RGB (100 for each classes) to 20 and the output have the same problem when I launch the training cell (RecursionError: maximum recursion depth exceeded while calling a Python object). I provide you the part of code that I modified in order to adapt the demo to my code/dataset: look at the respective comments on the code below. Do you have any idea about how to solve the problem?\nI appreciate your help, and thank you in advance.\nimport math\nimport random\nimport numpy as np\n#import pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n#import pennylane as qml\nimport sys\nimport os\nPytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n#%%\nimport pennylane as qml\n#%%\nSet the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nTHIS PART OF TEXT READ THE DATASET, RESIZE IMAGES 64X64 AND STORE IN ARRAY\n#\u2018class DigitsDataset(Dataset)\u2019 OF THE DEMO IS REPLACED BY THE FOLLOWING CODE:\nImgLocation=\u201cC:/Users/elyon/OneDrive/Desktop/Tesi/dataset/\u201d\nList image categories we are interested in\nCATEGORIES = set([\u201cDilbert\u201d,\u201cBoss\u201d])\nCreate a list to store image paths\nImagePaths=\nfor category in CATEGORIES:\nfor image in list(os.listdir(ImgLocation+category)):\nImagePaths=ImagePaths+[ImgLocation+category+\u201c/\u201d+image]\nLoad images and resize to 200x200\ndata_lowres=\nfor img in ImagePaths:\nimage = cv2.imread(img)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_lowres = cv2.resize(image, (64, 64))\ndata_lowres.append(image_lowres)\nConvert image data to numpy array and standardize values (divide by 255 since RGB values ranges from 0 to 255)\ndata_lowres = np.array(data_lowres, dtype=\u201cfloat32\u201d) / 255.0\ndata_mng=data_lowres.reshape(data_lowres.shape[0], 64, 64, 3)\nShow data shape\nprint(\"Shape of data_lowres: \", data_lowres.shape)\nprint(\"Shape of the scaled array: \", data_mng.shape)\n#%%\n#data = data_lowres.astype(np.float32).reshape(64, 64)\n**** LOADER DATASET\n#%%\nimage_size = 8  # Height / width of the square images\nbatch_size = 1\ndataloader = torch.utils.data.DataLoader(\ndata_mng, batch_size=batch_size, shuffle=True, drop_last=True\n)\nDISCRIMINATOR (the same of demo)\nclass Discriminator(nn.Module):\n\u201c\u201d\u201cFully connected classical discriminator\u201d\u201c\u201d\ndef __init__(self):\n    super().__init__()\n\n    self.model = nn.Sequential(\n        self.double() ,\n        # Inputs to first hidden layer (num_input_features -> 64)\n        nn.Linear(image_size * image_size, 64),\n        nn.ReLU(),\n        # First hidden layer (64 -> 16)\n        nn.Linear(64, 16),\n        nn.ReLU(),\n        # Second hidden layer (16 -> output)\n        nn.Linear(16, 1),\n        nn.Sigmoid(),\n    )\n\ndef forward(self, x):\n    return self.model(x)\n\n#%%\nquantum simulator is the same of demo\nQuantum variables\nn_qubits = 5  # Total number of qubits / N\nn_a_qubits = 1  # Number of ancillary qubits / N_A\nq_depth = 6  # Depth of the parameterised quantum circuit / D\nn_generators = 4  # Number of subgenerators for the patch method / N_G\nQuantum simulator\ndev = qml.device(\u201clightning.qubit\u201d, wires=n_qubits)\nEnable CUDA device if available\ndevice = torch.device(\u201ccuda:0\u201d if torch.cuda.is_available() else \u201ccpu\u201d)\n#%%\n@qml.qnode(dev, interface=\u201ctorch\u201d, diff_method=\u201cparameter-shift\u201d)\ndef quantum_circuit(noise, weights):\nweights = weights.reshape(q_depth, n_qubits)\n\n# Initialise latent vectors\nfor i in range(n_qubits):\n    qml.RY(noise[i], wires=i)\n\n# Repeated layer\nfor i in range(q_depth):\n    # Parameterised layer\n    for y in range(n_qubits):\n        qml.RY(weights[i][y], wires=y)\n\n    # Control Z gates\n    for y in range(n_qubits - 1):\n        qml.CZ(wires=[y, y + 1])\n\nreturn qml.probs(wires=list(range(n_qubits)))\n\ndef partial_measure(noise, weights):\n# Non-linear Transform\nprobs = quantum_circuit(noise, weights)\nprobsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\nprobsgiven0 /= torch.sum(probs)\n# Post-Processing\nprobsgiven = probsgiven0 / torch.max(probsgiven0)\nreturn probsgiven\n\n#%%\nGENERATOR the same of demo\nclass PatchQuantumGenerator(nn.Module):\n\u201c\u201d\u201cQuantum generator class for the patch method\u201d\u201c\u201d\ndef __init__(self, n_generators, q_delta=1):\n    \"\"\"\n    Args:\n        n_generators (int): Number of sub-generators to be used in the patch method.\n        q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n    \"\"\"\n\n    super().__init__()\n\n    self.q_params = nn.ParameterList(\n        [\n            nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n            for _ in range(n_generators)\n        ]\n    )\n    self.n_generators = n_generators\n\ndef forward(self, x):\n    # Size of each sub-generator output\n    patch_size = 2 ** (n_qubits - n_a_qubits)\n\n    # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n    images = torch.Tensor(x.size(0), 0).to(device)\n\n    # Iterate over all sub-generators\n    for params in self.q_params:\n\n        # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n        patches = torch.Tensor(0, patch_size).to(device)\n        for elem in x:\n            q_out = partial_measure(elem, params).float().unsqueeze(0)\n            patches = torch.cat((patches, q_out))\n\n        # Each batch of patches is concatenated with each other to create a batch of images\n        images = torch.cat((images, patches), 1)\n\n    return images\n\n#%%\nlrG = 0.3  # Learning rate for the generator\nlrD = 0.01  # Learning rate for the discriminator\nnum_iter = 500  # Number of training iterations\n#sys.setrecursionlimit(10000)\nprint(sys.getrecursionlimit())\n#%%\nTRAINING->when I run this cell I get the error\n#I replaced -1 with 3 because images are RGB colour\ndiscriminator = Discriminator().to(device)\ngenerator = PatchQuantumGenerator(n_generators).to(device)\nBinary cross entropy\ncriterion = nn.BCELoss()\nOptimisers\noptD = optim.SGD(discriminator.parameters(), lr=lrD)\noptG = optim.SGD(generator.parameters(), lr=lrG)\nreal_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\nFixed noise allows us to visually track the generated images throughout training\nfixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\nIteration counter\ncounter = 0\nCollect images for plotting later\nresults = \nwhile True:\n#for i (data, _) in enumerate(dataloader):\nfor data in (dataloader):\n# Data for training the discriminator\ndata = data.reshape(3, image_size * image_size)\nreal_data = data.to(device)\n    # Noise follwing a uniform distribution in range [0,pi/2)\n    noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n    fake_data = generator(noise)\n\n    # Training the discriminator\n    discriminator.zero_grad()\n    outD_real = discriminator(real_data).view(3)\n    outD_fake = discriminator(fake_data.detach()).view(3)\n\n    errD_real = criterion(outD_real, real_labels)\n    errD_fake = criterion(outD_fake, fake_labels)\n    # Propagate gradients\n    errD_real.backward()\n    errD_fake.backward()\n\n    errD = errD_real + errD_fake\n    optD.step()\n\n    # Training the generator\n    generator.zero_grad()\n    outD_fake = discriminator(fake_data).view(3)\n    errG = criterion(outD_fake, real_labels)\n    errG.backward()\n    optG.step()\n\n    counter += 1\n\n    # Show loss values\n    if counter % 10 == 0:\n        print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n        test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n\n        # Save images every 50 iterations\n        if counter % 50 == 0:\n            results.append(test_images)\n\n    if counter == num_iter:\n        break\nif counter == num_iter:\n    break\n\n#%%\nPLOT IMAGES\n\u2018\u2019\u2019\nfig = plt.figure(figsize=(10, 5))\nouter = gridspec.GridSpec(5, 2, wspace=0.1)\nfor i, images in enumerate(results):\ninner = gridspec.GridSpecFromSubplotSpec(1, images.size(0),\nsubplot_spec=outer[i])\nimages = torch.squeeze(images, dim=1)\nfor j, im in enumerate(images):\n\n    ax = plt.Subplot(fig, inner[j])\n    ax.imshow(im.numpy(), cmap=\"gray\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if j==0:\n        ax.set_title(f'Iteration {50+i*50}', loc='left')\n    fig.add_subplot(ax)\n\nplt.show()\n\u2018\u2019\u2019", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/3"}, "3": {"author": "CatalinaAlbornoz", "date": "1697771753962", "content": "Hi @Eleonora_Panini, thank you for sharing your code. Unfortunately it\u2019s very hard to read as it is. Can you please use the <> symbol here in the forum and then paste your code inside the code block? Thanks! Let me know if this isn\u2019t clear for you and I can try to explain it differently.2 Replies", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/4"}, "4": {"author": "Eleonora_Panini", "date": "1697786635720", "content": "\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nimport pennylane as qml\n\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n#THIS PART OF TEXT READ THE DATASET, RESIZE IMAGES 64X64 AND STORE IN ARRAY\n#\u2018class DigitsDataset(Dataset)\u2019 OF THE DEMO IS REPLACED BY THE FOLLOWING CODE:\n#I tried also with reduced dataset of 20 images instead of 200\nImgLocation=\u201cC:/Users/elyon/OneDrive/Desktop/Tesi/dataset/\u201d\nCATEGORIES = set([\u201cDilbert\u201d,\u201cBoss\u201d])\nfor category in CATEGORIES:\n    for image in list(os.listdir(ImgLocation+category)):\n       ImagePaths=ImagePaths+[ImgLocation+category+\u201c/\u201d+image]\ndata_lowres=[ ]\nfor img in ImagePaths:\n     image = cv2.imread(img)\n     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n     image_lowres = cv2.resize(image, (64, 64))\n     data_lowres.append(image_lowres)\ndata_lowres= np.array(data_lowres, dtype=\u201cfloat32\u201d) / 255.0\ndata_mng=data_lowres.reshape(data_lowres.shape[0], 64, 64, 3)\n#the output shape is : 20 (n. of images),64,64,3 (RGB)\n#LOADER DATASET\nimage_size = 8 # Height / width of the square images\nbatch_size = 1 #I tried also changing the batch size\ndataloader = torch.utils.data.DataLoader(\ndata_mng, batch_size=batch_size, shuffle=True, drop_last=True\n)\n#DISCRIMINATOR (the same of demo)\nclass Discriminator(nn.Module):\n\u201c\u201d\u201cFully connected classical discriminator\u201d\u201c\u201d\ndef __init__(self):\n    super().__init__()\n\n    self.model = nn.Sequential(\n        self.double() ,\n        # Inputs to first hidden layer (num_input_features -> 64)\n        nn.Linear(image_size * image_size, 64),\n        nn.ReLU(),\n        # First hidden layer (64 -> 16)\n        nn.Linear(64, 16),\n        nn.ReLU(),\n        # Second hidden layer (16 -> output)\n        nn.Linear(16, 1),\n        nn.Sigmoid(),\n    )\n\ndef forward(self, x):\n    return self.model(x)\n#quantum simulator is the same of demo\nn_qubits = 5 # Total number of qubits / N\nn_a_qubits = 1 # Number of ancillary qubits / N_A\nq_depth = 6 # Depth of the parameterised quantum circuit / D\nn_generators = 4 # Number of subgenerators for the patch method / N_G\ndev = qml.device(\u201clightning.qubit\u201d, wires=n_qubits)\ndevice = torch.device(\u201ccuda:0\u201d if torch.cuda.is_available() else \u201ccpu\u201d)\n\n@qml.qnode(dev, interface=\u201ctorch\u201d, diff_method=\u201cparameter-shift\u201d)\ndef quantum_circuit(noise, weights):\n    weights = weights.reshape(q_depth, n_qubits)\n\n# Initialise latent vectors\nfor i in range(n_qubits):\n    qml.RY(noise[i], wires=i)\n\n# Repeated layer\nfor i in range(q_depth):\n    # Parameterised layer\n    for y in range(n_qubits):\n        qml.RY(weights[i][y], wires=y)\n\n    # Control Z gates\n    for y in range(n_qubits - 1):\n        qml.CZ(wires=[y, y + 1])\n\nreturn qml.probs(wires=list(range(n_qubits)))\ndef partial_measure(noise, weights):\n# Non-linear Transform\nprobs = quantum_circuit(noise, weights)\nprobsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\nprobsgiven0 /= torch.sum(probs)\n# Post-Processing\nprobsgiven = probsgiven0 / torch.max(probsgiven0)\nreturn probsgiven\n#GENERATOR the same of demo\nclass PatchQuantumGenerator(nn.Module):\n\u201c\u201d\u201cQuantum generator class for the patch method\u201d\u201c\u201d\ndef __init__(self, n_generators, q_delta=1):\n    \"\"\"\n    Args:\n        n_generators (int): Number of sub-generators to be used in the patch method.\n        q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n    \"\"\"\n\n    super().__init__()\n\n    self.q_params = nn.ParameterList(\n        [\n            nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n            for _ in range(n_generators)\n        ]\n    )\n    self.n_generators = n_generators\n\ndef forward(self, x):\n    # Size of each sub-generator output\n    patch_size = 2 ** (n_qubits - n_a_qubits)\n\n    # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n    images = torch.Tensor(x.size(0), 0).to(device)\n\n    # Iterate over all sub-generators\n    for params in self.q_params:\n\n        # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n        patches = torch.Tensor(0, patch_size).to(device)\n        for elem in x:\n            q_out = partial_measure(elem, params).float().unsqueeze(0)\n            patches = torch.cat((patches, q_out))\n\n        # Each batch of patches is concatenated with each other to create a batch of images\n        images = torch.cat((images, patches), 1)\n\n    return images\nlrG = 0.3 # Learning rate for the generator\nlrD = 0.01 # Learning rate for the discriminator\nnum_iter = 500 # Number of training iterations\n\n#TRAINING->when I run this cell I get the error\n#I replaced -1 with 3 because images are RGB colour(I tried also with -1 but it get the same error)\ndiscriminator = Discriminator().to(device)\ngenerator = PatchQuantumGenerator(n_generators).to(device)\ncriterion = nn.BCELoss()\noptD=optim.SGD(discriminator.parameters(), lr=lrD)\noptG = optim.SGD(generator.parameters(), lr=lrG)\n\nreal_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\nfixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\ncounter = 0\nresults=[]\nwhile True:\n#for i (data, _) in enumerate(dataloader):\nfor data in (dataloader):\n# Data for training the discriminator\ndata = data.reshape(3, image_size * image_size)\nreal_data = data.to(device)\n# Noise follwing a uniform distribution in range [0,pi/2)\n    noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n    fake_data = generator(noise)\n\n    # Training the discriminator\n    discriminator.zero_grad()\n    outD_real = discriminator(real_data).view(3)\n    outD_fake = discriminator(fake_data.detach()).view(3)\n\n    errD_real = criterion(outD_real, real_labels)\n    errD_fake = criterion(outD_fake, fake_labels)\n    # Propagate gradients\n    errD_real.backward()\n    errD_fake.backward()\n\n    errD = errD_real + errD_fake\n    optD.step()\n\n    # Training the generator\n    generator.zero_grad()\n    outD_fake = discriminator(fake_data).view(3)\n    errG = criterion(outD_fake, real_labels)\n    errG.backward()\n    optG.step()\n\n    counter += 1\n\n    # Show loss values\n    if counter % 10 == 0:\n        print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n        test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n\n        # Save images every 50 iterations\n        if counter % 50 == 0:\n            results.append(test_images)\n\n    if counter == num_iter:\n        break\nif counter == num_iter:\n    break\n\n#PLOT IMAGES\nfig = plt.figure(figsize=(10, 5))\nouter = gridspec.GridSpec(5, 2, wspace=0.1)\n\nfor i, images in enumerate(results):\ninner = gridspec.GridSpecFromSubplotSpec(1, images.size(0),\nsubplot_spec=outer[i])\nimages = torch.squeeze(images, dim=1)\nfor j, im in enumerate(images):\n\n    ax = plt.Subplot(fig, inner[j])\n    ax.imshow(im.numpy(), cmap=\"gray\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if j==0:\n        ax.set_title(f'Iteration {50+i*50}', loc='left')\n    fig.add_subplot(ax)\nplt.show()\n", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/5"}, "5": {"author": "Eleonora_Panini", "date": "1697992238451", "content": "Hi @CatalinaAlbornoz  I pasted my code in the previous post using your suggestions. The pennylane demo of quantum GAN works normally, meanwhile the quantum demo modified for my dataset does not work: during the training cell I get the error: (RecursionError: maximum recursion depth exceeded while calling a Python object) or stack overflow. Can you help me? Thank you", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1698102985196", "content": "Hi @Eleonora_Panini,\nI can\u2019t run your code because I don\u2019t have access to your data, however I think the issue lies in the size of your images. In the code you have image_size = 8, however I can see that the shape of your images is larger. I don\u2019t know if your images make any sense if they get reduced to this size but at least you can try to see if this lets you run the code with no errors.\nI hope this helps!2 Replies", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/7"}, "7": {"author": "Eleonora_Panini", "date": "1698119451180", "content": "\u2018\u2019\u2019\nHI @CatalinaAlbornoz, I have just also tried reducing the images size to 8x8 and not 64x64, but this is not the problem.I get you access to my notebook in order to try runnin my code. See the link in previuous answer.\nThank you in advance for your help\nEleonora\n\u2018\u2019\u2019", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1698269565916", "content": "Hi @Eleonora_Panini ,\nYou shared your code but the images are in your own Google Drive so I can\u2019t run the code without the images.\nI don\u2019t think the error magically disappeared. I think it disappeared because you set the new size of the images to 8.\nIndeed it\u2019s normal that you cannot do good training on 8x8 images unless the images still look different after this reduction, as is the case with MNIST. If you need to resize the images to a different size, eg:16. I would recommend that you resize them and visualize them to see that they still resemble the original image and not just a bunch of coloured points.\nYou can look for the different parts of the code that have the value \u201c8\u201d and change them to \u201cimage_size\u201d. You should set the variable image_size in the code to be the size of the images, eg. 16. At the moment your code says image_size = 8.\nSomething else that I noticed is that you have diff_method=\"parameter-shift\". This will make your code slower. I would avoid specifying this unless you specifically want this differentiation method for some reason.\nThe code is made to work for the DigitsDataset. I would recommend that you keep all dimensions as specified in the demo and start by running the original demo with a modified image_size so that you can figure out all of the places where you need to modify this information. Then you can look into modifying other aspects one by one until you get a full understanding of how the code works and how it can be modified. I understand that this can take some trial and error and time but with some work and patience you should be able to modify some parameters. There\u2019s no guarantee that this will work for your specific dataset though, especially if you need a very big image size you will need to increase the number of qubits you\u2019re using and you may hit a limit from your current device.\nLet me know if this helps and if you get stuck again.\nI hope you can get this working! ", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/9"}, "9": {"author": "CatalinaAlbornoz", "date": "1698269617318", "content": "By the way, we have a very small survey for PennyLane v0.32, and it would be awesome if you\u2019d give us some feedback and tell us about your needs. Thank you! 2 Replies", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/10"}, "10": {"author": "CatalinaAlbornoz", "date": "1698843996805", "content": "Hi @Eleonora_Panini , I\u2019m glad you managed to solve your previous issue!\n13 qubits is something that you should be able to run on a laptop. I\u2019m not sure why Colab is giving you trouble. Have you tried running your code on a laptop?\nTo install PennyLane locally it\u2019s strongly recommended that you create a new virtual environment to prevent you from having any installation issues. You can create a virtual environment with Conda and install PennyLane as follows:\n\nInstall Anaconda following the instructions here.\nOpen your terminal (mac) or command line (Windows).\nCreate a new Conda environment with: conda create --name\n<name_of_your_environment> python=3.10\nActivate the environment with: conda activate <name_of_your_environment>\nInstall PennyLane with: python -m pip install pennylane\nInstall other useful packages with: python -m pip install pennylane\njupyter\n\nNote that you will be installing 2 packages here: PennyLane, and Jupyter. Also, note that where it says <name_of_your_environment> you can choose any name that you want. After this you can write jupyter notebook in your terminal/command line and you will be ready to create programs using PennyLane.\nYou can also watch the video here for detailed PennyLane installation instructions.1 Reply", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/11"}, "11": {"author": "CatalinaAlbornoz", "date": "1698844364252", "content": "Regarding your GPU problem, could you please post the output of qml.about()? Did you make any changes to your code?\nGoogle Colab offers free access to a GPU but only in limited amounts so if you want something that will run on a GPU for many hours you will need to look at buying a GPU or buying a Pro version of a service that gives you access to one.\nMake sure that you install and test everything as recommended here to rule out any installation or dependency issues.\nIf this doesn\u2019t fix your problem please post your full error message.1 Reply", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/12"}, "12": {"author": "Eleonora_Panini", "date": "1698858640779", "content": "!pip install pennylane custatevec-cu11 pennylane-lightning-gpu\n# Library imports\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport pennylane as qml\n\n# Pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set the random seed for reproducibility\nseed = 999\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 32\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 13\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 100\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\nimport torchvision.datasets as dset\nImgLocation='/content/drive/MyDrive/Colab Notebooks/Subset_Dil_Bos/'\n\ndataset = dset.ImageFolder(root=ImgLocation,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nclass Discriminator(nn.Module):\n    \"\"\"Fully connected classical discriminator\"\"\"\n\n    def __init__(self,ngpu):\n        super().__init__()\n        self.ngpu = ngpu\n        self.model = nn.Sequential(\n            # Inputs to first hidden layer (num_input_features -> 64)\n            nn.Linear(3*image_size * image_size, 64,device=device),\n            nn.ReLU(),\n            nn.Linear(64, 64,device=device),\n            nn.ReLU(),\n            # First hidden layer (64 -> 16)\n            nn.Linear(64, 16,device=device),\n            nn.ReLU(),\n            # Second hidden layer (16 -> output)\n            nn.Linear(16, 1,device=device),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\ndiscriminator = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\ndiscriminator.apply(weights_init)\n\n# Print the model\nprint(discriminator)\n\n# Quantum variables\nn_qubits = 13  # Total number of qubits / N\nn_a_qubits = 2  # Number of ancillary qubits / N_A\nq_depth = 6  # Depth of the parameterised quantum circuit / D\nn_generators = 6  # Number of subgenerators for the patch method / N_G\n\ndev = qml.device(\"lightning.gpu\", wires=n_qubits)\n@qml.qnode(dev, interface=\"torch\") #, diff_method=\"parameter-shift\"\ndef quantum_circuit(noise, weights):\n\n    weights = weights.reshape(q_depth, n_qubits)\n\n    # Initialise latent vectors\n    for i in range(n_qubits):\n        qml.RY(noise[i], wires=i)\n\n    # Repeated layer\n    for i in range(q_depth):\n        # Parameterised layer\n        for y in range(n_qubits):\n            qml.RY(weights[i][y], wires=y)\n\n        # Control Z gates\n        for y in range(n_qubits - 1):\n            qml.CZ(wires=[y, y + 1])\n\n    return qml.probs(wires=list(range(n_qubits)))\n\n\ndef partial_measure(noise, weights):\n    # Non-linear Transform\n    probs = quantum_circuit(noise, weights)\n    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\n    probsgiven0 /= torch.sum(probs)\n\n    # Post-Processing\n    probsgiven = probsgiven0 / torch.max(probsgiven0)\n    return probsgiven\n\n\nclass PatchQuantumGenerator(nn.Module):\n    \"\"\"Quantum generator class for the patch method\"\"\"\n\n    def __init__(self, n_generators, q_delta=1):\n        \"\"\"\n        Args:\n            n_generators (int): Number of sub-generators to be used in the patch method.\n            q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n        \"\"\"\n\n        super().__init__()\n\n        self.q_params = nn.ParameterList(\n            [\n                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n                for _ in range(n_generators)\n            ]\n        )\n        self.n_generators = n_generators\n\n    def forward(self, x):\n        # Size of each sub-generator output\n        patch_size = 2 ** (n_qubits - n_a_qubits)\n\n        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n        images = torch.Tensor(x.size(0), 0).to(device)\n\n        # Iterate over all sub-generators\n        for params in self.q_params:\n\n            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n            patches = torch.Tensor(0, patch_size).to(device)\n            for elem in x:\n                q_out = partial_measure(elem, params).float().unsqueeze(0)\n                patches = torch.cat((patches, q_out))\n\n            # Each batch of patches is concatenated with each other to create a batch of images\n            images = torch.cat((images, patches), 1)\n\n        return images\n\nlrG = 0.3  # Learning rate for the generator\nlrD = 0.01  # Learning rate for the discriminator\nnum_iter = 100# Number of training iterations\n\ndiscriminator = Discriminator(ngpu).to(device)\n\ngenerator = PatchQuantumGenerator(n_generators).to(device)\n\n\n# Binary cross entropy\ncriterion = nn.BCELoss()\n\n# Optimisers\noptD = optim.SGD(discriminator.parameters(), lr=lrD)\noptG = optim.SGD(generator.parameters(), lr=lrG)\n\nreal_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n\n# Fixed noise allows us to visually track the generated images throughout training\nfixed_noise = torch.rand(image_size, n_qubits, device=device) * math.pi / 2\n\n\n\n# Iteration counter\ncounter = 0\n\n# Collect images for plotting later\nresults = []\n\nwhile True:\n    for i, (data, _) in enumerate(dataloader):\n\n\n        # Data for training the discriminator\n        data = data.reshape( -1,image_size * image_size*3) #data=(32,12288)\n\n        real_data = data.to(device) #real_data=(32,12288)\n\n\n\n        # Training the discriminator\n        # Noise follwing a uniform distribution in range [0,pi/2)\n        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2 #noise=(32,13)\n        fake_data = generator(noise)  #fake_data=(32,12288)\n\n        discriminator.zero_grad()\n        outD_real = discriminator(real_data).view(-1) #(outD_real=32)\n\n        errD_real = criterion(outD_real, real_labels)  #(criterion(32, 32))\n        \n        outD_fake = discriminator(fake_data.detach()).view(-1) #(outD_fake=32)\n\n        errD_fake = criterion(outD_fake, fake_labels)  #(criterion(32,32))\n        # Propagate gradients\n        errD_real.backward()\n        errD_fake.backward()\n\n        errD = errD_real + errD_fake\n        optD.step()\n\n        # Training the generator\n        generator.zero_grad()\n\n        outD_fake = discriminator(fake_data).view(-1)  #outD_fake=32\n\n        errG = criterion(outD_fake, real_labels) #criterion(32,32)\n        errG.backward()\n        optG.step()\n\n        counter += 1\n\n        # Show loss values\n        if counter % 10 == 0:\n            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n            test_images = generator(fixed_noise).view(image_size,3,image_size,image_size).cpu().detach()\n\n            # Save images every 50 iterations\n            if counter % 50 == 0:\n                results.append(test_images)\n\n        if counter == num_iter:\n            break\n    if counter == num_iter:\n        break\n\n\nThe output of qml.about():\nName: PennyLane\nVersion: 0.33.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: GitHub - PennyLaneAI/pennylane: PennyLane is a cross-platform Python library for differentiable programming of quantum computers. Train a quantum computer the same way as a neural network.\nAuthor:\nAuthor-email:\nLicense: Apache License 2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml, typing-extensions\nRequired-by: PennyLane-Lightning, PennyLane-Lightning-GPU\nPlatform info:           Linux-5.15.120\u00b1x86_64-with-glibc2.35\nPython version:          3.10.12\nNumpy version:           1.23.5\nScipy version:           1.11.3\nInstalled devices:\nI tried with lighting gpu as you suggested and he device used is: cuda:\ndev = qml.device(\u201clightning.gpu\u201d, wires=n_qubits)\nLightning GPU PennyLane plugin: [No binaries found - Fallback: default.qubit]\nShort name: lightning.gpu\nPackage: pennylane_lightning\nPlugin version: 0.33.0\nAuthor: Xanadu Inc.\nWires: 13\nShots: None\nWhen I select lightning_gpu I get this warning:\n/usr/local/lib/python3.10/dist-packages/pennylane_lightning/lightning_gpu/lightning_gpu.py:958: UserWarning:\n\"Pre-compiled binaries for lightning.gpu are not available. Falling back to \"\n\"using the Python-based default.qubit implementation. To manually compile from \"\n\"source, follow the instructions at \"\n\u201chttps://pennylane-lightning.readthedocs.io/en/latest/installation.html.\u201d,  warn(\nThe code still have the same problem, it is really slow and at a certain point crash the RAM or It appears an error of different size between outDreal and real_labels in this part of code:\nerrD_real = criterion(outD_real, real_labels)\nbut I check running only a part of the training code and the size is correct (32) for each variables, indeed the code print errD_real correctly each iteration of \u2018for\u2019 cycle. So I think this is a problem of RAM, at some point does not compute the operation in the right way.\nWhat can I do in order to run the code and get the output?", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/13"}, "13": {"author": "CatalinaAlbornoz", "date": "1698899172923", "content": "Hi @Eleonora_Panini ,\nThanks for reporting the results on your laptop. If you could put your dataset on a GitHub repository and share it with me that would be very helpful because the problem seems intrinsically linked to the data you\u2019re using.\nGiven your error there may indeed be a RAM problem.\nSomething you can try is circuit cutting. Here is a demo on this topic and these are the docs. I would recommend using the automatic cutter, which requires installing the KaHyPar library. You can pip install kahypar on Mac or Linux with Python=3.10.\nHere\u2019s an example code on how to use the automatic cutter. Note that we have a 3-qubit circuit running on a 2-qubit device. Depending on the specific architecture of your circuit this can help you with memory issues, although it may increase the time it takes to run. I tested this code on Google Colab and it works nicely. Remember to !pip install pennylane kahypar.\nimport pennylane as qml\nfrom pennylane import numpy as pnp\n\ndev = qml.device(\"lightning.qubit\", wires=2)\n\n@qml.qnode(dev)\ndef circuit(x):\n    qml.RX(x, wires=0)\n    qml.RY(0.9, wires=1)\n    qml.RX(0.3, wires=2)\n\n    qml.CZ(wires=[0, 1])\n    qml.RY(-0.4, wires=0)\n\n    qml.CZ(wires=[1, 2])\n\n    return qml.expval(qml.pauli.string_to_pauli_word(\"ZZZ\"))\n\ncut_circuit = qml.cut_circuit(circuit, auto_cutter=True)\n\nx = pnp.array(0.531, requires_grad=True)\n\ncut_circuit(x)\n\nYou can try testing this on the original version of the GAN demo, which you know that it works. If everything works there, then you can try implementing it with your own version of the GAN and seeing if your memory use is reduced. I would recommend testing with a 7-qubit lightning.qubit device.\nThe GPU issue that you\u2019re seeing might indicate that you used all of the GPU resources that Google Colab allocates for you so you should try running your problem on CPU instead.\nI hope this helps!3 Replies", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/14"}, "14": {"author": "Eleonora_Panini", "date": "1699266206117", "content": "Hi @CatalinaAlbarnoz, can you help me to understand what is the right structure for the cutting circuit adapted to my model? Should I reduce the 13 qubits circuits cutting to 7 qubits?\nThe following architecture is correct? I inserted more RX abd RY blocks until 13 like the number of qubits\u2026but I don\u2019t understand how to implement this in order to cut the circuit in 7 qubits.\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef circuit(x):\n    qml.RX(x, wires=0)\n    qml.RY(0.9, wires=1)\n    qml.RX(0.3, wires=2)\n    qml.CZ(wires=[0, 1])\n    qml.RY(-0.4, wires=0)\n    qml.CZ(wires=[1, 2])\n    qml.RY(0.9, wires=3)\n    qml.CZ(wires=[2, 3])\n    qml.RX(0.3, wires=4)\n    qml.CZ(wires=[3, 4])\n    qml.RY(0.9, wires=5)\n    qml.CZ(wires=[4, 5])\n    qml.RX(0.3, wires=6)\n    qml.CZ(wires=[5, 6])\n    qml.RY(0.9, wires=7)\n    qml.CZ(wires=[6, 7])\n    qml.RX(0.3, wires=8)\n    qml.CZ(wires=[7, 8])\n    qml.RY(0.9, wires=9)\n    qml.CZ(wires=[8, 9])\n    qml.RX(0.3, wires=10)\n    qml.CZ(wires=[9, 10])\n    qml.RY(0.9, wires=11)\n    qml.CZ(wires=[10, 11])\n    qml.RX(0.3, wires=12)\n    qml.CZ(wires=[11, 12])\n    return qml.expval(qml.pauli.string_to_pauli_word(\"ZZZZZZZZZZZZZ\"))\n\ncut_circuit = qml.cut_circuit(circuit, auto_cutter=True)\n\nx = pnp.array(0.531, requires_grad=True)\n\ncut_circuit(x)\n", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/15"}, "15": {"author": "CatalinaAlbornoz", "date": "1699328115451", "content": "Hi @Eleonora_Panini ,\nGreat question!\nThe idea behind circuit cutting is simply that you keep your circuit as-is but then run it on smaller devices. The circuit is then run more times, but needing less RAM each time.\nI looked at the circuit in the Quantum GANs demo and this is actually not a good one for circuit cutting since it has a lot of entanglement between qubits. However, below is a small example of circuit cutting on a similar circuit:\nimport torch.nn as nn\nimport pennylane as qml\nimport torch\nimport math\n\n# We define our constants\nn_qubits = 8\nq_depth = 2 \nn_generators = 1\nq_delta = 1\nbatch_size = 1\n\n# We define our CUDA device (if available)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# We define our quantum function (without a decorator)\ndef quantum_function(noise, weights):\n  weights = weights.reshape(q_depth, n_qubits)\n  # Initialise latent vectors\n  for i in range(n_qubits):\n    qml.RY(noise[i], wires=i)\n\n  # Repeated layer\n  for i in range(q_depth):\n    # Parameterised layer\n    for y in range(n_qubits):\n      qml.RY(weights[i][y], wires=y)\n\n    # Control Z gates\n    for y in range(n_qubits - 1):\n      qml.CZ(wires=[y, y + 1])\n\n  return qml.expval(qml.PauliZ(0))\n\n# We define our parameters\nparams = nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\nnoise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n\n# We create a device with the number of qubits required to run our circuit\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n# We create a QNode\nquantum_circuit = qml.QNode(quantum_function, dev)\n\n# We run the circuit for a specific value of the noise and the parameters just to test\noutput = quantum_circuit(noise[0], params)\nprint(output)\nqml.draw_mpl(quantum_circuit)(noise[0], params);\n\nBelow is where the circuit cutting part actually starts\n\"\"\"\nNow let's imagine that we only have access to a 5-qubit device instead of the 8-qubit device \nthat we need in order to run the previous circuit \n\"\"\"\n# We create a 5-qubit device\ndev_small = qml.device(\"lightning.qubit\", wires=5)\n\n# We create a QNode with the original quantum function but with the smaller device\nquantum_circuit2 = qml.QNode(quantum_function, dev_small)\n\n# We use our circuit cutter in \"auto_cutter\" mode\ncut_circuit = qml.cut_circuit(quantum_circuit2, auto_cutter=True)\n\n# We run the cut circuit for a specific value of the noise and the parameters just to test\noutput = cut_circuit(noise[0], params)\nprint(output)\nqml.draw_mpl(cut_circuit)(noise[0], params);\n\nNotice that circuit cutting is not something that should be used in every situation, but it sometimes can help you. In this case, for the original quantumGAN circuit it\u2019s not a great help, so you may instead want to reduce the depth of your circuit to reduce your RAM issues.\nI hope this helps!1", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/16"}, "16": {"author": "CatalinaAlbornoz", "date": "1699408175997", "content": "Hi @Eleonora_Panini , did you modify anything in the code I shared?\nCan you please post the output of qml.about() and your full traceback error? Thanks!2 Replies", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/17"}, "17": {"author": "CatalinaAlbornoz", "date": "1699492210616", "content": "Hi @Eleonora_Panini ,\nI don\u2019t know why your system isn\u2019t being able to handle 13 qubits and 6 generators. Can you please post your full error traceback?\nYes, it\u2019s ok that the drawing is the same because the drawing represents the circuit you want to run, not all of the pieces that are actually run on the device. You can think of this as part of the compilation kind of, which isn\u2019t visible when you draw the original circuit you want to run.", "link": "https://discuss.pennylane.ai//t/problem-on-running-quantumgan/3569/18"}}