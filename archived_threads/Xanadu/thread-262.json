{"0": {"author": "QuantumMan", "date": "1690827397690", "content": "Hello! If applicable, put your complete code example down below. Make sure that your code:\n\nis 100% self-contained \u2014 someone can copy-paste exactly what is here and run it to\nreproduce the behaviour you are observing\nincludes comments\n\n# Put code here\n\nimport distributed\nimport pennylane as qml\n\nwires = 4\nlayers = 1\ndev = qml.device('lightning.gpu', wires=wires, shots=None)\n\n\n@qml.qnode(dev)\ndef circuit(parameters):\n    qml.StronglyEntanglingLayers(weights=parameters, wires=range(wires))\n    return [qml.expval(qml.PauliZ(i)) for i in range(wires)]\n\n\ndef run_circuit():\n    shape = qml.StronglyEntanglingLayers.shape(n_layers=layers, n_wires=wires)\n    weights = qml.numpy.random.random(size=shape)\n    val = circuit(weights)\n    return val\n\n\nif __name__ == \"__main__\":\n    dask_client = distributed.Client()\n    dask_client.scheduler_info()\n\n    print(dask_client.gather(dask_client.map(lambda a: a * a, range(10))))\n    print(dask_client.gather(dask_client.map(lambda a: run_circuit(), range(10))))\n    dask_client.close()\n\nIf you want help with diagnosing an error, please put the full error message below:\n# Put full error message here\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 350, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 73, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'pennylane_lightning_gpu.lightning_gpu_qubit_ops.LightningGPU_C128' object\n\n\nAnd, finally, make sure to include the versions of your packages. Specifically, show us the output of qml.about().\nName: PennyLane\nVersion: 0.31.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/PennyLaneAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /global/u1/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning, PennyLane-Lightning-GPU\n\nPlatform info:           Linux-5.14.21-150400.24.46_12.0.73-cray_shasta_c-x86_64-with-glibc2.31\nPython version:          3.10.11\nNumpy version:           1.25.0\nScipy version:           1.10.0\nInstalled devices:\n- default.gaussian (PennyLane-0.31.0)\n- default.mixed (PennyLane-0.31.0)\n- default.qubit (PennyLane-0.31.0)\n- default.qubit.autograd (PennyLane-0.31.0)\n- default.qubit.jax (PennyLane-0.31.0)\n- default.qubit.tf (PennyLane-0.31.0)\n- default.qubit.torch (PennyLane-0.31.0)\n- default.qutrit (PennyLane-0.31.0)\n- null.qubit (PennyLane-0.31.0)\n- lightning.qubit (PennyLane-Lightning-0.31.0)\n- lightning.gpu (PennyLane-Lightning-GPU-0.31.0)\n", "link": "https://discuss.pennylane.ai//t/lightning-gpu-pickle-error-on-dask-cluster/3244/1"}, "1": {"author": "mlxd", "date": "1690834551815", "content": "Hi @QuantumMan\nI think the main issue here is defining the device outside the dask environment, but aiming to call it inside. For something like this to work, the statevector would be copied across all spawning processes, and may be a limit as the number of qubits increases.\nTo express this in a way Dask likes, I suggest defining the device, QNode and weights all in a Dask-friendly callable, and pass that to the scheduler. I have attempted a quite rewrite of your script using lightning.qubit instead of lightning.gpu, but they should all work the same:\nimport distributed\nimport pennylane as qml\n\nlayers=2\n\ndef run_circuit(wires, layers):\n    shape = qml.StronglyEntanglingLayers.shape(n_layers=layers, n_wires=wires)\n    weights = qml.numpy.random.random(size=shape)\n    dev = qml.device('lightning.qubit', wires=wires, shots=None)\n\n    @qml.qnode(dev)\n    def circuit(parameters):\n        qml.StronglyEntanglingLayers(weights=parameters, wires=range(wires))\n        return qml.math.hstack([qml.expval(qml.PauliZ(i)) for i in range(wires)])\n\n    return circuit(weights)\n\n\nif __name__ == \"__main__\":\n    dask_client = distributed.Client()\n    dask_client.scheduler_info()\n    print(dask_client.gather(dask_client.map(lambda a: a * a, range(10))))\n    print(dask_client.gather(dask_client.map(lambda w: run_circuit(w, layers), range(4,10))))\n    dask_client.close()\n\nIn this situation I can pass arguments of the qubit count (and the layer count) to the function, which then creates the params and spins up the device. Since the parameters may not be bare numpy (they have autograd specific additions) it can be best to keep them on the same function as the dvice, though you can try serialising them multiple ways and seeing if keeping them on the host helps.\nWith the above, you should be able to spawn a Dask-CUDA cluster, and then have it call-back and register with a distributed scheduler, then let the lightning.gpu devices spin up on the available workers. Failing that, you can do the same with the default dask-distributed scheduler, and use lightning.qubit or lightning.kokkos to farm out CPU workers.\nI hope this helps!1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-pickle-error-on-dask-cluster/3244/2"}, "2": {"author": "QuantumMan", "date": "1690827397690", "content": "Hello! If applicable, put your complete code example down below. Make sure that your code:\n\nis 100% self-contained \u2014 someone can copy-paste exactly what is here and run it to\nreproduce the behaviour you are observing\nincludes comments\n\n# Put code here\n\nimport distributed\nimport pennylane as qml\n\nwires = 4\nlayers = 1\ndev = qml.device('lightning.gpu', wires=wires, shots=None)\n\n\n@qml.qnode(dev)\ndef circuit(parameters):\n    qml.StronglyEntanglingLayers(weights=parameters, wires=range(wires))\n    return [qml.expval(qml.PauliZ(i)) for i in range(wires)]\n\n\ndef run_circuit():\n    shape = qml.StronglyEntanglingLayers.shape(n_layers=layers, n_wires=wires)\n    weights = qml.numpy.random.random(size=shape)\n    val = circuit(weights)\n    return val\n\n\nif __name__ == \"__main__\":\n    dask_client = distributed.Client()\n    dask_client.scheduler_info()\n\n    print(dask_client.gather(dask_client.map(lambda a: a * a, range(10))))\n    print(dask_client.gather(dask_client.map(lambda a: run_circuit(), range(10))))\n    dask_client.close()\n\nIf you want help with diagnosing an error, please put the full error message below:\n# Put full error message here\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 350, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 73, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/global/homes/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'pennylane_lightning_gpu.lightning_gpu_qubit_ops.LightningGPU_C128' object\n\n\nAnd, finally, make sure to include the versions of your packages. Specifically, show us the output of qml.about().\nName: PennyLane\nVersion: 0.31.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/PennyLaneAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /global/u1/p/prmantha/.conda/envs/myenv/lib/python3.10/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning, PennyLane-Lightning-GPU\n\nPlatform info:           Linux-5.14.21-150400.24.46_12.0.73-cray_shasta_c-x86_64-with-glibc2.31\nPython version:          3.10.11\nNumpy version:           1.25.0\nScipy version:           1.10.0\nInstalled devices:\n- default.gaussian (PennyLane-0.31.0)\n- default.mixed (PennyLane-0.31.0)\n- default.qubit (PennyLane-0.31.0)\n- default.qubit.autograd (PennyLane-0.31.0)\n- default.qubit.jax (PennyLane-0.31.0)\n- default.qubit.tf (PennyLane-0.31.0)\n- default.qubit.torch (PennyLane-0.31.0)\n- default.qutrit (PennyLane-0.31.0)\n- null.qubit (PennyLane-0.31.0)\n- lightning.qubit (PennyLane-Lightning-0.31.0)\n- lightning.gpu (PennyLane-Lightning-GPU-0.31.0)\n", "link": "https://discuss.pennylane.ai//t/lightning-gpu-pickle-error-on-dask-cluster/3244/3"}}