{"0": {"author": "eltabre", "date": "1635964108628", "content": "Hello,\nI am trying to follow this tutorial 9 to try and connect a quantum circuit to a pretrained CNN. I keep getting this error when trying to use model.add(qlayer):\nValueError: Cannot infer num from shape (None, 2)\nFor reference I load in a model and then replace the final layers with a Dense(2) layer to try to be consistent with the tutorial. Here are the final few layers of the model architecture:\n\nbatch_normalization_11 (Batc (None, 256)               1024\n\ndense_6 (Dense)              (None, 512)               131584\n\ndense_7 (Dense)              (None, 2)                1026\n\nDoes this error have to do with the variable batch size or is there something else that I am missing from the tutorial?\nThanks for your help!\nEdit:\nI got past the error by changing the imports to be consistent.  I was importing from keras and tensorflow.  I changed my imports to be:\nimport tensorflow as tf\nfrom tensorflow import keras\n\nThat seemed to solve the Value error I was encountering but is brought up another problem. When I add the Quantum Keras Layer It has 0 params and is unused. How can I fix that?\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pennylane as qml\n\ntf.keras.backend.set_floatx('float64')\n\nbase_model = keras.models.load_model('mnist_base_model.h5')\n\nmodel = keras.models.Sequential()\nfor layer in base_model.layers[:-1]: # go through until last layer probably better way to do this\n    model.add(layer)\n    model.layers[-1].trainable = False\nflat = tf.keras.layers.Flatten()\ndownsize_layer = tf.keras.layers.Dense(10)\nmodel.add(Dense(10))\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\nn_layers = 6\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\nclayer = tf.keras.layers.Dense(2, activation=\"softmax\")\n\nmodel.add(qlayer)\nmodel.add(clayer)\nmodel.summary()\n\ndense_6 (Dense)              (None, 512)               131584\n\ndense_40 (Dense)             (None, 10)                5130\n\nkeras_layer_26 (KerasLayer)  (None, 2)                 0 (unused)\n\ndense_41 (Dense)             (None, 2)                 6\n\nTotal params: 692,688\nTrainable params: 5,136\nNon-trainable params: 687,552\n\n\n\n Solved by Tom_Bromley in post #11 \n\n\n                Hi @eltabre! \n\nWhen I add the Quantum Keras Layer It has 0 params and is unused. How can I fix that? \n\nThis can be fixed by doing a forward pass through the model before printing the summary, e.g., like so: \nmodel.add(qlayer)\nmodel.add(clayer)\n\nmodel(x_test[:2])\n\nmodel.summary()\n\n [image] \nThe summa\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1635974040903", "content": "Hi @eltabre, welcome to the Forum!\nThank you for posting your edit. What version of PennyLane and TensorFlow are you using?", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/2"}, "2": {"author": "eltabre", "date": "1635975234761", "content": "Hey @CatalinaAlbornoz!\nCurrently I am using TensorFlow 2.5.1 and PennyLane 0.15.1", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/3"}, "3": {"author": "CatalinaAlbornoz", "date": "1635979428171", "content": "Hi @eltabre, could you try updating to PennyLane 0.18 and seeing if the problem persists?\nPlease let me know if this solves the problem or not.", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/4"}, "4": {"author": "eltabre", "date": "1635991454177", "content": "Hey @CatalinaAlbornoz I tried upgrading the the problem of the quantum layer persists.", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/5"}, "5": {"author": "CatalinaAlbornoz", "date": "1636055558640", "content": "Hi @eltabre, I\u2019m not being able to run your model. I think it\u2019s because you haven\u2019t fit the model to any data. Have you tried fitting it to some data? Do you get the same problem?", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/6"}, "6": {"author": "eltabre", "date": "1636064513432", "content": "I didn\u2019t include the .h5 file for loading the pretrained model in.  I have it just trained on MNIST data.  I downgraded to tensorflow 2.2.0 and circuit now shows as having trainable parameters, but when I try to do a\nmodel.fit(x_train, y_train, epochs=6, batch_size=64, validation=0.20)\nIt just hangs on the first training epoch.\nEdit:\n@CatalinaAlbornoz Here is the code I use to make the simple CNN.\nimport tensorflow as tf\n#import tensorflow_datasets as tfds\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Lambda, MaxPooling2D # convolution layers\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten # core layers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport requests\nrequests.packages.urllib3.disable_warnings()\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    # Legacy Python that doesn't verify HTTPS certificates by default\n    pass\nelse:\n    # Handle target environment that doesn't support HTTPS verification\n    ssl._create_default_https_context = _create_unverified_https_context\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Data Prep\n\n# Grayscale Norm\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# Reshape\nx_test = x_test.reshape(-1, 28,28, 1)\nx_train = x_train.reshape(-1, 28,28, 1)\n\n# One-hot encoding\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# architecture from: https://www.kaggle.com/elcaiseri/mnist-simple-cnn-keras-accuracy-0-99-top-1#4.-Evaluate-the-model\nmodel=Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(512,activation=\"relu\"))\n\nmodel.add(Dense(10,activation=\"softmax\"))\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\nepochs = 10\nbatch_size = 64\n\n#datagen.fit(X_train)\ntrain_gen = datagen.flow(x_train, y_train, batch_size=batch_size)\ntest_gen = datagen.flow(x_test, y_test, batch_size=batch_size)\n\n# Fit the model\n#tf.config.run_functions_eagerly(True)\nhistory = model.fit(train_gen,\n                              epochs = epochs,\n                              steps_per_epoch = x_train.shape[0] // batch_size,\n                              validation_data = test_gen,\n                              validation_steps = x_test.shape[0] // batch_size)\n\nmodel.save(\"mnist_base_model.h5\")\n\nI keep getting an warning before epoch one hangs:\narray_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nI have tried multiple versions on Tensorflow (2.2.0 and 2.3.0). When I use 2.5.1 I get the unused params error above", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/7"}, "7": {"author": "NikSchet", "date": "1636103275150", "content": "I am not sure if it helps but in my case:\n\n\nI pretrained a Neural network (not CNN) called \u201cmodel\u201d:\n\nclayerM = tf.keras.layers.Dense(X_train.shape[1], activation=\u201crelu\u201d)\nclayerF = tf.keras.layers.LeakyReLU(alpha=0.3)\nclayerF1 = tf.keras.layers.Dense(n_qubits, activation=\u201crelu\u201d)\n\n\n\nmodel = tf.keras.models.Sequential([clayerM, clayerF,clayerF1, clayerDropout, clayerD])\n\nfreeze weights of the three classical layers:\n\n\nclayerM.trainable = False\nclayerF.trainable = False\nclayerF1.trainable = False\n\n\n\nI defined a new model called modelh which contains the previous layers plus the quantum node and a final decision layer:\n\n\nmodelh = tf.keras.models.Sequential([clayerM,clayerF,clayerF1,qlayer,clayerD])\n\n\nI trained the new network thus achieving transfer learning. In your case i suppose you just use a more complicated CNN.\n", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/8"}, "8": {"author": "eltabre", "date": "1636125378885", "content": "@NikSchet Where you able to save off the model you trained with something like model.save() then reload it in a new instance? I am having trouble when I load a pretrained model in and try to attach a quantum circuit to the end.", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/9"}, "9": {"author": "NikSchet", "date": "1636132780484", "content": "I did something similar. I pretrained a classical model (modelC), then i saved it. I then re-loaded for a different instance, I froze the weights of the model and i added a Qnode and a final classical decision layer thus obtaning a new hybrid model (modelH. The process it:\n\nLoad the model\n\n\nmodelC = load_model(\u201cmodel.h5\u201d)\n\n\nStop the training\nmodel.trainable = False\nInclude Qnode and final decision layer thus making a Hybrid model\n\n\nmodelH = tf.keras.models.Sequential([model,qlayer,clayerD])\n\n\nTrained the new Hybrid model.\n\nSidenote: Here the pretrained model contains 3 layers (clayer1.clayer2,clayer3) so instead of using the model.trainable = False command you can also freeze the weights using this commands:\nclayerM.trainable = False\nclayerF.trainable = False\nclayerF.trainable = False\nIt is very important to remember that the last classical layer before the Qnode must have some neurons as qubits in the Qnode (this is why i call it Feeding layer) so step 3. can be something like:\n\nmodelH = tf.keras.models.Sequential([model,Feedinglayer,qlayer,clayerD])\n\nand it make sense for the feeding layer:\n\nnot to pretrain it\nuse a custom trigonometric activation function (In my case i use standar scaler to map my data from 0 to pi, so for the feeding layer activation function i use a custom sigmoid, tan etc that has the same range )\n1", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/10"}, "10": {"author": "Tom_Bromley", "date": "1636133424302", "content": "Hi @eltabre!\n\nWhen I add the Quantum Keras Layer It has 0 params and is unused. How can I fix that?\n\nThis can be fixed by doing a forward pass through the model before printing the summary, e.g., like so:\nmodel.add(qlayer)\nmodel.add(clayer)\n\nmodel(x_test[:2])\n\nmodel.summary()\n\nimage841\u00d7265 12.5 KB\nThe summary then shows the Keras layer parameters properly loaded. I\u2019m not sure why you see (unused) without the forward pass, perhaps KerasLayer isn\u2019t correctly overriding a required method \nAs a paranoia check, you can confirm that the model is sensitive to the Keras layer parameters by calculating the gradient:\nwith tf.GradientTape() as tape:\n    out = model(x_test[:2])\n    \ntape.jacobian(out, qlayer.trainable_weights)\nSolution2", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/11"}, "11": {"author": "eltabre", "date": "1636142147554", "content": "Hey @Tom_Bromley!\nThat seemed to work I can now see that the parameters are showing up! Thanks everyone for their help over the past couple days!\nWhen I was doing training I also changed verbose=1 so I could see how much data that I was loading in and that ended up showing that training was happening just much slower than I anticipated.  Usually one training epoch for the MNIST data I was using on a purely classical CNN was about 1 min but a couple of hours for the hybrid-classical network.  Is this normal? I thought with the size of the circuit I had wouldn\u2019t effect the speed that much.", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/12"}, "12": {"author": "CatalinaAlbornoz", "date": "1636381073666", "content": "Hi @eltabre, I\u2019m glad the parameters are showing up!\nThe speed depends not only on the size of the circuit but also on the device that you use (and other factors). Using lightning.qubit 4 as your device may improve speed a lot.\nPlease let me know how much this improves your speed!", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/13"}, "13": {"author": "eltabre", "date": "1636390546502", "content": "Hey @CatalinaAlbornoz!\nThanks for the tip! using the different device sped up the training by three times!", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/14"}, "14": {"author": "CatalinaAlbornoz", "date": "1636400864079", "content": "That\u2019s great to hear @eltabre! I encourage you to keep using lightning.qubit in the future.\nEnjoy using PennyLane!", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/15"}, "15": {"author": "Tom_Bromley", "date": "1636133424302", "content": "Hi @eltabre!\n\nWhen I add the Quantum Keras Layer It has 0 params and is unused. How can I fix that?\n\nThis can be fixed by doing a forward pass through the model before printing the summary, e.g., like so:\nmodel.add(qlayer)\nmodel.add(clayer)\n\nmodel(x_test[:2])\n\nmodel.summary()\n\nimage841\u00d7265 12.5 KB\nThe summary then shows the Keras layer parameters properly loaded. I\u2019m not sure why you see (unused) without the forward pass, perhaps KerasLayer isn\u2019t correctly overriding a required method \nAs a paranoia check, you can confirm that the model is sensitive to the Keras layer parameters by calculating the gradient:\nwith tf.GradientTape() as tape:\n    out = model(x_test[:2])\n    \ntape.jacobian(out, qlayer.trainable_weights)\nSolution2", "link": "https://discuss.pennylane.ai//t/transfer-learning-with-pretrained-keras-model/1462/16"}}