{"0": {"author": "BlackFyre", "date": "1688475203334", "content": "Hi!\nI am working on training a QNN with 8 layers on the MNIST classification task using 10-15 qubits. I also have access to the Compute Canada CPU cluster that allows me to use up to 40 Intel Gold 6148 Skylake@2.4 GHz cores for running jobs with a maximum allowed running time of 168 hours (1 week).\nUntil this point, I had been running tests on my PC using the lightning.qubit device, but due to the lack of support for using multiple CPU cores, as answered in https://discuss.pennylane.ai/t/lightning-qubit-cpu-utilisation/833 1, the model is currently taking too long to train.\nI also considered using the pennylane-qulacs to make use of parallelization. However, the benchmarks shown in https://docs.pennylane.ai/projects/qulacs/en/latest/ 3 seem to show no promise of improvement for the range of qubits I am working with.\nIn short: I am looking for a way to capitalize on the availability of multiple cores to run my program in a meaningful time frame.\nAny help/suggestions regarding this would be greatly appreciated. I apologize in advance if anything is unclear from my query.\nThanks.", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/1"}, "1": {"author": "mlxd", "date": "1688489007366", "content": "Hi @BlackFyre\nFrom what I understand, you have only a single output observable, and so the adjoint gradient differentiation method with lightning.qubit won\u2019t allow the use of OpenMP threaded observable calculations as part of your workload. Would that be a correct statement? If you do use multiple observables, making sure to enable diff_method=\"adjoint\" with lightning.qubit will allow spinning up a thread per observable in the gradient evaluation pipeline.\nA lot has changed in the time since that blog-post \u2014 lightning.qubit does support multithreading, but only in the gradient evaluation pipeline with the adjoint diff method.\nIf your workload doesn\u2019t benefit from this, you may want to look into our lightning.kokkos (link) 3 device (python -m pip install pennylane-lightning-kokkos). This supports OpenMP at the gate-level, rather than the observable level, and, assuming you have few observables, should allow you to scale the performance by controlling the OpenMP theading environment.\nI\u2019d suggest trying this out with OMP_NUM_THREADS=<number of physical cores> OMP_PROC_BIND=false python yourscript.py and seeing how long it takes. Feel free to reach out if we can be any further help.3", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/2"}, "2": {"author": "BlackFyre", "date": "1691346218741", "content": "Hi @mlxd. Thanks a lot for your time in suggesting solutions. They helped me greatly!1", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/3"}, "3": {"author": "kevinkawchak", "date": "1691353530921", "content": "Hello @BlackFyre,\nA good paper by Kordzanganeh, M., et al. that discussing how to optimize settings on different devices. Also, the quantum transfer learning is a good model to try for image classification.\n\n\narxiv.org\n\n\n\n2211.15631.pdf 2\n5.02 MB\n\n\n\n\n\n\n\n\npennylane.ai\n\n\n\nQuantum transfer learning 1\nCombine PyTorch and PennyLane to train a hybrid quantum-classical image classifier using transfer learning.\n\n\n\n\n\n1", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/4"}, "4": {"author": "BlackFyre", "date": "1699332090711", "content": "Hi @kevinkawchak,\nThanks a lot for the interesting transfer learning suggestion. The benchmarking paper was also very helpful.\nThanks again to everyone for contributing!1", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/5"}, "5": {"author": "BlackFyre", "date": "1688475203334", "content": "Hi!\nI am working on training a QNN with 8 layers on the MNIST classification task using 10-15 qubits. I also have access to the Compute Canada CPU cluster that allows me to use up to 40 Intel Gold 6148 Skylake@2.4 GHz cores for running jobs with a maximum allowed running time of 168 hours (1 week).\nUntil this point, I had been running tests on my PC using the lightning.qubit device, but due to the lack of support for using multiple CPU cores, as answered in https://discuss.pennylane.ai/t/lightning-qubit-cpu-utilisation/833 1, the model is currently taking too long to train.\nI also considered using the pennylane-qulacs to make use of parallelization. However, the benchmarks shown in https://docs.pennylane.ai/projects/qulacs/en/latest/ 3 seem to show no promise of improvement for the range of qubits I am working with.\nIn short: I am looking for a way to capitalize on the availability of multiple cores to run my program in a meaningful time frame.\nAny help/suggestions regarding this would be greatly appreciated. I apologize in advance if anything is unclear from my query.\nThanks.", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/6"}, "6": {"author": "kevinkawchak", "date": "1691353530921", "content": "Hello @BlackFyre,\nA good paper by Kordzanganeh, M., et al. that discussing how to optimize settings on different devices. Also, the quantum transfer learning is a good model to try for image classification.\n\n\narxiv.org\n\n\n\n2211.15631.pdf 2\n5.02 MB\n\n\n\n\n\n\n\n\npennylane.ai\n\n\n\nQuantum transfer learning 1\nCombine PyTorch and PennyLane to train a hybrid quantum-classical image classifier using transfer learning.\n\n\n\n\n\n1", "link": "https://discuss.pennylane.ai//t/utilizing-multi-core-cpu-clusters-for-qnn-training/3139/7"}}