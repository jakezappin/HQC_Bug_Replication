{"0": {"author": "SuFong_Chien", "date": "1601823212650", "content": "Hi\nGood day.\nSay, I have a classical NN with 4 layers structured as [4,5,3,2] i.e., four input nodes,  five nodes for the first hidden layer, three for the second, and the last output nodes of size two.\nAccording to the paper \u201cContinuous-variable quantum neural networks\u201d, there is a layer-by-layer structure in QNN. Most examples there only maintain the same size for each layer. In QNN, no \u201cneuron\u201d is indicated.\n1.What is the most appropriate structure for QNN if I wish to have a fair performance comparison between the classical one mentioned above and QNN?\n\nThe examples in the paper are using the same size structure for each layer. How do I code if I want a network with different sizes for each layer? Is there any example that can be referred to?\n\nThanks", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/1"}, "1": {"author": "Tom_Bromley", "date": "1601908240252", "content": "Hi @SuFong_Chien,\nGood question! In Fig. 2 of the paper 4, we show how layers with different widths can be combined. In the case of width [4,5,3,2], we would have a 5-qubit device and, for example the first layer would operate on the first 4 qubits. This can be realised in PennyLane using:\nimport pennylane as qml\nfrom pennylane import numpy as np\n\nwidths = [4, 5, 3, 2, 1]\nwires = max(widths)\ncutoff = 4\nseed = 1967\n\ndev = qml.device(\"strawberryfields.fock\", wires=wires, cutoff_dim=cutoff)\n\n\n@qml.qnode(dev)\ndef qnn(inputs, weights):\n    \n    qml.templates.DisplacementEmbedding(inputs, wires=range(wires))\n    \n    for weight, width in zip(weights, widths):\n        qml.templates.CVNeuralNetLayers(*weight, wires=range(width))\n    \n    return qml.expval(qml.X(0))\n\n\nweights = [qml.init.cvqnn_layers_all(n_layers=1, n_wires=width, seed=seed) for width in widths]\ninputs = np.random.random(wires)\n\ndqnn = qml.grad(qnn)\n\nprint(qnn(inputs, weights))\nprint(dqnn(inputs, weights))\n\nHope that helps!", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/2"}, "2": {"author": "SuFong_Chien", "date": "1601968269254", "content": "Hi Tom\nTQVM for your solution. Before I go to my journey for XANADU QNN. (especially for the boson type). I wish to have further understanding of the given codes.\n\n\nHow do you judge the cutoff =4 is good enough? It is refer to number of phtons in the fock stage right? If I am not mistaken the function fitting example set to 10.\n\n\nWhat is the purpose of  \u20181\u2019 in [4,5,3,2,1]? In this case we need 5 wires to construct the network right?\n\n\nI am sorry I am a bit lost now. If I want to train this QNN, I must pass train data as input (making loops for shuffling data and etc). After that I use the weight for prediction, am I right? I suppose there is a *.predict() function available in PennyLane, isn\u2019t it?\n\n\nPossible to print out the network diagram like what we can do in IBM qiskit?\n\n\nThere is the last \u201cout of scope\u201d question for you  . Most of the engieering work like to ask about the Big-O for a proposed algorithm.What is the Big-O for XANADU CV quantum neural network? I have studied two other different types of QNNs. For example, paper written by \u2018Quantum algorithms for feed forward neural networks\u2019 by J. Allcock et al do describe the Big-O. However, there is no description about Big-O for the paper \u2018Training deep quantum neural networks\u2019.\n\n\nI am sorry to dump you \u2018trouble\u2019 but I hope I can be very familiar with XANADU product in the future. Thanks.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/3"}, "3": {"author": "Tom_Bromley", "date": "1602090243522", "content": "Hey @SuFong_Chien,\nGood questions!\n\nHow do you judge the cutoff =4 is good enough? It is refer to number of phtons in the fock stage right? If I am not mistaken the function fitting example set to 10.\n\nYes, the cutoff refers to the number of photons that are kept track of in each mode (strictly, a cutoff of 4 means that we track the 0, 1, 2, 3 photon states). Ideally, the higher the cutoff the better, allowing us to have greater levels of squeezing or displacement that will push up the average number of photons. Unfortunately this comes with a big trade-off in simulation speed - the overhead is (cutoff) ^ (modes), so increasing the number of modes is exponentially hard. We can get away with a cutoff of 10 because we have few modes, but going up to 5 modes we may need to compromise. One way to check is to calculate the trace of the output state, if it is significantly below 1 then we know the cutoff is perhaps too low.\n\nWhat is the purpose of \u20181\u2019 in [4,5,3,2,1]? In this case we need 5 wires to construct the network right?\n\nAh, the one was just to bring us down to a single mode and have a 1D output for the sake of this prototype, similar to having e.g. a final neuron in a neural network as a binary classifier. The code above should allow you to be free in your choice of widths.\n\nI am sorry I am a bit lost now. If I want to train this QNN, I must pass train data as input (making loops for shuffling data and etc). After that I use the weight for prediction, am I right? I suppose there is a *.predict() function available in PennyLane, isn\u2019t it?\n\nTraining the QNN would look something like:\nimport pennylane as qml\nfrom pennylane import numpy as np\n\nwidths = [4, 5, 3, 2]\nwires = max(widths)\ncutoff = 4\nseed = 1967\n\ndev = qml.device(\"strawberryfields.fock\", wires=wires, cutoff_dim=cutoff)\n\n\n@qml.qnode(dev)\ndef qnn(inputs, weights):\n    \n    qml.templates.DisplacementEmbedding(inputs, wires=range(wires))\n    \n    for weight, width in zip(weights, widths):\n        qml.templates.CVNeuralNetLayers(*weight, wires=range(width))\n    \n    return qml.expval(qml.X(0)), qml.expval(qml.X(1))\n\n\nweights = [qml.init.cvqnn_layers_all(n_layers=1, n_wires=width, seed=seed) for width in widths]\ninputs = np.random.random(wires)\noutputs = np.random.random(widths[-1])\n\nopt = qml.GradientDescentOptimizer(stepsize=0.4)\n\n\ndef cost(weights):\n    return np.sum((qnn(inputs, weights) - outputs) ** 2)\n    \n\nprint(\"Example weight before: \", weights[-1][0])\n    \nfor i in range(2):\n    weights = opt.step(cost, weights)\n\nprint(\"Example weight after: \", weights[-1][0])\n\n\nPossible to print out the network diagram like what we can do in IBM qiskit?\n\nWe have a circuit drawer that can be output using print(qnn.draw()). However, it is a text-based drawer and the circuit in this case might be a little too deep, so that the output doesn\u2019t fit nicely on your screen.\n\nThere is the last \u201cout of scope\u201d question for you  . Most of the engieering work like to ask about the Big-O for a proposed algorithm.What is the Big-O for XANADU CV quantum neural network? I have studied two other different types of QNNs. For example, paper written by \u2018Quantum algorithms for feed forward neural networks\u2019 by J. Allcock et al do describe the Big-O. However, there is no description about Big-O for the paper \u2018Training deep quantum neural networks\u2019.\n\nThis is still a bit of an open research question, especially for near-term algorithms. A lot of algorithms do show speed ups or improved data capacity, but these algorithms tend to require quite deep circuits and error correction. Instead, more \u201cnear-term\u201d algorithms focus on the variational approach: having a fixed circuit of limited depth and altering the parameters. In that case the scaling is less clear, but we might be expecting things like an improved quality of training (although, this needs to be formalized).", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/4"}, "4": {"author": "SuFong_Chien", "date": "1602162563930", "content": "Hi Tom\nThanks for your explanation. I have learned much.\nIf I have an input =17 data,  the network now with the width =[17, 4, 3, 2]. In this case,wires = max(widths) = 17. Can the program that using normal computer handle this? The cutoff is still =4?\nIn this case, possible to apply XANADU photonics real quantum computer to simulate? Are we allow to use the quantum computer?\nIf I wish to compare a big-size classical NN i.e., [17, 256, 128, 64, 32, 16, 4] to QNN for their accuracy of prediction,  Still possible to use CVQNN for simulation? Is the cutoff = 7?", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/5"}, "5": {"author": "theodor", "date": "1602192148709", "content": "Hi @SuFong_Chien,\n\nIf I have an input =17 data, the network now with the width =[17, 4, 3, 2]. In this case,wires = max(widths) = 17. Can the program that using normal computer handle this? The cutoff is still =4?\n\nYou can always attempt to run the circuit with different widths/inputs, but since the overhead is (cutoff) ^ (modes), as @Tom_Bromley mentioned, it\u2019s difficult to simulate many modes and/or a high cutoff. 17 modes would be a lot, even for a cutoff value of 4, and would most likely need more memory than modern personal computers have. An even higher cutoff would probably be needed as well to get any realistic results.\n\nIn this case, possible to apply XANADU photonics real quantum computer to simulate? Are we allow to use the quantum computer?\n\nUnfortunately this type of circuit cannot be executed on current hardware. The issue is that the  CVNeuralNetLayers  involve a non-Gaussian gate in each layer (the Kerr gate). This gate is quite challenging and not possible on our current device. We recommend that you checkout here to understand the type of circuits that will fit on our present device. However, you are encouraged to request access to our quantum hardware and cloud here.\n\nIf I wish to compare a big-size classical NN i.e., [17, 256, 128, 64, 32, 16, 4] to QNN for their accuracy of prediction, Still possible to use CVQNN for simulation? Is the cutoff = 7?\n\nThis would probably be very difficult due of the high number of modes and the large cutoff, for the same reasons as explained above.\nI hope this answers your questions. ", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/6"}, "6": {"author": "SuFong_Chien", "date": "1602210313152", "content": "Hi Theodor\nit is very clear now. A bit disappointed when I was told that there is a limitation to test the real photonics QC . Anyhow, thank you very much for your very informative explanation. I hope I can learn as much as possbible from you guys.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/7"}, "7": {"author": "SuFong_Chien", "date": "1602557544731", "content": "Dear Theodor & Tom\nJust have something in mind. Supposedly, I wish to \u201cescape\u201d from the (cutoff) ^ (modes) problem, I move to classical-quantum NN. The first layer input of size 17 modes let the classical part to handle it and let it pass to the next layer that with a small number of output nodes that can be handled by the quantum part, for example classical [17, 8]-> quantum [4,3,2]. Is it a workable solution? If so, what is the advantage by comparing this solution with the pure qnn? What are the interesting points if we compare both hybrid QNN and pure QNN to the pure classical NN for prediction?", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/8"}, "8": {"author": "Maria_Schuld", "date": "1602578584164", "content": "Hey SuFong_Chien,\nLet me jump in here. Sure, having a hybrid of classical and quantum layers in a neural network is something people do a lot (see also the transfer learning tutorial 1 and the neural network module extending classical NNs generically).\nWhat this means in terms of power and mechanics of the classifier is subject to ongoing research, and I don\u2019t think anyone knows for sure yet what advantages such an approach would have compared to a pure QNN or classical NN.1", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/9"}, "9": {"author": "SuFong_Chien", "date": "1602643274079", "content": "\u201cWhat this means in terms of power and mechanics of the classifier is subject to ongoing research, and I don\u2019t think anyone knows for sure yet what advantages such an approach would have compared to a pure QNN or classical NN.\u201d\nCan \u201cA rigorous and robust quantum speed-up in supervised machine learning\u201d (https://arxiv.org/pdf/2010.02174.pdf) that has just shared by Robert Sutor justify the advantage?", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/10"}, "10": {"author": "Maria_Schuld", "date": "1602661218926", "content": "The argument in the paper is basically that if your quantum machine learning model does something like Shor\u2019s algorithm, there is an advantage of quantum processing (which is beautifully shown, but does not really tell us much about whether QML is useful for more general applications).\nSo, if your QNN layer learns a quantum algorithm that is thought to be classically intractable, you could definitely argue that some advantage is achieved  The challenge is to find the dataset that leads to a cost function which favours such a solution\u2026in other words, is there an ML problem for which Shor\u2019s algorithm is required?\nThis is the ever-present question in QML: we do not only have to show that a QML algorithm is classically intractable, but also that it can be learnt efficiently and that it is useful for a realistic class of problems. That\u2019s really hard to formulate/benchmark/model.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/11"}, "11": {"author": "SuFong_Chien", "date": "1603378712306", "content": "Hi\nI am sorry I have not started any simulations yet. It is because I can only use my free time for learning a problem that I am interested. I have just gone through a paper that I am intereted in, I found there is no hope to use full QNN model because of the network size issue i.e.,  [17, 16, 8, 4] that cannot be handled by the machine with (cutoff) ^ (modes) = 4^17. I have tried to use the code given by Tom, to construct a hybrid version which is classical [17, 16, 8] which I take this as \u201cdata transformation\u201d and feed the output to QNN with [8, 4, 4] or [8,8,4]. However, I do not manage to code due to lack of skills and knowledge. In view of this, I move to the \u201cFraud detection fitting script\u201d. I have some issues to raise for this hybrid model that I do not fully understand acording to the script.\n\n\nIs the output layer of the classical net that set to 14 is because of the paper just takes half of the data features to study?\n\n\nThe paper does not state why it chooses 4 points to U gate, 2 for each S gate, 2 for each Dgate, and 1 for each K gate? Is it an arbitrary choice?\n\n\nIf I set 8 output neurons, can I simply put 2 for U, 1 for S,D,K each?\n\n\nThis example chooses the same number of neurons for the input layer and both hidden layers in claasical net, is it free to choose different type for this particular case?\n\n\nThis example has two final outputs in qnn for classication, it is either zero or one. However, my work needs to get 4 real outputs, how to modifiy the so-called \u201cone_hot_input\u201d? I think I may need phi = a1*[1,0,0,0]+ a2*[0,1,0,0]+ a3*[0,0,1,0]+ a4*[0,0,0,1]. Can you show the  way?\n\n\nThanks", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/12"}, "12": {"author": "Tom_Bromley", "date": "1603403147332", "content": "Hi @SuFong_Chien,\n\nIn view of this, I move to the \u201cFraud detection fitting script\u201d\n\nJust to check, do you mean this 1 script?\n\nIs the output layer of the classical net that set to 14 is because of the paper just takes half of the data features to study?\n\nLooking back at the paper 2, I recall we took the first ten principal components from the credit card data as the input features. We then had a series of classical layers with the final layer having 14 features. The justification for this was that there are 14 free parameters in the first layer of a 2-mode CV layer.\n\nThe paper does not state why it chooses 4 points to U gate, 2 for each S gate, 2 for each Dgate, and 1 for each K gate? Is it an arbitrary choice?\n\nGood question! We chose these numbers to match the free parameters of each gate. For squeezing (Sgate) and displacement (Dgate) the input can be thought of as complex, and so there is both a magnitude and angle input. The interferometer is broken down into a beamspliter (BSgate) and a rotation gate (Rgate) on each mode. The beamsplitter has two angle parameters, while each Rgate has a single parameter, summing to 4. Finally, the Kerr gate is simply described by a single parameter. You could check out here for some more context on the number of parameters.\n\nIf I set 8 output neurons, can I simply put 2 for U, 1 for S,D,K each?\n\nYes, this should be possible. Overall there are 14 gate parameters. If we only control 8 and assume the rest are zero or fixed, we may expect the layer to be less flexible and for training to hence take longer or maybe fail.\n\nThis example chooses the same number of neurons for the input layer and both hidden layers in claasical net, is it free to choose different type for this particular case?\n\nYes, there is also freedom here. The choice of [10, 10, 14] was arbitrary, except for the 14 which was motivated as discussed above. You could do [8, 8, 14] and things may work just as well. This is a familiar machine learning question about how to design the network.\n\nThis example has two final outputs in qnn for classication, it is either zero or one. However, my work needs to get 4 real outputs, how to modifiy the so-called \u201cone_hot_input\u201d? I think I may need phi = a1*[1,0,0,0]+ a2*[0,1,0,0]+ a3*[0,0,1,0]+ a4*[0,0,0,1]. Can you show the way?\n\nIf we stick with two modes, you could associate the probability of two photons in either mode as the remaining two classes. For example:\n(# photons in mode 1, # photons in mode 2)\n(0, 1): class 1\n(1, 0): class 2\n(0, 2): class 3\n(2, 0): class 4\nPractically this could be achieved by updating the one_hot_input around line 286.2 Replies", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/13"}, "13": {"author": "SuFong_Chien", "date": "1603636370172", "content": "\n\n\n Tom_Bromley:\n\nIf we stick with two modes, you could associate the probability of two photons in either mode as the remaining two classes. For example:\n(# photons in mode 1, # photons in mode 2)\n(0, 1): class 1\n(1, 0): class 2\n(0, 2): class 3\n(2, 0): class 4\n\n\nI think this suggestion is for classification purposes. What I want is to predict 4 real values, which is not for classification. I have browsed XANADU web,  there is an example like this:\ndef cost(weights):\n# Create a dictionary mapping from the names of the Strawberry Fields\n# symbolic gate parameters to the TensorFlow weight values\nmapping = {p.name: w for p, w in zip(sf_params.flatten(), tf.reshape(weights, [-1]))}\n# run the engine\nstate = eng.run(qnn, args=mapping).state\nket = state.ket()\n\ndifference = tf.reduce_sum(tf.abs(ket - target_state))\nfidelity = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state)) ** 2\nreturn difference, fidelity, ket, tf.math.real(state.trace())\n\nAm I right to look into this?\nthanks", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/14"}, "14": {"author": "SuFong_Chien", "date": "1603638930412", "content": "\nBSgate(bs_variables[layer_number, 0, 0, 0], bs_variables[layer_number, 0, 0, 1]) \n| (q[0], q[1])\n    for i in range(mode_number):\n        Rgate(phase_variables[layer_number, i, 0]) | q[i]\n\nFrom the diagram above, q[0] passes through BS and next to Rotation gate, q[1] doesn;t pass through Rgate, why should we compute Rgate(phase_variables[layer_number, 1, 0]) for q[1]? Is it means that both q[0] and q[1] have been splitted into two beams each that means they are 4 beams come out from BS?", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/15"}, "15": {"author": "Tom_Bromley", "date": "1603728829989", "content": "Hey @SuFong_Chien,\n\nI think this suggestion is for classification purposes. What I want is to predict 4 real values, which is not for classification.\n\nAh, for regression you could instead output the result of a continuous-valued homodyne measurement. For example as shown in the earlier comment, the qnn() function returns two real numbers measuring the X-position on each mode. To get to four outputs, you could either increase the number of modes to four, or use two systems composed of two modes. Also, what we often do is append a classical neural network onto the end (see the concept of a dressed quantum circuit 2). This let\u2019s us not worry so much about how we decode the results from the quantum circuit.\nThe code you shared looks like a snippet from Strawberry Fields that calculates the fidelity between a state and a target state.\n\nFrom the diagram above, q[0] passes through BS and next to Rotation gate, q[1] doesn;t pass through Rgate, why should we compute Rgate(phase_variables[layer_number, 1, 0]) for q[1]? Is it means that both q[0] and q[1] have been splitted into two beams each that means they are 4 beams come out from BS?\n\n This is likely a difference between the code and the diagram. The diagram shows applying just an R gate on the first mode, which could be viewed as the \u201cminimum\u201d we need to do in this transformation. We are also able to add an R gate onto the second mode for a bit more flexibility, knowing that the case with zero phase on the second mode approximates the case with no R gate on the second mode.\nIn other words, I think both options are fine, but adding an R gate onto the second mode might add a bit more flexibility.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/16"}, "16": {"author": "SuFong_Chien", "date": "1603984222158", "content": "Hi Tom\nTrying to run the program fraud_detction.py. The error message comes out \" is \u201cAttributeError: module \u2018tensorflow\u2019 has no attribute \u2018placeholder\u2019\u201d. I solved it by putting \" import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\u201c. This problem then doesn\u2019t come out anymore but another problem come out \" from .circuit import Circuit     \u2026trawberryfields\\backends\\tfbackend\\circuit.py\u201d, line 304 \u201calpha = self._maybe_batch(alpha)\u201d IndentationError: expected an indented block. How to solve this? Thanks\n\n\n\n Tom_Bromley:\n\n(# photons in mode 1, # photons in mode 2)\n(0, 1): class 1\n(1, 0): class 2\n(0, 2): class 3\n(2, 0): class 4\n\n\nDo you mean I set either classification = tf.placeholder(shape=[batch_size, 2,2], dtype=tf.int32) OR classification = tf.placeholder(shape=[batch_size, 4], dtype=tf.int32) which gives prob = tf.abs(ket[i, classification[i, 0, 0], classification[i, 0, 1], classification[i, 1, 0],[i, 1,1]]) ** 2 OR prob = tf.abs(ket[i, classification[i, 0], classification[0, 1], classification[i, 02],[i,3]]) ** 2? Following that I just need to take out the one-hot part, the probaility I get will be the prediction values, am I right? This {classification: one_hot_input} also has to take it off ?\nThanks", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/17"}, "17": {"author": "Tom_Bromley", "date": "1604061450659", "content": "Hi @SuFong_Chien,\nAh, one thing to mention about this 1 repo is that it was made with older versions of Strawberry Fields and TensorFlow, and we unfortunately have not had the chance to update to be compatible with current versions. In the Requirements 1 section, you can find some details on how to set up the correct environment.\nRegarding your second question: there are probably multiple ways you could get the idea to work. For me, classification = tf.placeholder(shape=[batch_size, 2], dtype=tf.int32) would probably be sufficient - we do not need to change the shape, we just know that terms in the classification tensor may now have value 2 instead of being restricted to {0, 1}. I believe that prob would then remain unchanged. Remember, the reason for the 2 in shape [batch_size, 2] is to reflect the number of modes rather than the number of classes. The remainder might be something like:\n        for i in range(batch_size):\n            if int(classes[i]) == 0:\n                # Encoded such that genuine transactions should be outputted as a photon in the first mode\n                one_hot_input[i] = [1, 0]\n            elif int(classes[i]) == 1:\n                one_hot_input[i] = [0, 1]\n            elif int(classes[i]) == 2:\n                one_hot_input[i] = [0, 2]\n            elif int(classes[i]) == 3:\n                one_hot_input[i] = [2, 0]\n\nThis might work, but I haven\u2019t tried directly.1 Reply1", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/18"}, "18": {"author": "SuFong_Chien", "date": "1604153653618", "content": "Thanks. Need to  do installation stuff for the said version ", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/19"}, "19": {"author": "SuFong_Chien", "date": "1604395605757", "content": "Hi Tom\nI finally managed to run fraud_detection.py by installing tensorflow and etc according to the requirements. I found that they are two folders in the outputs folder, i.e. models and tensorboard. In the \u201cmodels\u201d, they are files as below . However, I cannot view this file. How to view the sess.ckpt-xxxxx files? Is sess.ckpt-26000.data-0000-of-00001 the \u201ctrain model\u201d that will be used for testing/verifying ? Thank you.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/20"}, "20": {"author": "nathan", "date": "1604420670839", "content": "Hi @SuFong_Chien,\nWe do not have any dedicated features yet for saving/loading models. Instead, users can leverage the tools from TensorFlow directly.\nPerhaps the following guide 1 can help you with the task of loading saved/checkpointed models using Tensorflow.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/21"}, "21": {"author": "SuFong_Chien", "date": "1605711834187", "content": "\n\n\n Tom_Bromley:\n\nfor i in range(batch_size): if int(classes[i]) == 0: # Encoded such that genuine transactions should be outputted as a photon in the first mode one_hot_input[i] = [1, 0] elif int(classes[i]) == 1: one_hot_input[i] = [0, 1] elif int(classes[i]) == 2: one_hot_input[i] = [0, 2] elif int(classes[i]) == 3: one_hot_input[i] = [2, 0]\n\n\nHi Tom\nWhen I come back to this, it is still a classification problem. My output is with says [-0.358118\t-0.0280287\t-0.30103\t-0.129338]. They are not discrete like example above. They are continuous real values. Not possible for doing that. I have also looked into the mentioned transfer learning, again it is a classification example. Any suggestions. Thanks.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/22"}, "22": {"author": "Tom_Bromley", "date": "1606483040684", "content": "\nWhen I come back to this, it is still a classification problem. My output is with says [-0.358118 -0.0280287 -0.30103 -0.129338]. They are not discrete like example above. They are continuous real values. Not possible for doing that. I have also looked into the mentioned transfer learning, again it is a classification example. Any suggestions. Thanks.\n\nHi @SuFong_Chien, we\u2019re sorry for not getting back to you sooner! I missed this comment.\nIf I understand correctly, you would like to predict an output label for one of four classes? Given the vector [-0.358118 -0.0280287 -0.30103 -0.129338], this could be achieved by applying a softmax (e.g., using tf.nn.softmax) and finding a resulting probability vector. To find the prediction, you could then do tf.math.argmax or alternatively sample from the vector.\nHowever, one thing that is not clear to me is that the values you have are negative. If I recall correctly, these values should be associated with probabilities of getting certain numbers of photons in each mode, so we expect them to be non-negative and sum to less than one. If that were the case, instead of a softmax, we could also just renormalize.\nHope this helps answer your question, and let us know if you have any more!\nThanks", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/23"}, "23": {"author": "SuFong_Chien", "date": "1608642669993", "content": "Hi Tom\nI was very busy recently to chase the KPI for this year  ;). I need to predict the four outputs simultaneously, not of four classes as you mentioned. Do yo mean add one more layer after the last quantum layer with softmax? I am not so clear. But, the classification mentioned above can let me doing another work after this one. Thank you so much.\n\n\n\n Tom_Bromley:\n\nHowever, one thing that is not clear to me is that the values you have are negative.\n\n\nSo sharp observation. It is right, all negative due to the values is transform under Log10. The problem of this work is we cannot normalize them because we need to feed all the predicted  values into a function. The normalized values are invalid to the function.", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/24"}, "24": {"author": "nathan", "date": "1608753797658", "content": "Hi @SuFong_Chien,\n@Tom_Bromley will be able to respond himself in a few weeks after the winter holiday, but perhaps I can quickly help out if possible.\nTo answer your specific question, yes, I do believe Tom was proposing to use softmax on your outputs to normalize them into a probability distribution. If one then wanted a \u201cdiscrete\u201d output, you could sample from the resulting distribution.\nPerhaps tangentially, note that, even if your outputs are not proper probabilities, one can still use a function like this one to perform classification on arbitrary real-valued vectors (they are just considered \u201clogits\u201d). I think this notion of logit is similar to your values, since those are also transformed, as you say, using a logarithm.\nBut of course, it may make sense in your case to keep these feature vectors unnormalized if you know they will be fed to additional functions downstream.1", "link": "https://discuss.pennylane.ai//t/possible-to-create-a-qnn-like-classical-one/607/25"}}