{"0": {"author": "amir", "date": "1595801901971", "content": "On the documentation for \u201cHybrid computation 2\u201d there is a section near the end that states:\n\u201cThe ability to backpropagate through hybrid computations does not mean that one can backpropagate (i.e., compute errors)  through a quantum computation .\u201d\nI\u2019m trying to understand this better. What is the significance of the \u201cthrough a quantum computation\u201d in this statement?", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/1"}, "1": {"author": "Maria_Schuld", "date": "1595828044054", "content": "Hey @amir and welcome!\nWhat this \u201cinnocent\u201d comment is pointing at is indeed quite a deep research question. Let me try to explain:\nClassical automatic differentiation, i.e. the technique with which backpropagation in neural nets is implemented on computers, is very efficient because it shares intermediate results when computing the partial derivatives of parameters theta1,\u2026,thetaD. This means it does not have to run the circuit \u201cbackwards\u201d D times, but less often.\nIn quantum computing, the way information is processed makes this much harder, if not impossible. We cannot store the intermediate quantum state and then just use it for different computations, this is forbidden by the no-cloning theorem of quantum  mechanics. While research may still come up with clever ways to cut down on the 2D circuits that need to be evaluated to compute the partial derivative of D parameters with parameter-shift rules, it is unlikely that we get the efficiency of classical automatic differentiation.\nHowever, a quantum node in a hybrid computation can be part of classical automatic differentiation like any arithmetic function (such as a sine or addition), since it can provide its gradients. In some sense, one should therefore view it as a nuclear building block of the overall computational graph, rather than an element that one can differentiate through like a neural net\u2026\nHope this clarifies the very compact statement?1 Reply", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/2"}, "2": {"author": "amir", "date": "1596067404035", "content": "Thanks. Yes, it does.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/3"}, "3": {"author": "TylerRoost", "date": "1650225795534", "content": "\n\n\n Maria_Schuld:\n\nIn some sense, one should therefore view it as a nuclear building block of the overall computational graph, rather than an element that one can differentiate through like a neural net\n\n\nCan you expand on the intuition behind this. My assumption is that the complexity of backpropagation through the qnode is significantly large enough to warrant no consideration mid-model, and thus qnodes should be placed prior to differentiable calculations. Is there some scale of qubits on which gradient calculations of quantum computations can be calculated efficiently. In some ways this pertains to this post here Quantum Advantage by Quantum Simulators\nIf we can show exponential speed up, with limited simulated qubits on certain inputs, would it be possible to parallelize multiple quantum circuits so that the differentiation process scales linearly with the constant exponential of the limited number of qubits. O(k2^n), where n is relatively small, and k represents the number parallel computations on n qubits present in the computation graph.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/4"}, "4": {"author": "Maria_Schuld", "date": "1650300874261", "content": "Hey @TylerRoost! I\u2019m not sure l understand your comment. Let me try:\nBy \u201ccomplexity of backpropagation through the qnode\u201d, do you mean the number of circuit evaluations you need to estimate the gradient of the computation performed by the qnode? (So strictly speaking not bp through the qnode, but having the qnode as one node of the computational graph for which bp is done, as discussed above).\nIf so, then computing the gradient is efficient in that it requires the estimation of as many expectation values as there are parameters, times a small constant factor (see \u201cparameter shift rules\u201d).\nPlease also explain what you mean by exponential speedup ", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/5"}, "5": {"author": "TylerRoost", "date": "1650353969682", "content": "Interesting after seeing parameter shift rules, I think I am starting to see some more of the picture, as I was misinterpreting nuclear building block. So new question what structural limitations inherent in architecting models with quantum circuits should I be aware of?\nRe: Exponential speedup\nSo here are at two things that can have an impact on complexity that I\u2019m aware of, scale of the input and complexity of the algorithm. So let\u2019s say for fixed size input (small) we can show a quantum algorithm can solve our problem using parallelization of a bunch of low qubit systems, so overall number of total qubits is high, but the interconnections are low, while classical algorithms would not be capable within the life of Universe.\nI guess my general question in this regard is in what ways does/might parallelization attribute to the consideration of quantum supremacy?\nThere must be a whole class of solvable problems that are divide and conquer framable for low input low qubit systems to work separately then combine partial solutions? For this class of problems would the classical v quantum solution time scales not really indicate quantum supremacy for some reason?\nHow do hybrid models play a role in quantum supremacy, is there a hybrid supremacy moment?", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1650543570982", "content": "Hi @TylerRoost, very interesting questions. This probably deserves a much deeper discussion but hopefully this short answer will point you in the right direction.\nRegarding the parameter-shift rules you can learn more about them here and here 1. Most of what we have in PennyLane is differentiable via this rule, but you should be careful not to update global variables from within your circuit because you might run into problem there. Also, if you use sample() for your measurement you will not be able to find a gradient.\nRE: Exponential speedup\nQuantum computers are great but they still have a long way to go. Encoding data into a quantum computer can be particularly slow, and currently they are not fault-tolerant, which means that your measurement will probably contain some errors.\nMoreover, as soon as you measure you lose your quantum state and end up with a classical output. If you want to perform a complex computation that requires many qubits it\u2019s not obvious how to split it into low-qubit modules. It\u2019s not impossible though. In fact, we have implemented a \u201ccircuit cutter\u201d in our latest PennyLane version, which allows you to do exactly that. You can learn more about it here 2.\nThe divide and conquer strategy may work to a certain extent, but since it takes time to encode your information into the quantum computer and then you lose information upon measurement, it\u2019s possible that the computational cost added by this strategy is too large. I would invite you to try it for several examples and see if you find any particular trend by using this method.\nI also recommend reading thisblog post 2 by Maria, where she discusses exponential speedups in an excellent way.\nI must say I hadn\u2019t heard the term \u201chybrid supremacy\u201d until now, but it is true that hybrid models look like they can have interesting advantages compared to purely classical or quantum ones.\nI hope this was helpful, please let me know if any of your questions remain unanswered.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/7"}, "7": {"author": "TylerRoost", "date": "1650567474412", "content": "The project I\u2019m working on has hope by me to design composable low qubit modules, we\u2019ll see if it actually works soonish. I\u2019m on family vacation currently or else I\u2019d have more to share. Also in terms of circuit cutting I\u2019m not trying to decompose larger computations into a set of cuts. I\u2019m trying to build circuits that do work well together in a composable and modular fashion. As well I\u2019m trying to build hybrid models with said modules organically.\nIn regards to the blog post regarding exponential speed ups, I think the take on maximizing expressivity takes a stance that aligns with the no free lunch theorem. I pose here that the no free lunch theorem may have exceptions. While there aren\u2019t any models yet that perform well across all the different modalities I do think the trajectory of models is trending towards production of models that are manageable sizes and thus parameter efficiency is trending upwards in a value.\nI think there is a fair argument to be made that parameter efficiency may indicate a direction to pursue for exploration in the space of more expressive models.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1650653882839", "content": "Your project sounds very interesting @TylerRoost. Please do share the results of this project with us!\nI don\u2019t know exactly what you mean by parameter efficiency but it is true that as we explore different ansatze we may find some that require less parameters but are still capable of modelling our problems well. It would in fact be ideal to model our data well by using a smart ansatz and few parameters.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/9"}, "9": {"author": "TylerRoost", "date": "1650660684707", "content": "I suppose generalizable parameter efficiency is a better measure of model performance. So models that can perform on a wide set of disjoint tasks as well as adjoint tasks with the least amount of parameters should be considered the \u201cbest\u201d models or circuits or hybridization. My intuition tends towards thinking that the two best of pure models and quantum circuits by this definition should be components of hybrid models but I wouldn\u2019t be too surprised to be wrong.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/10"}, "10": {"author": "TylerRoost", "date": "1650664996222", "content": "In this regard of best I expand on agreeable terms of the blog post and present possible continuations. Another of which would be that I agree with the concept that performing generally in practical applications is quantifiably better by the impact it can have on a gross quantity people, but I don\u2019t think passing on the idea, that generalizing across these base \u201ceasy\u201d tasks, can present the modules that may be useful in more meaningful and practical tasks.\nThat being said an argument can be made that while neural models have had more practical success, I think this hinges on hardware as did the deep learning revolution depended on hardware advancements to reach a practical advantage. Is there a Moores law for qubits?\nDespite a clear need for more powerful qpus for grander scales of computation. I believe there are many low hanging fruit along the way, in regards to this quantity of best models which perform most optimally with regards to modular composability on a large range of different possible implementations of different representations. Here representations refers to quantum or classical system states that represent different relevant information across a a plethora of tasks. My guess would be to pursue pre-trained quantum circuits for composable feature capabilities.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/11"}, "11": {"author": "CatalinaAlbornoz", "date": "1650903424689", "content": "Hi @TylerRoost, it\u2019s a very interesting discussion. Regarding the Moore\u2019s law for qubits there\u2019s the Gambetta Law, which states that the quantum volume is doubling every year. I think that the trend has gone up since 2021 though.\nIf you come by any research about modular composability it would be very cool if you could share it here.\nHaving pre-trained circuits that are useful for a variety of tasks is a complicated task, not only for quantum computers but also classical ones. But maybe something like data re-uploading can work. You can learn more about this in our data-reuploading classifier demo 2.", "link": "https://discuss.pennylane.ai//t/hybrid-computation-backpropagation/495/12"}}