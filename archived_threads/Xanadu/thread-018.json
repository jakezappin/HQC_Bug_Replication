{"0": {"author": "akawarren", "date": "1593769579058", "content": "Hello,\nI\u2019m trying to run simple qnode training code in qml.qnn.TorchLayer from official pennylane documentation.\nI\u2019ve modifed the original code - replaced the \u201cAngleEmbedding\u201d with \u201cAmplitudeEmbedding\u201d and removed some classical tensor linear layers to let nn.module has only qnode layers.\nHowever, after each iteration, parameters in nn.model seems to have no update at all. Could anyone help me to find out which part is wrong?\nHere\u2019s my code(modified)\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport sklearn.datasets\nfrom pennylane.ops.qubit import CNOT\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface='torch')\ndef qnode(inputs, weights):\n    #print(inputs)\n    qml.templates.AmplitudeEmbedding(inputs, wires=[i for i in range(n_qubits)], normalize=True, pad=0.3)\n    #qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=[i for i in range(n_qubits)], ranges=None, imprimitive=CNOT)\n    \n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\n#qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n#clayer1 = torch.nn.Linear(2, 4)\n#clayer2 = torch.nn.Linear(2, 2)\n#softmax = torch.nn.Softmax(dim=1)\n#model = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax)\n\nclass FullyEntangledQNodeNN(torch.nn.Module):\n    def __init__(self):\n        super(FullyEntangledQNodeNN, self).__init__()\n        #self.__nn_layer_type = nn_layer_type\n        #self.__nn_layer_ver_type = nn_layer_ver_type\n\n        #quantum_node, num_qubits = fully_entangled_node(input_dim)\n        weight_shapes = {\"weights\": (1, n_qubits, 3)} # In this case, we will use 1 layer per qnode\n        #self.quantum_layer = qml.qnn.TorchLayer(quantum_node, weight_shapes).double()\n        #self.quantum_layer.weight = torch.nn.Parameter(torch.DoubleTensor(weight))\n        \n        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n        #self.clayer1 = torch.nn.Linear(2, 4)\n        self.clayer2 = torch.nn.Linear(2, 2)\n        #self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        output = None\n        #output = self.clayer1(x)\n        output = self.qlayer(x)\n        #output = self.clayer2(output)\n        #output = self.softmax(output)\n\n        return output\n\nmodel = FullyEntangledQNodeNN()\n\nsamples = 100\nx, y = sklearn.datasets.make_moons(samples)\ny_hot = np.zeros((samples, 2))\ny_hot[np.arange(samples), y] = 1\n\nX = torch.tensor(x).float()\nY = torch.tensor(y_hot).float()\n\nopt = torch.optim.SGD(model.parameters(), lr=0.5)\nloss = torch.nn.L1Loss()\n\nepochs = 8\nbatch_size = 5\nbatches = samples // batch_size\n\ndata_loader = torch.utils.data.DataLoader(list(zip(X, Y)), batch_size=batch_size,\n                                          shuffle=True, drop_last=True)\n\nfor epoch in range(epochs):\n\n    running_loss = 0\n\n    for x, y in data_loader:\n        opt.zero_grad()\n\n        loss_evaluated = loss(model(x), y)\n        loss_evaluated.backward()\n\n        opt.step()\n\n        running_loss += loss_evaluated\n\n    avg_loss = running_loss / batches\n    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\nresult:\nAverage loss over epoch 1: 0.5575\nAverage loss over epoch 2: 0.5575\nAverage loss over epoch 3: 0.5575\nAverage loss over epoch 4: 0.5575\nAverage loss over epoch 5: 0.5575\nAverage loss over epoch 6: 0.5575\nAverage loss over epoch 7: 0.5575\nAverage loss over epoch 8: 0.5575", "link": "https://discuss.pennylane.ai//t/parameters-seem-to-be-fail-update-when-using-amplitudeembedding/451/1"}, "1": {"author": "nathan", "date": "1593779967125", "content": "Hi @akawarren,\nThe AmplitudeEmbedding is non-differentiable, since it involves some nontrivial pre-processing of the inputs. You can see a warning for this in the docs: https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.AmplitudeEmbedding.html 9\nWe are thinking about some future upgrades to the library which will make this operation, and others involving pre-processing, naturally differentiable, but as of the current version, it is not.1", "link": "https://discuss.pennylane.ai//t/parameters-seem-to-be-fail-update-when-using-amplitudeembedding/451/2"}, "2": {"author": "Tom_Bromley", "date": "1593784428936", "content": "Hi @akawarren, as well as changing from AmplitudeEmbedding to, e.g., AngleEmbedding, you could also try removing the interface=\"torch\"  part of the  @qml.qnode argument. The qml.qnn.TorchLayer() converts to a Torch QNode internally.1", "link": "https://discuss.pennylane.ai//t/parameters-seem-to-be-fail-update-when-using-amplitudeembedding/451/3"}, "3": {"author": "akawarren", "date": "1593769579058", "content": "Hello,\nI\u2019m trying to run simple qnode training code in qml.qnn.TorchLayer from official pennylane documentation.\nI\u2019ve modifed the original code - replaced the \u201cAngleEmbedding\u201d with \u201cAmplitudeEmbedding\u201d and removed some classical tensor linear layers to let nn.module has only qnode layers.\nHowever, after each iteration, parameters in nn.model seems to have no update at all. Could anyone help me to find out which part is wrong?\nHere\u2019s my code(modified)\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport sklearn.datasets\nfrom pennylane.ops.qubit import CNOT\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface='torch')\ndef qnode(inputs, weights):\n    #print(inputs)\n    qml.templates.AmplitudeEmbedding(inputs, wires=[i for i in range(n_qubits)], normalize=True, pad=0.3)\n    #qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=[i for i in range(n_qubits)], ranges=None, imprimitive=CNOT)\n    \n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\n#qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n#clayer1 = torch.nn.Linear(2, 4)\n#clayer2 = torch.nn.Linear(2, 2)\n#softmax = torch.nn.Softmax(dim=1)\n#model = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax)\n\nclass FullyEntangledQNodeNN(torch.nn.Module):\n    def __init__(self):\n        super(FullyEntangledQNodeNN, self).__init__()\n        #self.__nn_layer_type = nn_layer_type\n        #self.__nn_layer_ver_type = nn_layer_ver_type\n\n        #quantum_node, num_qubits = fully_entangled_node(input_dim)\n        weight_shapes = {\"weights\": (1, n_qubits, 3)} # In this case, we will use 1 layer per qnode\n        #self.quantum_layer = qml.qnn.TorchLayer(quantum_node, weight_shapes).double()\n        #self.quantum_layer.weight = torch.nn.Parameter(torch.DoubleTensor(weight))\n        \n        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n        #self.clayer1 = torch.nn.Linear(2, 4)\n        self.clayer2 = torch.nn.Linear(2, 2)\n        #self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        output = None\n        #output = self.clayer1(x)\n        output = self.qlayer(x)\n        #output = self.clayer2(output)\n        #output = self.softmax(output)\n\n        return output\n\nmodel = FullyEntangledQNodeNN()\n\nsamples = 100\nx, y = sklearn.datasets.make_moons(samples)\ny_hot = np.zeros((samples, 2))\ny_hot[np.arange(samples), y] = 1\n\nX = torch.tensor(x).float()\nY = torch.tensor(y_hot).float()\n\nopt = torch.optim.SGD(model.parameters(), lr=0.5)\nloss = torch.nn.L1Loss()\n\nepochs = 8\nbatch_size = 5\nbatches = samples // batch_size\n\ndata_loader = torch.utils.data.DataLoader(list(zip(X, Y)), batch_size=batch_size,\n                                          shuffle=True, drop_last=True)\n\nfor epoch in range(epochs):\n\n    running_loss = 0\n\n    for x, y in data_loader:\n        opt.zero_grad()\n\n        loss_evaluated = loss(model(x), y)\n        loss_evaluated.backward()\n\n        opt.step()\n\n        running_loss += loss_evaluated\n\n    avg_loss = running_loss / batches\n    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\nresult:\nAverage loss over epoch 1: 0.5575\nAverage loss over epoch 2: 0.5575\nAverage loss over epoch 3: 0.5575\nAverage loss over epoch 4: 0.5575\nAverage loss over epoch 5: 0.5575\nAverage loss over epoch 6: 0.5575\nAverage loss over epoch 7: 0.5575\nAverage loss over epoch 8: 0.5575", "link": "https://discuss.pennylane.ai//t/parameters-seem-to-be-fail-update-when-using-amplitudeembedding/451/4"}}