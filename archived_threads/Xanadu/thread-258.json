{"0": {"author": "therafael", "date": "1607587995964", "content": "I am trying to implement and run  the variational circuit from the paper \u201cQuantum Machine Learning in Feature Hilbert spaces\u201d and apply to Iris dataset. After following the tutorial on the strawberryfields demos on variational I am getting the error:\nSympifyError: SympifyError: <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.0025, 1.0025, 1.0025], dtype=float32)> when I run with engine.run() whilst testing before applying on the dataset. Here is my code:\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\n# hyper-parameters\nbatch_size = 3\neng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 5, \"batch_size\": batch_size})\ncircuit = sf.Program(2)\n# defining variables\ntheta1 = tf.Variable([0.1] * batch_size)\ntheta2 = tf.Variable([0.1] * batch_size)\ntheta3 = tf.Variable([0.1] * batch_size)\ntheta4 = tf.Variable([0.1] * batch_size)\ntheta5 = tf.Variable([0.1] * batch_size)\ntheta6 = tf.Variable([0.1] * batch_size)\ntheta7 = tf.Variable([0.1] * batch_size)\ntheta8 = tf.Variable([0.1] * batch_size)\nx1,x2 = circuit.params(\"x1\", \"x2\")\n_x1 = tf.zeros([batch_size], dtype=tf.float32)\n_x2 = tf.zeros([batch_size],  dtype=tf.float32)\n# construct the circuit\nwith circuit.context as q:\n    Squeezed(sq, x1) | q[0]\n    Squeezed(sq, x2) | q[1]\n    BSgate(theta1, theta2) | (q[0], q[1])\n    Dgate(theta3) | q[0]\n    Dgate(theta4) | q[1]\n    Pgate(theta5) | q[0]\n    Pgate(theta6) | q[1]\n    Vgate(theta7) | q[0]\n    Vgate(theta8) | q[1]\nif eng.run_progs:\n    eng.reset()\n# results = eng.run(circuit,run_options={\"eval\": False})\nresults = eng.run(circuit, args={\"x1\": _x1, \"x2\": _x2})\n\n\n\n Solved by nathan in post #23 \n\n\n                Hi @therafael, \nLooking at your code and the error msg, my guess is that you have not aligned the batch_size correctly with the size of data you are feeding in. \nFor example, you create your features like \n_x1 = tf.Variable(tf.convert_to_tensor(feat_df[0], dtype=tf.float32)) \nso these variables will\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/1"}, "1": {"author": "Tom_Bromley", "date": "1607615702126", "content": "Thanks @therafael!\nThe error is occurring because the theta1, theta2, \u2026 parameters are being defined as TensorFlow objects and not being passed through the args dictionary in eng.run(). In the TF backend, all TensorFlow-based objects must be passed through eng.run(). To fix this, you could redefine the thetas as NumPy arrays:\ntheta1 = np.array([0.1] * batch_size)\ntheta2 = np.array([0.1] * batch_size)\ntheta3 = np.array([0.1] * batch_size)\ntheta4 = np.array([0.1] * batch_size)\ntheta5 = np.array([0.1] * batch_size)\ntheta6 = np.array([0.1] * batch_size)\ntheta7 = np.array([0.1] * batch_size)\ntheta8 = np.array([0.1] * batch_size)\nx1,x2 = circuit.params(\"x1\", \"x2\")\n_x1 = tf.zeros([batch_size], dtype=tf.float32)\n_x2 = tf.zeros([batch_size],  dtype=tf.float32)\n\nHowever, this means that these parameters would not be trainable.\nIf you want to train them, you can keep them as tf.Variable() but also define them as symbolic parameters and update eng.run:\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\nbatch_size = 3\neng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 5, \"batch_size\": batch_size})\ncircuit = sf.Program(2)\n\nx1, x2 = circuit.params(\"x1\", \"x2\")\nthetas = circuit.params(*[f\"theta{i}\" for i in range(8)])\n\n_x1 = tf.zeros([batch_size], dtype=tf.float32)\n_x2 = tf.zeros([batch_size],  dtype=tf.float32)\n_thetas = 0.1 * tf.ones(8)\n\nsq = 1.0\n\nwith circuit.context as q:\n    Squeezed(sq, x1) | q[0]\n    Squeezed(sq, x2) | q[1]\n    BSgate(thetas[0], thetas[1]) | (q[0], q[1])\n    Dgate(thetas[2]) | q[0]\n    Dgate(thetas[3]) | q[1]\n    Pgate(thetas[4]) | q[0]\n    Pgate(thetas[5]) | q[1]\n    Vgate(thetas[6]) | q[0]\n    Vgate(thetas[7]) | q[1]\n    \nif eng.run_progs:\n    eng.reset()\n    \nresults = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\nHope this helps!1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/2"}, "2": {"author": "therafael", "date": "1607629160662", "content": "Let me give this a try and update on the results", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/3"}, "3": {"author": "therafael", "date": "1607630223495", "content": "@Tom_Bromley it worked thanks. But my question is I was initially defining thetas in terms of batch_size eg. theta1 = tf.Variable([0.1] * batch_size)\nbut in your second answer, define thetas as 0.1 * tf.ones(8).Why is the batch_size not used in this case?", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/4"}, "4": {"author": "Tom_Bromley", "date": "1607632686248", "content": "Hey @therafael,\nGood question, you should be free to choose whether to do 0.1 * tf.ones(8) or 0.1 * tf.ones((8, batch_size)). In this case, since the params are all the same, then it makes sense to go with the former, but yes in general the latter gives you the ability to have a different value for each element of the batch.\nFor completeness, here is the code:\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\u200b\nbatch_size = 3\neng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 5, \"batch_size\": batch_size})\ncircuit = sf.Program(2)\n\u200b\nx1, x2 = circuit.params(\"x1\", \"x2\")\nthetas = circuit.params(*[f\"theta{i}\" for i in range(8)])\n\u200b\n_x1 = tf.zeros([batch_size], dtype=tf.float32)\n_x2 = tf.zeros([batch_size],  dtype=tf.float32)\n_thetas = 0.1 * tf.ones((8, batch_size))\n\u200b\nsq = 1.0\n\u200b\nwith circuit.context as q:\n    Squeezed(sq, x1) | q[0]\n    Squeezed(sq, x2) | q[1]\n    BSgate(thetas[0], thetas[1]) | (q[0], q[1])\n#     Dgate(thetas[2]) | q[0]\n#     Dgate(thetas[3]) | q[1]\n    Pgate(thetas[4]) | q[0]\n    Pgate(thetas[5]) | q[1]\n    Vgate(thetas[6]) | q[0]\n    Vgate(thetas[7]) | q[1]\n    \nif eng.run_progs:\n    eng.reset()\n    \nresults = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\nNote that I have commented out the Dgate because otherwise I get an error. I\u2019m not sure what\u2019s going on here, it may be a bug, but for now it might be worth leaving out Dgate or using the non-batch approach to setting its parameters.1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/5"}, "5": {"author": "therafael", "date": "1607653003289", "content": "Great @Tom_Bromley , but I would want to apply it on the iris dataset so I think the batch is the best approach.\nSo in this case, what would be the parameters to update if I am using\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n minimize_opt = optimizer.minimize(loss, [var])\nwhat should be [var] in this case?", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/6"}, "6": {"author": "nathan", "date": "1607697629252", "content": "Hi @therafael,\nIt\u2019s up to you which variables you want to optimize over (in this example, you could optimize over the x variables, the theta variables, or both!\nHere\u2019s a quick example of how to optimize using a Keras optimizer (adapted from here):\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\nbatch_size = 3\neng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 5, \"batch_size\": batch_size})\ncircuit = sf.Program(2)\n\nx1, x2 = circuit.params(\"x1\", \"x2\")\nthetas = circuit.params(*[f\"theta{i}\" for i in range(8)])\n\n_x1 = tf.Variable(tf.zeros([batch_size], dtype=tf.float32))\n_x2 = tf.Variable(tf.zeros([batch_size],  dtype=tf.float32))\n_thetas = tf.Variable(0.1 * tf.ones((8, batch_size)))\n\nsq = 1.0\n\nwith circuit.context as q:\n    Squeezed(sq, x1) | q[0]\n    Squeezed(sq, x2) | q[1]\n    BSgate(thetas[0], thetas[1]) | (q[0], q[1])\n#     Dgate(thetas[2]) | q[0]\n#     Dgate(thetas[3]) | q[1]\n    Pgate(thetas[4]) | q[0]\n    Pgate(thetas[5]) | q[1]\n    Vgate(thetas[6]) | q[0]\n    Vgate(thetas[7]) | q[1]\n    \nif eng.run_progs:\n    eng.reset()\n    \nresults = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.1)\nsteps = 50\n\nfor step in range(steps):\n\n    # reset the engine if it has already been executed\n    if eng.run_progs:\n        eng.reset()\n\n    with tf.GradientTape() as tape:\n        # execute the engine\n        results = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i] for i in range(8)}})\n        # get the probability of fock state |1>\n        prob = results.state.fock_prob([1,1])\n        # negative sign to maximize prob\n        loss = -tf.reduce_sum(prob)\n\n    gradients = tape.gradient(loss, [_x1, _x2])\n    opt.apply_gradients(zip(gradients, [_x1, _x2]))\n    print(\"Loss at step {}: {}\".format(step, loss))\n\nYou\u2019ll have to modify it to suit your case, in particular the loss function. The main modifications to notice are: making sure to create tf.Variables for the parameters you want to optimize, and using tf.reduce_sum() to sum over the batch dimension.1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/7"}, "7": {"author": "therafael", "date": "1607707896507", "content": "Hi @Tom_Bromley @nathan,\nUsing the same code as you have shown above before editing, I keep getting the error\n    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nThis happens at line for\n  results = eng.run(circuit, args = {**{...}, **{...})", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/8"}, "8": {"author": "therafael", "date": "1607709253109", "content": "I have been able to fix this by using\nresults = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i].numpy()[j] for i in range(8) for j in range(len(_thetas[i]))}})\nNow the test runs successfully so I am now going to edit it to suit my case and apply to iris datasets, I will share results here as well", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/9"}, "9": {"author": "Tom_Bromley", "date": "1607726677090", "content": "Hey @therafael,\n\nUsing the same code as you have shown above before editing, I keep getting the error\n    ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nWere you getting this error with the Dgate commented out? I only get an error if the Dgate is being used  However, I\u2019ve just added this PR to support using the Dgate in a batch mode (thanks for bringing it to our attention!).\nAlthough from your second comment, it sounds like you got things to work. Let us know if you still need a hand!1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/10"}, "10": {"author": "therafael", "date": "1607745668223", "content": "Hi @Tom_Bromley @nathan,\nI got it working together with the Dgate . However I am unable to combine both the _x and _thetas in the optimization step. I am also struggling to get it working with the iris dataset.1 Reply", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/11"}, "11": {"author": "kareem_essafty", "date": "1607895536873", "content": "another weird thing, when i try to follow the quantum neural network tutorial in SF except for the usage of a homodyne measurement and try to compute the gradients, the gradients are None. let\u2019s say a single shot and the version is 0.16.0", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/12"}, "12": {"author": "nathan", "date": "1607959668428", "content": "Hi @kareem_essafty,\nThanks for reporting a possible issue. Just to help keep things organized for future readers (since this sounds like a distinct issue than the one above), could you share your observation with a new post?", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/13"}, "13": {"author": "nathan", "date": "1607977271581", "content": "Hi @therafael,\nIf you want to also optimize over the theta variables, you can change the line to\ngradients = tape.gradient(loss, [_x1, _x2] + [_thetas[i] for i in range(8)])\n    \n1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/14"}, "14": {"author": "therafael", "date": "1608010023952", "content": "Hi @nathan @Tom_Bromley,\nThank you for the response. In using this on iris dataset, I can pick two columns, one as x1 and the other as x2. Is it possible to pick more than one column in the dataset ie. multi-class ?", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/15"}, "15": {"author": "nathan", "date": "1608058518610", "content": "Hi @therafael,\nCould you provide a bit more detail? I\u2019m not sure I understand the question \nx1 and x2 are variables in this model. In principle, the number of variables does not have to be related to the number of classes. I\u2019m also uncertain what you mean by \u201ccolumns\u201d.\nI\u2019m thinking you mean a dimension/axis in the dataset? If you have more dimensions in your data, you could encode them into the theta variables as well, but then I\u2019m not sure where the point about \u201cmulti-class\u201d comes in", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/16"}, "16": {"author": "therafael", "date": "1608059556555", "content": "Hi @nathan @Tom_Bromley ,\nAssuming I read my data into a dataframe. I would to use this dataset with the circuit, so with four features and species as classes(there are 3 classes) , Do I have to load x_1 as the matrix of features and x_2 as the labels?\nI have something as shown below:\n\niris_data865\u00d7401 51.6 KB\n", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/17"}, "17": {"author": "nathan", "date": "1608140827250", "content": "Hi @therafael,\nSince you have 4 features, the most straightforward thing to do is pick four out of the 10 free variables (the xs and thetas) and assign one feature to each. The other 6 you circuit parameters you could treat as free variables for your model.\nFor the prediction, you\u2019ll need to assign that to the output of the circuit (the quantity that is returned by results), rather than to the input variables. There\u2019s no \u201cright\u201d way to do this. One choice could be to assign the class based on which photon number has the highest measurement probability (\\vert 0 \\rangle\\rightarrow class \u201c0\u201d, \\vert 1 \\rangle\\rightarrow class \u201c1\u201d) etc.1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/18"}, "18": {"author": "therafael", "date": "1608141119462", "content": "Hi @nathan,\nThank you very much. Let me give this a try and would provide update when I am done.", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/19"}, "19": {"author": "therafael", "date": "1608292788210", "content": "Hi @nathan, @Tom_Bromley ,\nI am getting this error when applying to the dataset ValueError: Parameter can be either a scalar or a vector of length 12.\nThis is the modified code for 4 qumodes\nbatch_size = 12\neng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 5, \"batch_size\": batch_size})\ncircuit = sf.Program(4)\nsq = 1.0\n\n    iris = load_iris()\n\n    X = iris.data\n    y = iris.target\n    feat_df = pd.DataFrame(X)\n\nx1,x2, x3, x4 = circuit.params(\"x1\", \"x2\", \"x3\", \"x4\")\n\nthetas = circuit.params(*[f\"theta{i}\" for i in range(12)])\n\n_x1 = tf.convert_to_tensor(feat_df[0], dtype=tf.float32)\n\n_x2 = tf.convert_to_tensor(feat_df[1], dtype=tf.float32)\n\n_x3 = tf.convert_to_tensor(feat_df[2], dtype=tf.float32)\n\n_x4 = tf.convert_to_tensor(feat_df[3], dtype=tf.float32)\n\n_thetas = tf.Variable(0.1 * tf.ones((12, batch_size)))\n\n# construct the circuit\nwith circuit.context as q:\n    Squeezed(sq, x1) | q[0]\n    Squeezed(sq, x2) | q[1]\n    Squeezed(sq, x3) | q[2]\n    Squeezed(sq, x4) | q[3]\n    BSgate(thetas[0], thetas[1]) | (q[0], q[1])\n    BSgate(thetas[2], thetas[3]) | (q[1], q[2])\n    BSgate(thetas[4], thetas[5]) | (q[2], q[3])\n    Dgate(thetas[6]) | q[0]\n    Dgate(thetas[7]) | q[1]\n    Pgate(thetas[8]) | q[2]\n    Pgate(thetas[9]) | q[3]\n    Vgate(thetas[10]) | q[0]\n    Vgate(thetas[11]) | q[1]\n\n   steps = 50\n   opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n   for step in range(steps):\n        if eng.run_progs:\n        eng.reset()\n\n   with tf.GradientTape() as tape:\n          results = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2,\"x3\": _x3, \"x4\": _x4}, **{f\"theta{i}\": _thetas[i].numpy()[j] for i in range(12) for j in range(len(_thetas[i]))}})\n    # prob = results.state.fock_prob([1,1])\n    prob20 = results.state.fock_prob([2,0,0,0])\n    prob02 = results.state.fock_prob([0,2,0,0])\n# print(type(prob02))\n# negative sign to maximize prob\n    loss = -tf.reduce_sum(prob20/(prob02 + prob20))\n    # loss = -tf.reduce_sum(prob)\n\n  gradients = tape.gradient(loss, [_x1, _x2, _x3, _x4])\n  opt.apply_gradients(zip(gradients, [_x1, _x2, _x3, _x4] + [_thetas[i] for i in range(12)]))\n# print('Loss at step {}: {}'.format(step, loss))\n  print(f'Loss at step {step}: {loss} ')", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/20"}, "20": {"author": "therafael", "date": "1608059556555", "content": "Hi @nathan @Tom_Bromley ,\nAssuming I read my data into a dataframe. I would to use this dataset with the circuit, so with four features and species as classes(there are 3 classes) , Do I have to load x_1 as the matrix of features and x_2 as the labels?\nI have something as shown below:\n\niris_data865\u00d7401 51.6 KB\n", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/21"}, "21": {"author": "nathan", "date": "1608140827250", "content": "Hi @therafael,\nSince you have 4 features, the most straightforward thing to do is pick four out of the 10 free variables (the xs and thetas) and assign one feature to each. The other 6 you circuit parameters you could treat as free variables for your model.\nFor the prediction, you\u2019ll need to assign that to the output of the circuit (the quantity that is returned by results), rather than to the input variables. There\u2019s no \u201cright\u201d way to do this. One choice could be to assign the class based on which photon number has the highest measurement probability (\\vert 0 \\rangle\\rightarrow|0\u27e9\u2192 class \u201c0\u201d, \\vert 1 \\rangle\\rightarrow|1\u27e9\u2192 class \u201c1\u201d) etc.1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/22"}, "22": {"author": "nathan", "date": "1608312752160", "content": "Hi @therafael,\nIn order for us to best help you, could you provide a full standalone example of code that gives you this error? Your code above has a number of issues that prevent it from running (missing imports, indentation problems). I\u2019m hesitant to debug the code unless I can reproduce your error msg without having to modify the code.", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/23"}, "23": {"author": "therafael", "date": "1608314150089", "content": "Hi @nathan, @Tom_Bromley,\nHere is the link to google colab: colab_notebook_sf 2\ncheck the section quantum machine learning in feature hilbert space", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/24"}, "24": {"author": "nathan", "date": "1608324796715", "content": "Hi @therafael,\nLooking at your code and the error msg, my guess is that you have not aligned the batch_size correctly with the size of data you are feeding in.\nFor example, you create your features like\n_x1 = tf.Variable(tf.convert_to_tensor(feat_df[0], dtype=tf.float32))\nso these variables will have the same size as the number of datapoints in the training data (150 I believe). However, you later set a batch_size of 6. So when you feed in your datapoints later on to the engine, it expects either scalar values, or vectors with six dimensions, not vectors with 150 dimensions.\nSomething like\n**{\"x1\": _x1[:6], \"x2\": _x2[:6],\"x3\": _x3[:6], \"x4\": _x4[:6]}\nshould work. If you want to iterate through the dataset, you can change [:6] to dynamically iterate through the training data at each step of training.\nAs a tip for debugging these kinds of things yourself, you can always print the output of, e.g., _x1 in the notebook and see if it\u2019s shape/values match what you expect.Solution1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/25"}, "25": {"author": "therafael", "date": "1608341022601", "content": "Thank you very much @nathan and @Tom_Bromley1", "link": "https://discuss.pennylane.ai//t/sf-implementation-of-quantum-machine-learning-in-feature-hilbert-spaces/718/26"}}