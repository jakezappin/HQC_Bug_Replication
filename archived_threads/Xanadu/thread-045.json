{"0": {"author": "andrew", "date": "1597940882665", "content": "I am trying to explore using different methods of optimisation with variational classifiers. I have built mine in the style shown here 8, where you have an optimisation step similar too:\nfor batch_index in tqdm(X_batches): \n     X_batch = X_train[batch_index] # grab out batches\n     y_batch = y_train[batch_index]\n\n     batch_cost = lambda v: cost(v, X_batch, y_batch) \n     theta = pennylane_opt.step(batch_cost, theta) \n\nThis, until now, has worked great! However, I have tried switching out the optimiser for QNGOptimizer and now get the error:\nValueError: The objective function must either be encoded as a single QNode or a VQECost object for the natural gradient to be automatically computed. Otherwise, metric_tensor_fn must be explicitly provided to the optimizer.\n\nThough the message is very precise, I\u2019m not sure how to fix my code to solve the problem. This isn\u2019t a VQE problem, so it would seem I cant create a VQECost object and have a metric_tensor method.\nTherefore, do I need to restructure my classifier to use only 1 QNode? The example here 3 uses 3 though (if number of QNodes = number of wires), while my code only has 2. Due to this, I\u2019m not sure how to proceed this way either.\nOr, have I misunderstood and I cannot use QNGOptimizer for classification problems?\nIf anyone has any advice on how to use QNGOptimizer with a variational classifier that would be appreciated, thanks!\nEdit:\nAdded some code for clarity:\n# quantum circuit\n@qml.qnode(dev)\ndef circuit(weights, x=None):\n    AngleEmbedding(x, wires = range(n_qubits))\n    StronglyEntanglingLayers(weights, wires = range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n# variational quantum classifier\ndef variational_classifier(theta, x=None):\n    weights = theta[0]\n    bias = theta[1]\n    return circuit(weights, x=x) + bias\n\ndef cost(theta, X, expectations):\n    e_predicted = np.array([variational_classifier(theta, x=x) for x in X])\n    loss = np.mean((e_predicted - expectations)**2)    \n    return loss\n", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/1"}, "1": {"author": "josh", "date": "1597968955169", "content": "Hi @andrew! The theory behind the quantum natural gradient as presented in https://arxiv.org/abs/1909.02108 4 only considers the optimization/natural gradient of a single QNode with no classical post-processing \u2014 hence the restriction you are seeing!\nExtending the QNG optimization method to a model that includes both classical and quantum processing is therefore somewhat non-trivial, and involves some research.\nFor this particular cost function, however, it should be doable by hand, since your trainable weights appear only in the quantum portion of the model.\nFor example, consider your cost function:\n\\text{cost}(w, x, y) = \\frac{1}{N} \\sum_i (U(w,x_i) + b - y_i)^2\nwhere  w  are the trainable weights, b the bias, x_i the input data, y_i the predicted expectations, and N the size of the input data. We can take the natural gradient of this expression:\n\n\\begin{align}\n\\nabla^{(ng)}_w \\text{cost}(w, x, y) &= \\frac{1}{N} \\sum_i \\nabla^{(ng)}_w(U(w,x_i) + b - y_i)^2\\\\\n&= \\frac{1}{N} \\sum_i 2(U(w,x_i) + b - y_i) \\nabla^{(ng)}_wU(w,x_i)\n\\end{align}\n\nwhere we have applied the chain rule.\nThe quantum natural gradient of U(w,x_i) is given by g^{-1}(w,x_i) \\nabla_w U(w,x_i) where g^{-1} is the inverse of the metric tensor; therefore\n\n\\begin{align}\n\\nabla^{(ng)}_w \\text{cost}(w, x, y) &= \\frac{1}{N} \\sum_i 2(U(w,x_i) + b - y_i) g^{-1}(w, x_i)\\nabla_wU(w,x_i)\n\\end{align}\n\nSo if we can construct the above function in Python, we can pass this to the qml.GradientDescentOptimizer as the gradient function! This could be done as follows:\nquantum_grad = qml.grad(circuit)\n\ndef cost_ng(theta, X, expectations):\n    \"\"\"Calculate the natural gradient of the cost function\"\"\"\n    qgrad = quantum_grad(theta, x, expectations)\n    ginv = sp.linalg.pinvh(circuit.metric_tensor(theta, x=X))\n    e_predicted = np.array([variational_classifier(theta, x=x) for x in X])\n    loss_ng = np.mean(2*(e_predicted - expectations)*ginv*qgrad) \n    return loss_ng\n\nYou can then run the optimization as follows:\nopt = qml.GradientDescentOptimizer(stepsize=0.4)\n\n\nfor i in range(steps):\n    # update the circuit parameters\n    weights = opt.step(lambda w: cost(w, X, Y), weights, grad_fn=lambda w:cost_ng(w, X, Y))\n\n(where the lambda functions are just to force the cost/cost_ng functions to have a single parameter weights). Note: I have not tested the above code!\nLet me know if you have any questions!1", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/2"}, "2": {"author": "andrew", "date": "1598002721469", "content": "Thank you Josh - this is great, its a very thorough answer, and has helped a lot!\nSo, just so I totally get it, in this particular example I can use QNG optimisation, however, the QNGOptimizer object is not designed for it. I can though create this optimisation manually, and pass it to GradientDescentOptimizer (which is what we have done here). For all intents and purposes, using this method, and directly using QNGOptimizer would give the same result?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/3"}, "3": {"author": "josh", "date": "1598009053323", "content": "\nFor all intents and purposes, using this method, and directly using QNGOptimizer would give the same result?\n\nAssuming my maths above is correct, then yes \nInternally, this is exactly what the QNGOptimizer class is doing; it is using g^{-1}(w)\\nabla U(w) to perform the gradient update step. However it cannot keep track of additional classical processing, so here we do it manually.\nThis has given me an idea on how we can possibly incorporate the classical processing however!1", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/4"}, "4": {"author": "andrew", "date": "1598017400386", "content": "I\u2019m trying to get cost_ng to run, and have a couple more questions. I\u2019m going through each line, trying to ensure it will run, and have had to make some small changes. This is where I am at so far:\nquantum_grad = qml.grad(circuit)\ndef cost_ng(theta, X, expectations):\n    weights = theta[0]\n    bias = theta[1] \n\n    print(circuit(weights, x=X[0]))\n    qgrad = [quantum_grad(weights, x=x) for x in X]\n\n    print(circuit.metric_tensor) #this runs, and returns an object\n    print(circuit.metric_tensor(weights, x=X)) #this does not\n\ncost_ng(theta, X_batch, y_batch)\n\n\nI split theta into 2, assuming we won\u2019t want to pass biases to the circuit at the moment.\nqgrad does not run when I pass it x=X, so I\u2019m treating it the same way I treated e_predicted, though I am unsure if this is correct.\n\ncircuit.metric_tensor is causing me some issues. If I pass it x=X it says it has got an unexpected keyword \u2018x\u2019. If I just pass it weights, then I get the error 'circuit() got multiple values for argument \u2018x\u2019. When I pass it nothing, and it runs, it then crashes when I pass it to pinvh().\n\nDo you have any suggestions on how I should approach how to solve my problem with .metric_tensor?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/5"}, "5": {"author": "josh", "date": "1598082628791", "content": "Hi @andrew, the following should work:\nimport pennylane as qml\nfrom pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\nfrom pennylane import numpy as np\nimport scipy as sp\n\nn_qubits = 3\nbatch_size = 2\n\nweights = qml.init.strong_ent_layers_normal(n_wires=n_qubits, n_layers=3)\nbias = 0.5\n\ntheta = (weights, bias)\nX = np.random.random(size=[batch_size, n_qubits])\nY = np.random.random(size=[batch_size])\n\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\ndev.operations.remove(\"Rot\")\n\n\n@qml.qnode(dev)\ndef circuit(weights, x):\n    \"\"\"Variational quantum circuit\"\"\"\n    AngleEmbedding(x, wires=range(n_qubits))\n    StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n\nquantum_grad = qml.grad(circuit, argnum=0)\nprint(\"Circuit evaluation:\", circuit(weights, X[0]))\nprint(\"Gradient evaluation:\", quantum_grad(weights, X[0]))\n\n\ndef variational_classifier(theta, x):\n    \"\"\"Variational quantum classifier\"\"\"\n    weights = theta[0]\n    bias = theta[1]\n    return circuit(weights, x) + bias\n\n\ndef cost(theta, X, expectations):\n    \"\"\"Cost function for the variational quantum classifier\"\"\"\n    e_predicted = np.array([variational_classifier(theta, x) for x in X])\n    loss = np.mean((e_predicted - expectations) ** 2)\n    return loss\n\n\nprint(\"Cost evaluation:\", cost(theta, X, Y))\n\n\ndef cost_ng(theta, X, expectations):\n    \"\"\"Natural gradient of the cost function\"\"\"\n    weights = theta[0]\n    bias = theta[1]\n\n    qnatgrad = np.empty((batch_size,) + weights.shape)\n    e_predicted = np.empty([batch_size])\n\n    for idx, x in enumerate(X):\n        e_predicted[idx] = variational_classifier(theta, x)\n\n        # compute the gradient of each QNode with repsect to `weights`\n        qgrad = quantum_grad(weights, x)\n\n        # compute the metric tensor of each QNode with respect to `weights`\n        num_params = np.prod(qgrad.shape)\n        g = circuit.metric_tensor([weights, x])[:num_params, :num_params]\n\n        # compute g^{-1} \\nabla U, and reshape it so it has the same shape as `weights`/`qgrad`\n        qnatgrad[idx] = np.linalg.solve(g, qgrad.flatten()).reshape(*qgrad.shape)\n\n    # Take the tensordot between the natural gradient and the loss,\n    # and divide by the batch size (i.e., taking the mean).\n    loss_ng = np.tensordot(2 * (e_predicted - expectations), qnatgrad, axes=1) / batch_size\n    return loss_ng\n\n\ncost_grad = qml.grad(cost, argnum=0)\nprint(\"Cost gradient:\", cost_grad(theta, X, Y))\nprint(\"Cost natural gradient:\", cost_ng(theta, X, Y))\n\nNote that to get this to work, I\u2019ve had to modify the devices to not support the qml.Rot operation. This is because qml.Rot() is not natively supported by metric_tensor() \u2014 but if the device does not support it, then PennyLane will automatically decompose it into qml.RZ and qml.RY gates, which are supported by metric_tensor()!\nTo verify that this works correctly, you can replace the metric tensor calculation with the identity matrix:\n# g = circuit.metric_tensor([weights, x])[:num_params, :num_params]\ng = np.identity(num_params)\n\nand you should find that the cost gradient and cost natural gradient are now equivalent.\n\nNote: since the combination of quantum natural gradient + classical processing is an open question, I\u2019m curious to hear if this trains better than standard gradient descent 2", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/6"}, "6": {"author": "andrew", "date": "1598101884640", "content": "Again, thank you! I have implemented this, I am now able to run cost_ng(theta, X_batch, y_batch)\nI had to make a subtle change, right at the start of cost_ng() I have said _batch_size = len(X), and used this object throughout the function instead of the \u201creal\u201d batch_size.\nI have been looking at passing these gradients to the optimisation using the grad_fn argument. If I create an object:\nclassic = lambda v: cost_grad(v, X_batch, y_batch)\n\nand passed this I find it works fine (I am assuming this is the \u201cnormal\u201d gradient descent grad function). However, if I create something like:\ngrad_cost = lambda v: cost_ng(v, X_batch, y_batch)\n\nand pass this I run into problems. I get an error:\nindex 0 is out of bounds for axis 0 with size 0\n\nI believe its because cost_grad and cost_ng are creating objects that are different shapes - cost_grad is a tuple, where the 2nd entry in it (I think\u2026) is related to the bias. It seems that grad_fn demands something of this shape. To get around this I have made cost_ng return (loss_ng, bias). This now allows optimisation to \u201cwork\u201d. However, I feel I am definitely handling this incorrectly. For one, this value seems stuck at 0.0, what it is initialised at. I have tried to understand how to include the bias correctly in the optimisation, but I\u2019m kinda coming up short - do you have any suggestions where I should go from here? I feel I will need to optimise it with respect to something in cost_ng (a line similar to:  qgrad = quantum_grad(weights, x)), but can\u2019t get something like that to work\nAnother small question - you mention that what I am doing is a combination of quantum natural gradient + classical processing. What exactly do you mean by this - are you meaning that since this is a QVC, I need a loss function like MSE, and this MSE part is \u201cclassical processing\u201d, or, is there another thing at play here I am overlooking?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/7"}, "7": {"author": "Tom_Bromley", "date": "1598303687116", "content": "Hey @andrew,\nI\u2019m joining this thread a bit late but can have a go at helping!\n\nI believe its because cost_grad and cost_ng are creating objects that are different shapes - cost_grad is a tuple, where the 2nd entry in it (I think\u2026) is related to the bias. It seems that grad_fn demands something of this shape. To get around this I have made cost_ng return (loss_ng, bias) . This now allows optimisation to \u201cwork\u201d. However, I feel I am definitely handling this incorrectly. For one, this value seems stuck at 0.0, what it is initialised at. I have tried to understand how to include the bias correctly in the optimisation, but I\u2019m kinda coming up short - do you have any suggestions where I should go from here? I feel I will need to optimise it with respect to something in cost_ng (a line similar to:  qgrad = quantum_grad(weights, x) ), but can\u2019t get something like that to work\n\nYes I think this is on the right track! Have you tried handling theta as a flat array? I\u2019ve found this helps in the past. For example, the following seemed to work:\nimport pennylane as qml\nfrom pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\nfrom pennylane import numpy as np\nimport scipy as sp\n\nn_qubits = 3\nbatch_size = 2\n\nweights = qml.init.strong_ent_layers_normal(n_wires=n_qubits, n_layers=3)\nbias = 0.5\n\ntheta = np.hstack([weights.flatten(), bias])\nX = np.random.random(size=[batch_size, n_qubits])\nY = np.random.random(size=[batch_size])\n\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\ntry:\n    dev.operations.remove(\"Rot\")\nexcept KeyError:\n    pass\n\n\n@qml.qnode(dev)\ndef circuit(weights, x):\n    \"\"\"Variational quantum circuit\"\"\"\n    AngleEmbedding(x, wires=range(n_qubits))\n    StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n\ndef variational_classifier(theta, x):\n    \"\"\"Variational quantum classifier\"\"\"\n    weights = theta[:-1].reshape((3, n_qubits, 3))\n    bias = theta[-1]\n    return circuit(weights, x) + bias\n\n\nquantum_grad = qml.grad(variational_classifier, argnum=0)\n\n\ndef cost(theta, X, expectations):\n    \"\"\"Cost function for the variational quantum classifier\"\"\"\n    e_predicted = np.array([variational_classifier(theta, x) for x in X])\n    loss = np.mean((e_predicted - expectations) ** 2)\n    return loss\n\n\ndef cost_ng(theta, X, expectations):\n    \"\"\"Natural gradient of the cost function\"\"\"\n    weights = theta[:-1].reshape((3, n_qubits, 3))\n    bias = theta[-1]\n\n    qnatgrad = np.empty((batch_size,) + weights.shape)\n    biasgrad = np.empty((batch_size,))\n    e_predicted = np.empty([batch_size])\n\n    for idx, x in enumerate(X):\n        e_predicted[idx] = variational_classifier(theta, x)\n\n        # compute the gradient of each QNode with repsect to `weights`\n        overall_grad = quantum_grad(theta, x)\n        qgrad = overall_grad[:-1].reshape((3, n_qubits, 3))\n        bgrad = overall_grad[-1]\n        \n        biasgrad[idx] = bgrad\n\n        # compute the metric tensor of each QNode with respect to `weights`\n        num_params = np.prod(qgrad.shape)\n        g = circuit.metric_tensor([weights, x])[:num_params, :num_params]\n\n        # compute g^{-1} \\nabla U, and reshape it so it has the same shape as `weights`/`qgrad`\n        qnatgrad[idx] = np.linalg.solve(g, qgrad.flatten()).reshape(*qgrad.shape)\n        \n    # Take the tensordot between the natural gradient and the loss,\n    # and divide by the batch size (i.e., taking the mean).\n    loss_ng = (np.tensordot(2 * (e_predicted - expectations), qnatgrad, axes=1) / batch_size).flatten()\n    bias_avg = np.mean(biasgrad, axis=0)\n    \n    return np.hstack([loss_ng, bias_avg])\n\n\ncost_fn = lambda theta: cost(theta, X, Y)\ncost_fn_g = lambda theta: cost_ng(theta, X, Y)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.4)\n\nopt.step(cost_fn, theta, grad_fn=cost_fn_g)\n\nHere, the bias gradient is just concatenated onto the end of the flat weights gradient.\n\nAnother small question - you mention that what I am doing is a combination of quantum natural gradient + classical processing. What exactly do you mean by this - are you meaning that since this is a QVC, I need a loss function like MSE, and this MSE part is \u201cclassical processing\u201d, or, is there another thing at play here I am overlooking?\n\n My understanding of what we mean by classical post processing here is that we are taking the average of the QNode over batches of data, also with the addition of a bias. Maybe @josh has some more thoughts there?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/8"}, "8": {"author": "andrew", "date": "1598458078130", "content": "\n\n\n Tom_Bromley:\n\nYes I think this is on the right track! Have you tried handling theta as a flat array? I\u2019ve found this helps in the past. For example, the following seemed to work:\n\n\nThank you, this has solved my problem. If you\u2019re interested, I have found an edge case caused (I think) if 0 is in x. Then, g can contain a 0 in the diagonal, and np.linalg.solve will fail. I\u2019m working around this by just continuing-ing over calculation if this condition of if 0 is in x is met, though I plan on either finding a different way to compute it or slightly changing each 0 value, shifting it slightly into a positive number.\nI am curious about how you are able to call overall_grad = quantum_grad(theta, x) inside of cost_ng. I was under the impression that a gradient cannot be calculated inside of a cost function unless you move to the PyTorch backend (which I have done for another thing I\u2019m trying) - does this not apply here as this isn\u2019t the cost function, but the gradient function itself?\n\n\n\n Tom_Bromley:\n\ntaking the average of the QNode over batches of data, also with the addition of a bias.\n\n\nI see. So, if I moved to an architecture/ training setup where batch_size=0 and I removed the bias term, this would be considered a purely qNG problem?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/9"}, "9": {"author": "Tom_Bromley", "date": "1598463505369", "content": "Hey @andrew,\n\nIf you\u2019re interested, I have found an edge case caused (I think) if 0 is in x. Then, g can contain a 0 in the diagonal, and np.linalg.solve will fail. I\u2019m working around this by just continuing -ing over calculation if this condition of if 0 is in x is met, though I plan on either finding a different way to compute it or slightly changing each 0 value, shifting it slightly into a positive number.\n\nGood spot! Yes I believe there can be issues with inverting the metric tensor and the QNGOptimizer 1 provides an optional lam argument to regularize the matrix.\n\nI am curious about how you are able to call overall_grad = quantum_grad(theta, x) inside of cost_ng . I was under the impression that a gradient cannot be calculated inside of a cost function unless you move to the PyTorch backend (which I have done for another thing I\u2019m trying) - does this not apply here as this isn\u2019t the cost function, but the gradient function itself?\n\nYes, that constraint comes from Autograd, which is the default interface in PennyLane. However, when we feed a callable function to the grad_fn argument of PennyLane optimizer methods, we are actually skipping the need to evaluate the gradient using Autograd, since the optimizer can simply call the function passed through the grad_fn argument. You can see this by checking out for example the source 1 of the GradientDescentOptimizer.\n\nI see. So, if I moved to an architecture/ training setup where batch_size=0 and I removed the bias term, this would be considered a purely qNG problem?\n\nPennyLane should be good with anything that is directly the output of a QNode or VQECost object. If we have classical postprocessing, things get a bit more complicated. So yes, in this case I believe that removing the bias, batch average, and squared error postprocessing should be good. However, this also makes the model more restricted.1", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/10"}, "10": {"author": "andrew", "date": "1598637593717", "content": "\n\n\n Tom_Bromley:\n\nYou can see this by checking out for example the source  of the GradientDescentOptimizer\n\n\nI see it even skips looking at the original version of cost altogether\n\n\n\n Tom_Bromley:\n\nSo yes, in this case I believe that removing the bias, batch average, and squared error postprocessing should be good. However, this also makes the model more restricted.\n\n\nHandling the bias is an interesting problem. With what we\u2019ve currently done, I realise that the bias gradient is always 1 or -1, and the bias term slowly grows (or shrinks) forever. The loss then also perpetually increases. I think its because the bias term is never being compared to the \u201ctruth\u201d. Looking at qnatgrad,  that in a sense is compared to the truth in the line we find loss_ng. I tried to \u201cclassically\u201d handle the bias term - ie, do something like:\ncost_grad = qml.grad(cost, argnum=0)\ngrad = cost_grad(theta, X, expectations)\nbias_ng = grad[-1]\n\nwhich removes the issue of the bias drifting larger and larger, but doesn\u2019t train very well. I\u2019ve settled on something like:\nbias_ng = np.tensordot(2 * (e_predicted - expectations), biasgrad, axes=1) / batch_size\n\nThis seems to make the process of training better and is maybe \u201ccloser\u201d to how we\u2019ve treated the weights, but just without g^-1.\nI suppose we\u2019re kinda in uncharted territory here? I would like to include the bias term so as to not restrict my model, it seems this may be the way to do it?", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/11"}, "11": {"author": "Tom_Bromley", "date": "1598650749662", "content": "Hi @andrew,\nThat sounds reasonable and it\u2019s great you\u2019ve found a way to get training working!\n\nI suppose we\u2019re kinda in uncharted territory here?\n\nYes, this topic definitely has some open research questions to sort out! It would be nice to increase support for hybrid optimization and if you\u2019d like to get involved as a contributor then let us know.\nThanks,\nTom1", "link": "https://discuss.pennylane.ai//t/variational-classifiers-and-qngoptimizer/524/12"}}