{"0": {"author": "Pavan", "date": "1621304177204", "content": "I\u2019m trying to make a QGAN with a quantum generator and a classical NN as the discriminator but I\u2019m having trouble fetching the gradients of the gen_weights in a trainable circuit.\nHowever, finding the gradients of the discriminator\u2019s trainable variables work. I\u2019ve tested each loss function and it works as well (I\u2019m using tf.BinaryCrossEntropy). I have a hunch that the problem here is that the generator loss isn\u2019t being differentiated beyond the point of the classical NN but I\u2019m not sure why this is the case. This is what the structure of each loss looks like:\n\ndisc_loss: generate array of data from gen_circuit(gen_weights) --> feed both generated data and real data into discriminator NN --> sum up cross-entropies between the confidence rates of both real and fake data, and ideal\ngen_loss: generate array of data from gen_circuit(gen_weights) --> feed generated data into discriminator NN --> find cross-entropy between discriminator output for fake data and a ones matrix\n\ntf can fetch gradients of each loss with respect to the classical NN weights but can\u2019t reach back further like in the case of gen_loss to calculate the gradients of gen_weights. So how can I fetch these gradients as well? Here is the relevant code sample which return none when fetching the generator gradients but returns the relevant tensor when fetching discriminator\u2019s gradients:\nThank you!\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(fake_output):\n    \"\"\"Calculating loss\"\"\"\n    return cross_entropy(np.ones_like(fake_output), fake_output)\n\ndef discriminator_loss(fake_output, real_output):    \n    \"\"\"Compute discriminator loss.\"\"\" \n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\ndef train_step(equity_data, gen_weights):\n    \"\"\"Run train step on provided image batch.\"\"\"\n    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape: \n        generated_prices = [equity_data[0], gen_circuit(equity_data[0], gen_weights)] \n        \n        \"\"\"Reshaping equity arrays to feed into discrim\"\"\"\n        gen_prices_in_one_arr = reshape_to_one_axis(generated_prices)\n        real_prices_in_one_arr = reshape_to_one_axis(equity_data)\n\n        \"\"\"Getting outputs from discrim\"\"\"\n        real_output = discriminator(real_prices_in_one_arr)\n        fake_output = discriminator(gen_prices_in_one_arr)\n\n        \"\"\"Calculating loss\"\"\"\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    gradients_of_generator = gen_tape.gradient(gen_loss, gen_weights)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    print(gradients_of_generator )\n#     generator_optimizer.apply_gradients(\n#         zip(gradients_of_generator, gen_weights))\n#     discriminator_optimizer.apply_gradients(\n#         zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss, disc_loss\n\n\n Solved by Pavan in post #12 \n\n\n                Hi @josh, thanks so much for your help but I think I figured it out! The problem in the minimum non-working example is that I didn\u2019t feed in  a tf.Variable  to the tape.gradient()   func. When extending it to my actual code sample, I encountered more problems but fixed them after realizing that all \u2026\n              \n", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/1"}, "1": {"author": "jmarrazola", "date": "1621351849487", "content": "Hi @Pavan, thank you for your question.\nLooking at your code, this seems like a completely classical model built using tensorflow and keras, right? So unless I\u2019m missing something, it means that the difficulties you are observing are not related to PennyLane, correct?\nI can still try to help out, but I want to make sure that I understand what the problem is exactly", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/2"}, "2": {"author": "Pavan", "date": "1621385456294", "content": "Hi,\nI should have included the gen_circuit function too, since it\u2019s an evaluation of a q_node. Here is the qnode being evaluated in the train_step function:\ndef gen_ansatz(w):\n    qml.broadcast(unitary=qml.RY, pattern = 'single', wires = wires, parameters = w[0:15])\n    for k in range(1, int(len(w)) // 15):\n        qml.broadcast(unitary=qml.CZ, pattern = 'ring', wires=wires)\n        qml.broadcast(unitary=qml.RY, pattern = 'single', wires = wires, parameters = w[(15*k):(15*(k+1))])\n@qml.qnode(dev, interface=\"tf\")\ndef gen_circuit(b_seq, gen_weights):\n    qml.templates.AngleEmbedding(b_seq, wires, rotation='X')\n    gen_ansatz(gen_weights)\n    return [qml.expval(qml.PauliZ(i)) for i in range(4)]", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/3"}, "3": {"author": "jmarrazola", "date": "1621457991043", "content": "Hi @Pavan!\nI went through your code and I think I identified the issues. I was seeing errors when calling gen_ansatz(), so I played with that function on its own and found a couple of problems related to the shape of the parameter.\nI believe you need to make sure that w is a one-dimensional array of size 15**2. In your case you are passing init_gen_weights, but initializing it to have dimension 15.  When I change the line to init_gen_weights = np.random.normal(0, 1, 15 **2), I don\u2019t see any errors.\nLet me know if that fixes your problem!", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/4"}, "4": {"author": "Pavan", "date": "1621465771461", "content": "Hi Juan,\nI just tried that and it still returns an array None for the generator\u2019s gradients. I don\u2019t think the parameter\u2019s size is what matters here since I made it such that any size that is a multiple of 15 works (15 params per layer).\nI think one problem could be that gen_ansatz() isn\u2019t a QNode function making TF unable to differentiate its weights. However, when I tried just moving the content in gen_ansatz to the broader gen_circuit() QNode with tf-interface, I still encountered the same None gradient when evaluating for its\u2019 gradients in the train_step() function.\nThank you!\nPavan", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/5"}, "5": {"author": "jmarrazola", "date": "1621521287777", "content": "Hmmm, this may be a tough nut to crack. I realize that I also changed the gen_ansatz() function to the following:\ndef gen_ansatz(w):\nqml.broadcast(unitary=qml.RY, pattern = 'single', wires = wires, parameters = w[0:15])\nfor k in range(1, 15):\n    qml.broadcast(unitary=qml.CZ, pattern = 'ring', wires=wires)\n    qml.broadcast(unitary=qml.RY, pattern = 'single', wires = wires, parameters = w[(15*k):(15*(k+1))])\n\nThis, together with the change to the dimension I mentioned above, makes it so that when I run your gen_grad() function I don\u2019t see any immediate issues.\nI am sharing my notebook below. Please let me know if you still see any issue\nQGAN_JM.ipynb (28.9 KB)", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/6"}, "6": {"author": "Pavan", "date": "1621544971567", "content": "Thank you for that but actually the gen_grad() wasn\u2019t what I was trying to fix. That is a toy function I made to test if it\u2019s possible to calculate gradients of the quantum circuit which I found it was.\nMy question has to do with the train_step() function below that and why it isn\u2019t calculating the generator\u2019s gradients with respect to the actual loss function. When calling it, it returns None.\nThanks!", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/7"}, "7": {"author": "jmarrazola", "date": "1621605329461", "content": "Got it! Helps a lot to have clear explanations of the precise issue. Let me dig deeper and see what\u2019s going on1", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/8"}, "8": {"author": "josh", "date": "1621698466323", "content": "Hi @Pavan! This is a puzzling problem, I\u2019d like to get to the bottom of it  Unfortunately, it\u2019s a bit hard to debug as the provided Jupyter notebook is quite big, and includes a lot of auxiliary code and functions!\nDo you think you might be able to reduce it down to a minimal non-working Python script example? That is, remove as much of the code as possible such that the strange behaviour (gradients being returned as None) remains?\nThat would be a huge help, and allow us to easily work out what is going wrong \nIn the meantime, since the problem seems to be occuring within gen_circuit, I have some ideas/questions that might shine some light on the problem.\n\n\nIs gen_circuit, isolated by itself, differentiable? That is, can you do\nwith tf.GradientTape() as tape:\n    res = np.sum(gen_circuit(b_seq, gen_weights))\n\ngrad = tape.gradient(res, gen_weights)\n\n\n\nIf grad remains None, it might be worth investigating the internals of the QNode to see where the differentiability breaks. For example, what happens if you simplify gen_ansatz bit by bit? If you replace qml.broadcast with a manual for loop?\n\n", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/9"}, "9": {"author": "Pavan", "date": "1621702033782", "content": "Hi @josh, thanks for helping  The forum says I can\u2019t attach anything since I\u2019m a new user but I\u2019ll link the minimum non-working py file 1 as a GitHub gist here. It\u2019s basically just a parameterized RY gate on a single wire quantum circuit that feeds the expectation value into a single neuron neural network.\nAfter trying the first suggestion, it does still return None when calling the gradient but I\u2019m going to experiment with it more now. Thanks for the suggestion!", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/10"}, "10": {"author": "Pavan", "date": "1621706081359", "content": "Hey, quick update: I was experimenting with the block you sent and I found that it returns a gradient only if the import statement is import numpy as np instead of importing numpy from pennylane like from pennylane import numpy as np.\nIn the latter case, you get an error: TypeError: iteration over a 0-d array This still doesn\u2019t fix the minimum non-working example but it is a good thing to note ", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/11"}, "11": {"author": "Pavan", "date": "1621730805254", "content": "Hi @josh, thanks so much for your help but I think I figured it out! The problem in the minimum non-working example is that I didn\u2019t feed in  a tf.Variable  to the tape.gradient()   func. When extending it to my actual code sample, I encountered more problems but fixed them after realizing that all data transformations have to be done using tf functions instead of np functions like  np.reshape()  to maintain mutability of the tape.\nThis is what the final train_step function looks like!\ndef train_step(equity_data, gen_weights):\n    \"\"\"Run train step on provided image batch.\"\"\"\n    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape: \n        generated_prices = tf.concat([equity_data[0], gen_circuit(equity_data[0], gen_weights)], 0)\n        generated_prices = tf.reshape(generated_prices, (1,1,19))\n\n        real_prices = tf.concat([equity_data[0], equity_data[1]], 0)\n        real_prices = tf.reshape(real_prices, (1,1,19))\n        \"\"\"Getting outputs from discrim\"\"\"\n        real_output = discriminator(real_prices)\n        fake_output = discriminator(generated_prices)\n\n        \"\"\"Calculating loss\"\"\"\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    print(f\"Generator loss: {gen_loss} \\n Discriminator loss: {disc_loss}\")\n    gradients_of_generator = gen_tape.gradient(gen_loss, gen_weights)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(\n        zip([gradients_of_generator], [gen_weights]))\n    discriminator_optimizer.apply_gradients(\n        zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss, gen_weightsSolution", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/12"}, "12": {"author": "josh", "date": "1621781281242", "content": "@Pavan great to hear you got it working! Let us know if you have any other questions 1", "link": "https://discuss.pennylane.ai//t/gradients-of-quantum-generator-with-tf-interface-is-none/1052/13"}}