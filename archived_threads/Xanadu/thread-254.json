{"0": {"author": "Hemant_Gahankari", "date": "1603894140255", "content": "Hi , I am trying to use qml.GradientDescentOptimizer along with Templates as follows ,\nfrom sklearn.datasets import load_iris\n\n# import some data to play with\n\niris = datasets.load_iris()\n\nX = iris.data[:, :]  # we only take the first two features.\n\nY = iris.target\n\nn_qubits = 4\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\n\ndef qnode(X,weights):\n\n    qml.templates.AngleEmbedding(X, wires=range(n_qubits))\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweights = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.1,0.2,0.3,0.4,0.5,0.6]).reshape(1,4,3)\n\ndef cost(x,weights):\n\n    return qnode(x,weights)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\n# set the number of steps\n\nsteps = 100\n\n# set the initial parameter values\n\nparams = weights\n\nfor i in range(steps):\n\n    # update the circuit parameters\n\n    weights = opt.step(cost, weights)\n\n    if (i + 1) % 5 == 0:\n\n        print(\"Cost after step {:5d}: {: .7f}\".format(i + 1, cost(params)))\n\nprint(\"Optimized rotation angles: {}\".format(params))\n\nThe problem I have is that how should I be passing inputs and weights to the cost function ?\nI was able to do the same with Keras layer since it allows me to pass weight_shapes and data was passed during model.fit(X,Y)", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/1"}, "1": {"author": "Hemant_Gahankari", "date": "1603897247701", "content": "Is it expected that we follow steps given in https://github.com/HIPS/autograd/blob/master/docs/tutorial.md 16", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/2"}, "2": {"author": "Tom_Bromley", "date": "1603901971787", "content": "Hi @Hemant_Gahankari,\nYou can find more details on using the Autograd interface here 8. You could also check out this 11 tutorial.\nIt seems like the problem is in opt.step(). Here, one should pass a function with a single argument (the trainable weights):\nopt.step(lambda weights: cost(X[i], weights), weights)\n\nAlso, note that the cost function should return a single number, so that opt.step() knows what to minimize.\nHope that helps!", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/3"}, "3": {"author": "Hemant_Gahankari", "date": "1603983572478", "content": "Thanks for the suggestion , I made a few changes as suggested ,\nn_qubits = 4\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\n\ndef qnode(X,weights):\n\n    qml.templates.AngleEmbedding(X, wires=range(n_qubits))\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweights = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.1,0.2,0.3,0.4,0.5,0.6]).reshape(1,4,3)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 10\n\nfor e in range(epochs):\n\n  for i in X:\n\n    weights = opt.step(lambda weights: qnode(X[i], weights), weights)\n\nI get an error , IndexError: arrays used as indices must be of integer (or boolean) type.", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/4"}, "4": {"author": "antalszava", "date": "1604018085648", "content": "Hi @Hemant_Gahankari,\nA couple of thoughts here:\n\nThe definition of the quantum function would require an update so that the non-differentiable parameter (X) has a default parameter: def qnode(weights, X=None). This is required when defining the QNode (included a bit more context in this answer).\nThe quantum function that you\u2019ve defined currently returns an array with 4 (n_qubits) scalars. Therefore optimization will happen as described for Vector-valued QNodes:\n\nfor e in range(epochs):\n    for i in range(len(X)):\n        weights = opt.step(lambda weights: cost(weights, X=X[i]), weights, grad_fn=lambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X[i]))\n\nAs convenience, defined a cost function:\ndef cost(weights, X):\n    res = qnode(weights, X=X)\n    return res\n\nBreaking this down a bit:\n\n\nlambda weights: cost(weights, X=X[i]): this helps to pass the current batch of features (X[i]) to the QNode\n\n\nlambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X[i]): this has to be defined so that Autograd can compute the gradient of the cost function. argnum=0 is specified as the first argument is differentiable, and we need to pass X[i] as keyword argument.\n\n\n\nA small adjustment would be required for looping through the extracted features (for i in range(len(X)) loop definition):\n\nfor e in range(epochs):\n    for i in range(len(X)):\n        weights = opt.step(lambda weights: cost(weights, X=X[i]), weights, grad_fn=lambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X[i]))\n\n\nA possible way of creating batches for each step is defining a batch_size and selecting a random batch for each epoch:\n\nbatch_size = 4\nfor e in range(epochs):\n    batch_index = np.random.randint(0, len(X), (batch_size))\n    X_batch = X[batch_index,0]\n    weights = opt.step(lambda weights: cost(weights, X=X_batch), weights, grad_fn=lambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X_batch))\n\nHopefully, this gives a bit of direction! \n(Note: edited this answer as on the first read didn\u2019t consider the solution as a vector-valued QNode, but rather a QNode that has a tensor product observable as measurement and returns a single scalar. Let us know if specifying a tensor product observable would be desired here, and can provide further pointers!)", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/5"}, "5": {"author": "Hemant_Gahankari", "date": "1604078245522", "content": "Thanks a lot for giving some clarity on this. The optimization loop is running fine now. I need your suggestion on writing a loss/cost function here. I am using iris dataset as follows and trying to predict the class.\nThe idea here is to adjust weights of the StronglyEntangledLayer , so that loss is minimised.\nThe catch is how to write a loss function for comparing 4 expected value returns and hot encoded label consisting of 3 bits. Like in Keras, when we compile we select loss function, is there any way to do so here as well ? If not how should we compare expectation values, 4 in this case and hot encoded label ?\nfrom sklearn.datasets import load_iris\n\n# import some data to play with\n\niris = datasets.load_iris()\n\nX = iris.data[:, :]  # we only take the first two features.\n\nY = iris.target\n\ntrainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrainy = tf.one_hot(trainy, depth=3)\n\ntesty = tf.one_hot(testy, depth=3)\n\nn_qubits = 4\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\n\ndef qnode(weights,X=None):\n\n    qml.templates.AngleEmbedding(X, wires=range(n_qubits))\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweights = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.1,0.2,0.3,0.4,0.5,0.6]).reshape(1,4,3)\n\ndef cost(weights, X):\n\n    res = qnode(weights, X=X)\n\n    return res\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 10\n\nfor e in range(epochs):\n\n    print(e)\n\n    for i in range(len(X)):\n\n        weights = opt.step(lambda weights: cost(weights, X=X[i]), weights, grad_fn=lambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X[i]))\n\n        print(weights)\n\nThanks for your time and help in advance.", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/6"}, "6": {"author": "Hemant_Gahankari", "date": "1604083211675", "content": "I am trying to use something like below ,\nfrom sklearn.metrics import log_loss\n\ndef cost1(weights, X , Y):\n\n    pred = qnode(weights, X=X)\n\n    return log_loss(Y,pred)\n\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 10\n\nfor e in range(epochs):\n\n    print(e)\n\n    for i in range(len(X)):\n\n        weights = opt.step(lambda weights: cost1(weights, X=trainX[i],Y=trainy[i]), weights, grad_fn=lambda weights: qml.jacobian(qnode, argnum=0)(weights, X=X[i]))\n\n        print(weights)\n\nDo you see any problem with this ? Also what is a good way to print cost1 ? like if I execute cost1 , it updates weights , so how to print cost1 ?", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/7"}, "7": {"author": "antalszava", "date": "1604092634567", "content": "Hi @Hemant_Gahankari,\nOne challenge with the definition of cost1 is that its gradient needs to support automatic differentiation and this involves both classical processing (using sklearn) and quantum computation (QNode using qml.jacobian). As sklearn does not support automatic differentiation, his might not be feasible, unfortunately. For further points see this relevant discussion on using sklearn in a cost function. As suggested there, using a supported PennyLane interface (Torch/TF) or using layers from qml.qnn package with built-in optimizers & functions could be a feasible approach.\nPrinting cost1: one way to do this is to evaluate the cost function right before/after the update of weights.\nHope this gives an idea for moving forward! \n\nFor further reference, could you help out with a couple of small adjustments when sharing code?\n\nAdjusting formatting of the code (e.g., enclosing strings with \"\" and '' instead of \u201c\u201c, double-checking that imports are correct and the code snippet can be executed independently)\nPlacing code into a highlighted code blocks so that the code renders and indentation is correct\n\nOverall this helps with quickly recreating the case that is shared and also helps other users easily get up to speed with parts of the solution.\nThank you so much! 1", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/8"}, "8": {"author": "Hemant_Gahankari", "date": "1604131937306", "content": "Hi , I am trying to use tf optimizer and loss function as follows , but getting an error , TypeError: zip argument #2 must support iteration. What do you think could be the issue here ?\nimport pennylane as qml\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom pennylane import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import load_iris\n\n# import some data to play with\n\niris = datasets.load_iris()\n\nX = iris.data[:, :]  # we only take the first two features.\n\nY = iris.target\n\ntrainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrainy = tf.one_hot(trainy, depth=4)\n\ntesty = tf.one_hot(testy, depth=4)\n\nn_qubits = 4\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface='tf')\n\ndef qnode(weights,X=None):\n\n    qml.templates.AngleEmbedding(X, wires=range(n_qubits))\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\n\nweights = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.1,0.2,0.3,0.4,0.5,0.6]).reshape(1,4,3)\n\nweights = tf.Variable(weights, dtype=tf.float32 ,trainable=True)\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\n\nfor i in range(len(trainX)):\n    pred = qnode(weights, X=trainX[i])\n    with tf.GradientTape() as tape:        \n        loss = tf.keras.losses.categorical_crossentropy(trainy[i],pred)\n        print(loss)\n    gradients = tape.gradient(loss,weights)\n    opt.apply_gradients(zip(gradients,weights))", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/9"}, "9": {"author": "Hemant_Gahankari", "date": "1604229840170", "content": "I also tried making a change to the above optimizer calling code as follows but even that is giving an error , TypeError: \u2018NoneType\u2019 object is not callable. Can you suggest a right way to do this ?\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\n\ndef cost(weights,X,Y):\n\n  pred = qnode(weights, X=X)\n\n  loss = tf.keras.losses.categorical_crossentropy(Y,pred)\n\nfor i in range(len(trainX)):\n\n  weights = opt.minimize(cost(weights, X=trainX[i],Y=trainy[i]), weights)", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/10"}, "10": {"author": "Tom_Bromley", "date": "1604345962702", "content": "Hi @Hemant_Gahankari,\nI managed to get it working using\nweights = tf.Variable(weights, dtype=tf.float64 ,trainable=True)\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\n\nfor i in range(len(trainX)):\n    with tf.GradientTape() as tape:        \n        pred = qnode(weights, X=trainX[i])\n        loss = tf.keras.losses.categorical_crossentropy(trainy[i],pred)\n\n    gradients = tape.gradient(loss, [weights])\n\n    opt.apply_gradients(zip(gradients,[weights]))\n\nThings to note:\n\nWhen directly using a QNode rather than a KerasLayer, the output tensor will be of type tf.float64. Hence, the weights also have to be cast to the same type.\nThe pred = qnode(weights, X=trainX[i]) line must be included within the tape context to keep track of the gradient, otherwise the gradient will be None.\n\nweights is nested as [weights] to get things to work.\n\nAn alternative to the above could be:\nweights = tf.Variable(weights, dtype=tf.float64 ,trainable=True)\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\n\ndef loss(weights, x, y):\n    pred = qnode(weights, X=x)\n    return tf.keras.losses.categorical_crossentropy(y,pred)\n\nfor x, y in zip(trainX, trainy):\n    opt.minimize(lambda: loss(weights, x, y), [weights])\n1", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/11"}, "11": {"author": "Hemant_Gahankari", "date": "1604394480568", "content": "Thanks a lot for your help , it is working now.1", "link": "https://discuss.pennylane.ai//t/how-to-use-qml-gradientdescentoptimizer-with-angleembedding-template/644/12"}}