{"0": {"author": "Yan_Li", "date": "1685081255238", "content": "Currently, I\u2019m working on a quantum neural network that integrates a Qnode into a PyTorch model. To accelerate the computation, I use the \u2018Parameter broadcast\u2019 feature. The device I used is default.qubit. However, it only works when the diff_method is backprop. When use parameter_shift or adjoint, it will throw error like\n\nimage2050\u00d7356 44.2 KB\n\n\nimport numpy as np\nfrom sklearn.datasets import make_moons\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pennylane as qml\nimport sys\nfrom time import perf_counter\nimport math\nclass Model(nn.Module):\n    def __init__(self, device, diff_method=\"backprop\"):\n        super().__init__()\n\n        self.cnet_in = self.cnet__in()\n        n_qubits = math.ceil(math.log2(16))\n        self.qlayer = self.qnode(device,n_qubits,diff_method)\n        quantum_weights = np.random.normal(0, np.pi, (2*(n_qubits-1),))\n        self.quantum_weights = nn.parameter.Parameter(torch.tensor(quantum_weights,\\\n                                    dtype=torch.float32,requires_grad=True).cuda())\n        self.cnet_out = self.cnet__out()\n\n    def cnet__in(self):\n        layers = [nn.Linear(2,10), nn.ReLU(True), nn.Linear(10,16), nn.Tanh()]\n        return nn.Sequential(*layers)  \n\n    def cnet__out(self):\n        layers = [nn.Linear(1,10), nn.ReLU(True), nn.Linear(10,2), nn.Tanh()]\n        return nn.Sequential(*layers) \n\n    def qnode(self,device,n_qubits,diff_method):\n        dev = qml.device(device, wires=n_qubits)\n        def block(weights, wires):\n            qml.RX(weights[0], wires=wires[0])\n            qml.RY(weights[1], wires=wires[1])\n            qml.CNOT(wires=wires)\n        @qml.qnode(dev, interface=\"torch\", diff_method=diff_method)\n        def qnode(inputs,weights):\n            # print(\"excute qnode \",weights.shape,inputs.shape)\n            qml.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),pad_with=0.,normalize=True) \n            # qml.Hadamard(wires=n_qubits-1)\n            weights = weights.reshape(-1,2)\n            qml.MPS(\n                wires=range(n_qubits),\n                n_block_wires=2,\n                block=block,\n                n_params_block=2,\n                template_weights=weights,\n            )\n            r = qml.PauliZ(wires=n_qubits-1)\n            # print(\"r \",r)\n            return qml.expval(r)\n        return qnode\n\n    def forward(self, x):\n        print(\"self.quantum_weights \",self.quantum_weights.shape)\n        x1 = self.cnet_in(x)\n        print('x ',x1.shape, x1.requires_grad)\n        x2 = self.qlayer(x1,self.quantum_weights)\n        x2 = x2.cpu().to(torch.float)\n        x2 = x2.reshape(-1,1)\n        print(\"X2 \",x2.shape)\n        x_output = self.cnet_out(x2)\n        print(\"x_output \",x_output.shape)\n        return x_output\n\ndef train(X, y_hot, dev_name, diff_method):\n    \n    model  = Model(dev_name, diff_method)\n    \n    # Train the model\n    opt = torch.optim.SGD(model.parameters(), lr=0.2)\n    loss = torch.nn.L1Loss()\n\n    X = torch.tensor(X, requires_grad=False).float()\n    y_hot = y_hot.float()\n\n    batch_size = 5\n    batches = 200 // batch_size\n\n    data_loader = torch.utils.data.DataLoader(\n        list(zip(X, y_hot)), batch_size=batch_size, shuffle=True, drop_last=True\n    )\n\n    epochs = 6\n\n    for epoch in range(epochs):\n\n        running_loss = 0\n\n        for xs, ys in data_loader:\n            opt.zero_grad()\n\n            loss_evaluated = loss(model(xs), ys)\n            loss_evaluated.backward()\n\n            opt.step()\n\n            running_loss += loss_evaluated\n\n        avg_loss = running_loss / batches\n        print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\n    y_pred = model(X)\n    predictions = torch.argmax(y_pred, axis=1).detach().numpy()\n\n    correct = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\n    accuracy = sum(correct) / len(correct)\n    print(f\"Accuracy: {accuracy * 100}%\")\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nX, y = make_moons(n_samples=200, noise=0.1)\ny_ = torch.unsqueeze(torch.tensor(y), 1)  # used for one-hot encoded labels\ny_hot = torch.scatter(torch.zeros((200, 2)), 1, y_, 1)\nbegin_time = perf_counter()\ntrain(X, y_hot, 'default.qubit', 'adjoint')\nend_time = perf_counter()\nruntime = end_time-begin_time\nprint(f'Runtime: {runtime:.2e} s or {(runtime/60):.2e} min.')\n\n\nAnd the full error message below:\nself.quantum_weights  torch.Size([6])\nx  torch.Size([5, 16]) True\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-93-0d7150a7efeb> in <cell line: 7>()\n      5 y_hot = torch.scatter(torch.zeros((200, 2)), 1, y_, 1)\n      6 begin_time = perf_counter()\n----> 7 train(X, y_hot, 'default.qubit', 'adjoint')\n      8 end_time = perf_counter()\n      9 runtime = end_time-begin_time\n\n15 frames\n<ipython-input-91-467a37879882> in train(X, y_hot, dev_name, diff_method)\n     91             opt.zero_grad()\n     92 \n---> 93             loss_evaluated = loss(model(xs), ys)\n     94             loss_evaluated.backward()\n     95 \n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501             return forward_call(*args, **kwargs)\n   1502         # Do not call functions when jit is used\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\n<ipython-input-91-467a37879882> in forward(self, x)\n     56         x1 = self.cnet_in(x)\n     57         print('x ',x1.shape, x1.requires_grad)\n---> 58         x2 = self.qlayer(x1,self.quantum_weights)\n     59         x2 = x2.cpu().to(torch.float)\n     60         x2 = x2.reshape(-1,1)\n\n/usr/local/lib/python3.10/dist-packages/pennylane/qnode.py in __call__(self, *args, **kwargs)\n    865                 self.execute_kwargs.pop(\"mode\")\n    866             # pylint: disable=unexpected-keyword-arg\n--> 867             res = qml.execute(\n    868                 [self.tape],\n    869                 device=self.device,\n\n/usr/local/lib/python3.10/dist-packages/pennylane/interfaces/execution.py in execute(tapes, device, gradient_fn, interface, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n    427         if gradient_kwargs.get(\"method\", \"\") == \"adjoint_jacobian\":\n    428             mode = \"forward\" if grad_on_execution else \"backward\"\n--> 429             tapes = _adjoint_jacobian_expansion(tapes, mode, interface, max_expansion)\n    430 \n    431         # grad on execution or best was chosen\n\n/usr/local/lib/python3.10/dist-packages/pennylane/interfaces/execution.py in _adjoint_jacobian_expansion(tapes, grad_on_execution, interface, max_expansion)\n     81     for i, tape in enumerate(tapes):\n     82         if any(not stop_at(op) for op in tape.operations):\n---> 83             tapes[i] = tape.expand(stop_at=stop_at, depth=max_expansion)\n     84 \n     85     return tapes\n\n/usr/local/lib/python3.10/dist-packages/pennylane/tape/qscript.py in expand(self, depth, stop_at, expand_measurements)\n   1077         RY(0.2, wires=['a'])]\n   1078         \"\"\"\n-> 1079         new_script = qml.tape.tape.expand_tape(\n   1080             self, depth=depth, stop_at=stop_at, expand_measurements=expand_measurements\n   1081         )\n\n/usr/local/lib/python3.10/dist-packages/pennylane/tape/tape.py in expand_tape(tape, depth, stop_at, expand_measurements)\n    210 \n    211             # recursively expand out the newly created tape\n--> 212             expanded_tape = expand_tape(obj, stop_at=stop_at, depth=depth - 1)\n    213 \n    214             new_prep.extend(expanded_tape._prep)\n\n/usr/local/lib/python3.10/dist-packages/pennylane/tape/tape.py in expand_tape(tape, depth, stop_at, expand_measurements)\n    202                 # Object is an operation; query it for its expansion\n    203                 try:\n--> 204                     obj = obj.expand()\n    205                 except DecompositionUndefinedError:\n    206                     # Object does not define an expansion; treat this as\n\n/usr/local/lib/python3.10/dist-packages/pennylane/operation.py in expand(self)\n   1390             raise DecompositionUndefinedError\n   1391 \n-> 1392         qscript = qml.tape.make_qscript(self.decomposition)()\n   1393 \n   1394         if not self.data:\n\n/usr/local/lib/python3.10/dist-packages/pennylane/tape/qscript.py in wrapper(*args, **kwargs)\n   1376     def wrapper(*args, **kwargs):\n   1377         with AnnotatedQueue() as q:\n-> 1378             result = fn(*args, **kwargs)\n   1379 \n   1380         qscript = QuantumScript.from_queue(q)\n\n/usr/local/lib/python3.10/dist-packages/pennylane/operation.py in decomposition(self)\n   1200             list[Operator]: decomposition of the operator\n   1201         \"\"\"\n-> 1202         return self.compute_decomposition(\n   1203             *self.parameters, wires=self.wires, **self.hyperparameters\n   1204         )\n\n/usr/local/lib/python3.10/dist-packages/pennylane/templates/state_preparations/mottonen.py in compute_decomposition(state_vector, wires)\n    359         # Apply inverse y rotation cascade to prepare correct absolute values of amplitudes\n    360         for k in range(len(wires_reverse), 0, -1):\n--> 361             alpha_y_k = _get_alpha_y(a, len(wires_reverse), k)\n    362             control = wires_reverse[k:]\n    363             target = wires_reverse[k - 1]\n\n/usr/local/lib/python3.10/dist-packages/pennylane/templates/state_preparations/mottonen.py in _get_alpha_y(a, n, k)\n    195         for j in range(2 ** (n - k))\n    196     ]\n--> 197     numerator = qml.math.take(a, indices=indices_numerator, axis=-1)\n    198     numerator = qml.math.sum(qml.math.abs(numerator) ** 2, axis=-1)\n    199 \n\n/usr/local/lib/python3.10/dist-packages/autoray/autoray.py in do(fn, like, *args, **kwargs)\n     77     \"\"\"\n     78     backend = choose_backend(fn, *args, like=like, **kwargs)\n---> 79     return get_lib_fn(backend, fn)(*args, **kwargs)\n     80 \n     81 \n\n/usr/local/lib/python3.10/dist-packages/pennylane/math/single_dispatch.py in _take_torch(tensor, indices, axis, **_)\n    538 \n    539     fancy_indices = [slice(None)] * axis + [indices]\n--> 540     return tensor[fancy_indices]\n    541 \n    542 \n\nIndexError: index 8 is out of bounds for dimension 0 with size 5\n\nAnd the qml.about()\n\nimage2456\u00d7640 83.4 KB\n1 Reply", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/1"}, "1": {"author": "Yan_Li", "date": "1685081453069", "content": "I think the reason is that the input features that need to be broadcasted require the grad (requires_grad = True), but the examples of the parameter broadcast have the inputs of which the requires_grad = False", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/2"}, "2": {"author": "isaacdevlugt", "date": "1685114381455", "content": "Hey @Yan_Li! It doesn\u2019t look like you\u2019re using it, but we have a dedicated module in Pennylane for handling how Pennylane interfaces with Pytorch. You can check it out here: Turning quantum nodes into Torch Layers \u2014 PennyLane documentation 3\nCurrently, true parameter broadcasting within qnn.TorchLayer isn\u2019t supported. I.e., you can still \u201cbroadcast\u201d, but what happens under the hood is a serial execution, not parallel. So, it looks like broadcasting is happening, but really isn\u2019t.\nThere\u2019s a PR 3 for this on our Github that will add support for true broadcasting if you\u2019re interested in following its progress!\nRegarding your specific error, adjoint doesn\u2019t support parameter-broadcasting\u2026 so the error you\u2019re getting is a little strange. In any case, we should have a more verbose and informative error message.\nIn the mean time, what you can do is:\n\nAdd qml.trasnforms.broadcast_expand to the qnode:\n\n        @qml.transforms.broadcast_expand\n        @qml.qnode(dev, interface=\"torch\", diff_method=diff_method)\n        def qnode(inputs,weights):\n            # print(\"excute qnode \",weights.shape,inputs.shape)\n            qml.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),pad_with=0.,normalize=True) \n            # qml.Hadamard(wires=n_qubits-1)\n            weights = weights.reshape(-1,2)\n            qml.MPS(\n                wires=range(n_qubits),\n                n_block_wires=2,\n                block=block,\n                n_params_block=2,\n                template_weights=weights,\n            )\n            r = qml.PauliZ(wires=n_qubits-1)\n            # print(\"r \",r)\n            return qml.expval(r)\n\n\nswitch to a different differentiation method (backprop)\nswitch to a different device that doesn\u2019t think its should be supporting parameter broadcasting\n1", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/3"}, "3": {"author": "isaacdevlugt", "date": "1685114810368", "content": "I opened an issue to try and have the error message fixed \n\n\ngithub.com/PennyLaneAI/pennylane\n\n\n\n\n\n\n\n\nMore informative error message needed for lack of support for parameter-broadcasting with `adjoint` 2\n\n\n\n        opened \n\n\n\nMay 26, 2023\n\n\n\n\n\n          isaacdevlugt\n        \n\n\n\n\n          bug \n\n\n\n\n\n### Expected behavior\n\nIf a circuit with `diff_method = \"adjoint\"` is attempted \u2026to be broadcasted, the error message should say something to the effect that that isn't supported.\n\n### Actual behavior\n\n```\nimport pennylane as qml\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev, diff_method=\"adjoint\")\ndef circuit(x):\n    qml.RX(x, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n>>> circuit([1/2, 1/2, 1/2, 1/2])\nValueError: could not broadcast input array from shape (4,2,2) into shape (2,2)\n```\n\n\n\n### Additional information\n\nIndirectly found from a forum post: https://discuss.pennylane.ai/t/parameter-broadcast-bug/2975/3\n\n### Source code\n\n```shell\nimport pennylane as qml\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev, diff_method=\"adjoint\")\ndef circuit(x):\n    qml.RX(x, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ncircuit([1/2, 1/2, 1/2, 1/2])\n```\n\n\n### Tracebacks\n\n```shell\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 8\n      5     qml.RX(x, wires=0)\n      6     return qml.expval(qml.PauliZ(0))\n----> 8 circuit([1/2, 1/2, 1/2, 1/2])\n\nFile ~/pennylane-torch/lib/python3.9/site-packages/pennylane/qnode.py:867, in QNode.__call__(self, *args, **kwargs)\n    865     self.execute_kwargs.pop(\"mode\")\n    866 # pylint: disable=unexpected-keyword-arg\n--> 867 res = qml.execute(\n    868     [self.tape],\n    869     device=self.device,\n    870     gradient_fn=self.gradient_fn,\n    871     interface=self.interface,\n    872     gradient_kwargs=self.gradient_kwargs,\n    873     override_shots=override_shots,\n    874     **self.execute_kwargs,\n    875 )\n    877 res = res[0]\n    879 if old_interface == \"auto\":\n\nFile ~/pennylane-torch/lib/python3.9/site-packages/pennylane/interfaces/execution.py:483, in execute(tapes, device, gradient_fn, interface, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n    480     elif mapped_interface == \"jax\":\n    481         _execute = _get_jax_execute_fn(interface, tapes)\n...\n-> 1939     bras[kk, ...] = self._apply_operation(ket, tape.observables[kk])\n   1941 expanded_ops = []\n   1942 for op in reversed(tape.operations):\n\nValueError: could not broadcast input array from shape (4,2,2) into shape (2,2)\n```\n\n\n### System information\n\n```shell\nName: PennyLane\nVersion: 0.30.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor:\nAuthor-email:\nLicense: Apache License 2.0\nLocation: /Users/isaac/pennylane-torch/lib/python3.9/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning\n\nPlatform info:           macOS-13.3.1-x86_64-i386-64bit\nPython version:          3.9.14\nNumpy version:           1.23.5\nScipy version:           1.10.1\nInstalled devices:\n- default.gaussian (PennyLane-0.30.0)\n- default.mixed (PennyLane-0.30.0)\n- default.qubit (PennyLane-0.30.0)\n- default.qubit.autograd (PennyLane-0.30.0)\n- default.qubit.jax (PennyLane-0.30.0)\n- default.qubit.tf (PennyLane-0.30.0)\n- default.qubit.torch (PennyLane-0.30.0)\n- default.qutrit (PennyLane-0.30.0)\n- null.qubit (PennyLane-0.30.0)\n- lightning.qubit (PennyLane-Lightning-0.30.0)\n```\n\n\n### Existing GitHub issues\n\n- [X] I have searched existing GitHub issues to make sure the issue does not already exist.\n\n\n\n\n\n\n1", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/4"}, "4": {"author": "neak11", "date": "1694468610861", "content": "\n\n\n Yan_Li:\n\n/usr/local/lib/python3.10/dist-packages/pennylane/templates/state_preparations/mottonen.py in _get_alpha_y(a, n, k)\n    195         for j in range(2 ** (n - k))\n    196     ]\n--> 197     numerator = qml.math.take(a, indices=indices_numerator, axis=-1)\n    198     numerator = qml.math.sum(qml.math.abs(numerator) ** 2, axis=-1)\n    199 \n\n/usr/local/lib/python3.10/dist-packages/autoray/autoray.py in do(fn, like, *args, **kwargs)\n     77     \"\"\"\n     78     backend = choose_backend(fn, *args, like=like, **kwargs)\n---> 79     return get_lib_fn(backend, fn)(*args, **kwargs)\n     80 \n     81 \n\n/usr/local/lib/python3.10/dist-packages/pennylane/math/single_dispatch.py in _take_torch(tensor, indices, axis, **_)\n    538 \n    539     fancy_indices = [slice(None)] * axis + [indices]\n--> 540     return tensor[fancy_indices]\n    541 \n    542 \n\nIndexError: index 8 is out of bounds for dimension 0 with size 5\n\n\n\nI\u2019m currently using MottonenStatePreparation and I get the same error, so I\u2019ve looked into its code:\ndef _get_alpha_z(omega, n, k):\n    r\"\"\"Computes the rotation angles required to implement the uniformly-controlled Z rotation\n    applied to the :math:`k`th qubit.\n\n    The :math:`j`th angle is related to the phases omega of the desired amplitudes via:\n\n    .. math:: \\alpha^{z,k}_j = \\sum_{l=1}^{2^{k-1}} \\frac{\\omega_{(2j-1) 2^{k-1}+l} - \\omega_{(2j-2) 2^{k-1}+l}}{2^{k-1}}\n\n    Args:\n        omega (tensor_like): phases of the state to prepare\n        n (int): total number of qubits for the uniformly-controlled rotation\n        k (int): index of current qubit\n\n    Returns:\n        array representing :math:`\\alpha^{z,k}`\n    \"\"\"\n    indices1 = [\n        [(2 * j - 1) * 2 ** (k - 1) + l - 1 for l in range(1, 2 ** (k - 1) + 1)]\n        for j in range(1, 2 ** (n - k) + 1)\n    ]\n    indices2 = [\n        [(2 * j - 2) * 2 ** (k - 1) + l - 1 for l in range(1, 2 ** (k - 1) + 1)]\n        for j in range(1, 2 ** (n - k) + 1)\n    ]\n\n    term1 = qml.math.take(omega, indices=indices1, axis=-1)\n    term2 = qml.math.take(omega, indices=indices2, axis=-1)\n    diff = (term1 - term2) / 2 ** (k - 1)\n\n    return qml.math.sum(diff, axis=-1)\n\n\ndef _get_alpha_y(a, n, k):\n    r\"\"\"Computes the rotation angles required to implement the uniformly controlled Y rotation\n    applied to the :math:`k`th qubit.\n\n    The :math:`j`-th angle is related to the absolute values, a, of the desired amplitudes via:\n\n    .. math:: \\alpha^{y,k}_j = 2 \\arcsin \\sqrt{ \\frac{ \\sum_{l=1}^{2^{k-1}} a_{(2j-1)2^{k-1} +l}^2  }{ \\sum_{l=1}^{2^{k}} a_{(j-1)2^{k} +l}^2  } }\n\n    Args:\n        a (tensor_like): absolute values of the state to prepare\n        n (int): total number of qubits for the uniformly-controlled rotation\n        k (int): index of current qubit\n\n    Returns:\n        array representing :math:`\\alpha^{y,k}`\n    \"\"\"\n    indices_numerator = [\n        [(2 * (j + 1) - 1) * 2 ** (k - 1) + l for l in range(2 ** (k - 1))]\n        for j in range(2 ** (n - k))\n    ]\n    numerator = qml.math.take(a, indices=indices_numerator, axis=-1)\n    numerator = qml.math.sum(qml.math.abs(numerator) ** 2, axis=-1)\n\n    indices_denominator = [[j * 2**k + l for l in range(2**k)] for j in range(2 ** (n - k))]\n    denominator = qml.math.take(a, indices=indices_denominator, axis=-1)\n    denominator = qml.math.sum(qml.math.abs(denominator) ** 2, axis=-1)\n\n    # Divide only where denominator is zero, else leave initial value of zero.\n    # The equation guarantees that the numerator is also zero in the corresponding entries.\n\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        division = numerator / denominator\n\n    # Cast the numerator and denominator to ensure compatibility with interfaces\n    division = qml.math.cast(division, np.float64)\n    denominator = qml.math.cast(denominator, np.float64)\n\n    division = qml.math.where(denominator != 0.0, division, 0.0)\n\n    return 2 * qml.math.arcsin(qml.math.sqrt(division))\n\nThe problem here I believe is that in these two functions which mottonen.py uses in its compute_decomposition function, qml.math.take is used with a negative axis index. At least in the torch implementation, which I am using, the take function isn\u2019t really built to handle negative axis indices:\ndef _take_torch(tensor, indices, axis=None, **_):\n    \"\"\"Torch implementation of np.take\"\"\"\n    torch = _i(\"torch\")\n\n    if not isinstance(indices, torch.Tensor):\n        indices = torch.as_tensor(indices)\n\n    if axis is None:\n        return tensor.flatten()[indices]\n\n    if indices.ndim == 1:\n        if (indices < 0).any():\n            # index_select doesn't allow negative indices\n            dim_length = tensor.size()[0] if axis is None else tensor.shape[axis]\n            indices = torch.where(indices >= 0, indices, indices + dim_length)\n\n        return torch.index_select(tensor, dim=axis, index=indices)\n\n    fancy_indices = [slice(None)] * axis + [indices]\n    return tensor[fancy_indices]\n\nBasically, what seems to happen here is when you set axis=-1, the code still ends up applying the \u201ctake\u201d to the first (zeroth) axis and not the last one, since  [slice(None)] * axis produces an empty array for all axis<=1. At least that\u2019s the behavior for 2d tensors.", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/5"}, "5": {"author": "CatalinaAlbornoz", "date": "1694564290536", "content": "Hi @neak11 , welcome to the Forum and thank you for your comment here!\nDo you have a minimal code example that is self-contained so that we can try to replicate your error? I understand that the sign in the axis could be causing issues so I\u2019d love to see your specific code. Maybe it falls within a known situation that isn\u2019t supported, or maybe it\u2019s a new and unknown problem.", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/6"}, "6": {"author": "neak11", "date": "1694615725308", "content": "Hi @CatalinaAlbornoz , I\u2019ve posted this code in the GitHub issue about the same error:\nimport pennylane as qml\nfrom pennylane import qnn\nimport torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\n\n\nimages = datasets.FakeData(size=8000, image_size=(1, 8, 8), num_classes=1, transform=ToTensor())\n\n\ndevice = qml.device('default.qubit', wires=6, shots=None)\n\n\n@qml.qnode(device, interface='torch', diff_method='adjoint')\ndef circuit(inputs, param):\n    norms = torch.norm(inputs, dim=1)\n    inputs = inputs/torch.stack([norms for i in range(inputs.size(dim=1))], dim=1)\n    qml.MottonenStatePreparation(inputs, wires=range(6))\n    qml.BasicEntanglerLayers(param, wires=range(6))\n    return qml.expval(qml.PauliY(wires=0))\n\n\nparam_shape = qml.BasicEntanglerLayers.shape(1, 6)\ndictionary = {\"param\": param_shape}\nquantum_layer = qnn.TorchLayer(circuit, dictionary)\n\nflat_layer = nn.Flatten(1, -1)\nmodel_net = nn.Sequential(flat_layer, quantum_layer).to(\"cpu\")\n\ndataloader_images = DataLoader(images, batch_size=10, shuffle=True)\nlosses = nn.L1Loss()\noptim = torch.optim.SGD(model_net.parameters(), lr=1e-6)\n\n\ndef training(dataloader, model, lossf, optimizer):\n    model.train()\n    for batch, (data, labels) in enumerate(dataloader):\n        result = model(data)\n        loss = lossf(result, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor ep in range(10):\n    training(dataloader_images, model_net, losses, optim)\n", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/7"}, "7": {"author": "CatalinaAlbornoz", "date": "1694728377422", "content": "Hi @neak11 ,\nThank you! I\u2019m linking the issue here 2 for future reference.\nI can replicate your issue. We\u2019re looking into it.", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1695218066844", "content": "Hi!\nFor anyone looking at this thread the problem is solved with using qml.StatePrep instead of qml.MottonenStatePreparation since the latter doesn\u2019t have broadcasting support at the moment.\nThe Torch axis=-1 has been fixed.2", "link": "https://discuss.pennylane.ai//t/parameter-broadcast-bug/2975/9"}}