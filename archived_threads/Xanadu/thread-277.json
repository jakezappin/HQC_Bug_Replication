{"0": {"author": "Viro", "date": "1635232317386", "content": "Hi!\nI am new to Pennylane, and have some questions regarding how to gain some speedups with some of the code that I have. My goal is to see how QAOA performs with both increasing graph sizes and depth. In order to get some data, I need to run the simulations multiple times, however, my code quickly becomes inefficient as I increase the number of qubits or the depth of the QAOA circuit. So I\u2019m curious about 3 things:\nWhich qubit device is recommended to use to get increased speed? (I am currently using default.qubit)\nIs there any efficient way to run the optimization? (possibly jit-ing the computations?)\nI calculate the expectation-value of the hamiltonian using the ExpVal function. Is there an argument that can be passed to increase the speed of these evaluations?\nCurrently, running a 6 qubit system on a regular 3-degree graph requires around 15 minutes to run.\nHere is some of the code that I have written. Note that the \u201cedges\u201d structure in my case is essentially just a list with tuples (i,j,w) that describe the edges with their corresponding weights, while graph is a networkx class. Out of these classes, it is QAOA complete in particular I think that is the slowest part of the code, so any help in speeding that part of the code would be incredibly helpful. Thanks in advance for any tips \nHere is the python file if it is easier to read off that: QAOA-MaxcutAttempt.py (10.5 KB)\ndef ProblemUnitary(edge1,edge2,t,weight):\n    \"\"\"Creates the problem unitary which is specific to MaxCut. Applies e^{-i t H_c}\n\n    Args:\n        edge1 ([int]): [The control qubit]\n        edge2 ([int]): [The target qubit]\n        t ([type]): [The t in the expression]\n        weight ([float]): [weight of the edge between the nodes in the graph]\n    \"\"\"\n    qml.CNOT(wires = [edge1,edge2])\n    qml.RZ(2*t*weight,wires = edge2)\n    qml.CNOT(wires = [edge1,edge2])\n\ndef MixerUnitary(edge1,t):\n    \"\"\"Creates the mixer unitary given by e^{-i t\\sum_i X_i}. In this function, it is decomposed into a Rz rotation\n\n    Args:\n        edge1 ([int]): [The edge to apply the circuit on]\n        t ([float]): [The t in the expression]\n    \"\"\"\n    qml.Hadamard(wires = edge1)\n    qml.RZ(2*t,wires = edge1)\n    qml.Hadamard(wires = edge1)\n\ndef OneLayer(gamma,beta):\n    \"\"\"Create the one layer of the QAOA algorithm\n\n    Args:\n        gamma ([float]): [The k'th value of gamma]\n        beta ([float]): [The k'th value of beta]\n    \"\"\"\n    for i,j,w in edges:\n        ProblemUnitary(i,j,gamma,w)\n    for i in graph.nodes():\n        MixerUnitary(i,beta)\n\ndef QAOAMaxCutAnsatz(parameters,**kwargs):\n    #This creates the QAOA ansatz\n    if len(np.shape(parameters))== 2:\n        p = len(parameters[0])\n    else:\n        p = len(parameters)//2 \n    for i in range(len(graph.nodes())):\n        qml.Hadamard(wires = i)\n    for i in range(p):\n        if len(np.shape(parameters))== 2:\n            OneLayer(parameters[0][i],parameters[1][i])\n        else:\n            OneLayer(parameters[0:p][i],parameters[p:][i])\n    #return [qml.sample(qml.PauliZ(i)) for i in range(len(graph.nodes()))]\n\ndef QAOAcomplete(p,init_parameters, Hamiltonian, dev, graph, steps = 200):\n    params = init_parameters\n\n    cost_function = qml.ExpvalCost(QAOAMaxCutAnsatz, Hamiltonian,dev, graph = graph) #diff_method = autograd currently\n    \n    opt = qml.AdamOptimizer()\n\n    for i in range(steps):\n        params = opt.step(cost_function,params)\n        if  (i+1)%steps == 0:\n            print(f'objective after step {i+1}: {cost_function(params)}')\n    \n    return params,cost_function(params)\n", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/1"}, "1": {"author": "josh", "date": "1635247938099", "content": "Hey @Viro, welcome \nI have a few tips that can help:\n\n\nPrefer to use return qml.expval(H) within a QNode, rather than qml.ExpvalCost(). This is because, if you return a Hamiltonian expectation within a QNode, PennyLane can perform various optimizations.\n\n\nFinally, a big speed advantage can be achieved by JITing the computation, as you suggest.\n\n\nBelow, I have  modified your code to use JAX, and have applied the @jax.jit operator to the QNode:\nimport pennylane as qml\nfrom networkx import Graph\nimport jax\nfrom jax import numpy as np\n\n\ndef ProblemUnitary(edge1, edge2, t, weight):\n    qml.CNOT(wires=[edge1, edge2])\n    qml.RZ(2 * t * weight, wires=edge2)\n    qml.CNOT(wires=[edge1, edge2])\n\n\ndef MixerUnitary(edge1, t):\n    qml.Hadamard(wires=edge1)\n    qml.RZ(2 * t, wires=edge1)\n    qml.Hadamard(wires=edge1)\n\n\ndef OneLayer(gamma, beta, graph):\n    for i, j in graph.edges():\n        ProblemUnitary(i, j, gamma, 1)\n    for i in graph.nodes():\n        MixerUnitary(i, beta)\n\n\ndev = qml.device(\"default.qubit\", wires=3)\ngraph = Graph([(0, 1), (1, 2), (2, 0)])\ncost_h, mixer_h = qml.qaoa.maxcut(graph)\ninit_parameters = np.ones([2, 2], dtype=np.float32)\n\n\n@jax.jit\n@qml.qnode(dev, interface=\"jax\")\ndef cost_function(parameters):\n    for i in range(len(graph.nodes())):\n        qml.Hadamard(wires=i)\n\n    for i in range(2):\n        OneLayer(parameters[0][i], parameters[1][i], graph)\n\n    return qml.expval(cost_h)\n\n\nparams = init_parameters\nsteps = 200\n\nopt = qml.AdamOptimizer()\n\nfor i in range(steps):\n    params, cost = opt.step_and_cost(\n        cost_function, params, grad_fn=jax.grad(cost_function)\n    )\n\n    if (i + 1) % 5 == 0:\n        print(f\"objective after step {i+1}: {cost}\")\n\nHere, I made some assumptions as to your original graph and initial parameters.\nYou should find that this runs a lot faster, once the initial compilation is complete!\nHowever, there are a couple of caveats to note with JIT support:\n\n\nCurrently, only default.qubit has JIT support, so applying @jax.jit will likely fail with other devices.\n\n\nUsing the JIT requires some restrictions in terms of classical processing. For example, if and for statements that depend on a tensor/array parameter are not allowed; note that I had to remove the if statements in your cost function.\n\n1", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/2"}, "2": {"author": "josh", "date": "1635248211614", "content": "Some timing data for 200 steps of the above script on my laptop:\n\nUsing interface=\"autograd\": 2.31s\nUsing interface=\"jax\": 60.95s\nUsing interface=\"jax\" and @jax.jit: 11.04s (note that this includes compilation time of 3.99s!)\n\nWhile Autograd is faster for my small 3 qubit example, I imagine that as the number of qubits scales, the @jax.jit approach will likely surpass autograd in terms of speed ", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/3"}, "3": {"author": "Viro", "date": "1635255106030", "content": "Hi again, thanks alot for the input and the quick reply!\nI tried your suggestion and used jax.jit as you mentioned, however, the code seems to be unable to compile the circuit for relatively small depths. I tried to run QAOA with depth p = 5 for a graph with 6 nodes and 3 edge pr. node, however, the code won\u2019t compile. Any ideas as to what is causing the compilation to halt?\nAutograd works fine though, runs relatively quickly \nAttaching the code here for convenience: PennyLaneHelp.py (4.3 KB)", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/4"}, "4": {"author": "CatalinaAlbornoz", "date": "1635343573510", "content": "Hi @Viro, what is the exact error you\u2019re getting?\nIn any case I would suggest checking the following which might help you:\n\nMake sure you\u2019re using the latest version of PennyLane and Python\nTry to break your problem into the smallest version of itself. This can help you debug and it can help us help you.\nTry a different example, such as this demo 1, and see if you get the same problem. If you do get the same problem then it\u2019s probably not a problem in your code.\n\nPlease let me know how it goes!", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/5"}, "5": {"author": "Viro", "date": "1636715533314", "content": "Hi again, sorry for the late reply.\nIt\u2019s been a while since a worked with that code in particular, but I believe the reason for the non-compilation was because of the @vectorize decorator I used, however, I am not sure of this.\nI have tried to work with the JAX workflow but find it a bit difficult. For instance, I tried to create a code that runs a circuit for several different initializations, however, whenever I try to optimize, I get the error\n\u201cCan\u2019t differentiate w.r.t. type <class \u2018jaxlib.xla_extension.DeviceArray\u2019>\u201d\nQAOARandomSearch.py (6.1 KB)\nIs there an obvious error here that makes it so that optimization is not possible?\nThanks in advance", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1636727336813", "content": "Hi @Viro!\nI get a different error message when I try to run your code. What versions of the different libraries and python are you using?\nAlso, could you please post the full error message? Thanks!", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/7"}, "7": {"author": "Viro", "date": "1636732444972", "content": "Here is an image of the full error message: \n432198\u00d7876 110 KB\n\nI think I am running python 3.8.8, pennylane 0.18, numpy 1.20.1 and jax 0.2.24", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1636770481600", "content": "Hi @Viro, I have looked into this and haven\u2019t been able to find an answer.\nWhat the error is basically saying is that it can\u2019t differentiate with respect to the output of your cost function.\nI would suggest that you try to simplify the problem as much as possible. Find the simplest piece of code that reproduces your error. This will be useful in finding how to fix the problem.\nIn line with this, try to create a problem with a single parameter and set the trainable parameter as trainable with requires_grad=True and for any non-trainable parameters use requires_grad=False . Not defining this often causes errors.\nIf you haven\u2019t been able to find the cause of the problem by Monday please let me know here and I will ask some PennyLane developers to take a look too.\nIf you do find the cause of the problem please also write it here so that others can benefit from the answer in the future.\nI wish I could help more but hopefully you will be able to find the cause by creating a Minimum Working Example.", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/9"}, "9": {"author": "josh", "date": "1636897946902", "content": "Hey @Viro!\nBased on your error message, it looks like autograd is being used for differentiation, and not JAX! For example, the error message is coming from autograd/tracer.py:\n\nimage1866\u00d766 79.2 KB\n\nThis could mean one of two things:\n\n\nYou are using qml.grad() to compute the derivative. This only works with Autograd; when using JAX, use jax.grad instead.\n\n\nThe QNode needs to be created with interface=\"jax\".\n\n\nLet me know if that helps!", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/10"}, "10": {"author": "Viro", "date": "1636901273996", "content": "Hmm, that\u2019s odd.\nDuring optimization, I call\n> params,cost = opt.step_and_cost(cost_function, params,grad_fun = jax.grad(cost_function))\nand the QNode indeed uses the \u2018jax\u2019 interface.  ", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/11"}, "11": {"author": "josh", "date": "1636902109090", "content": "Ah, that must be the issue; you want grad_fn=, not grad_fun!", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/12"}, "12": {"author": "Viro", "date": "1636905008295", "content": "You are indeed correct @josh, a small typo made all the difference \nThanks!\nI have one more question. When attempting to optimize, I want to set the parameters to different values at each iteration. However,\nfor i in range(len(gammaArray)):\n    params = np.ones((2,p))\n    print(gammaArray[i])\n    params.at[0].set(gammaArray[i])\n    params.at[1].set(betaArray[i])\n    print(params)\n    prevcost = 0\n    cost = cost_function(params)\n    j = 0\n    opt = qml.AdamOptimizer()\n    while (j < maxsteps):\n        params,cost = opt.step_and_cost(cost_function, params,grad_fn = jax.grad(cost_function))\n\nWhen printing out \u201cparams\u201d, I only get a ones((2,p)) array instead of the altered versions (Jax numpy does not support the typical\nparams[0,:] = gammaArray[i]\nparams[1,:] = betaArray[I]\n\nway of changing the arrays). How does one go about changing such \u201cjax\u201d arrays as one would do a regular numpy array so that I can use different initial points when optimizing?\nEDIT: calling params = np.array([gammaArray[i],betaArray[I]]) instead of the convoluted version I mentioned earlier solved the issue ", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/13"}, "13": {"author": "Viro", "date": "1636921519639", "content": "Either way, I am curious as to how many qubits it is expected to be able to compile for. I tried compiling for graphs of 5 nodes, which equates to 5 wires, which compiled reasonably quickly (like a minute or two). Now I tried to do the same for 10 qubits, however, I get the following message:\n\n111726\u00d7134 9.56 KB\n\nAt least from what I found, the compilation time Is heavily dependent on the number of wires. This message pops up multiple times as well. Is it a CPU hardware problem (I am running this on a low-spec 2017 Macbook pro) or some other problem that can be readily fixed?\nUpdate: it does compile in the end, it just takes like around 20 minutes ", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/14"}, "14": {"author": "CatalinaAlbornoz", "date": "1636992885908", "content": "@josh great catch on that typo!\n@Viro thank you for posting your solutions to the other problems you mentioned.\nI\u2019m not sure if it works with jax but I suggest you try using lightning.qubit instead of default.qubit. This can increase your speed by 3X or more.\nLet me know if this works!", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/15"}, "15": {"author": "Viro", "date": "1636921519639", "content": "Either way, I am curious as to how many qubits it is expected to be able to compile for. I tried compiling for graphs of 5 nodes, which equates to 5 wires, which compiled reasonably quickly (like a minute or two). Now I tried to do the same for 10 qubits, however, I get the following message:\n\n111726\u00d7134 9.56 KB\n\nAt least from what I found, the compilation time Is heavily dependent on the number of wires. This message pops up multiple times as well. Is it a CPU hardware problem (I am running this on a low-spec 2017 Macbook pro) or some other problem that can be readily fixed?\nUpdate: it does compile in the end, it just takes like around 20 minutes ", "link": "https://discuss.pennylane.ai//t/help-with-optimizers-and-speedups/1442/16"}}