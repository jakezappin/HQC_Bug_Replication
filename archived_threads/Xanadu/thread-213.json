{"0": {"author": "vijpandaturtle", "date": "1596792247811", "content": "Hi i am trying to make the weights of this network trainable in this tutorial https://pennylane.ai/qml/demos/tutorial_quanvolution.html 21\nusing this layer https://pennylane.readthedocs.io/en/stable/code/api/pennylane.qnn.KerasLayer.html?highlight=keraslayer#pennylane.qnn.KerasLayer 1\nHowever I am unsure as to what arguments i should pass for weight shapes and output dimensions. Can I please get some help ?\nThank you", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/1"}, "1": {"author": "Tom_Bromley", "date": "1596820677436", "content": "Hey @vijpandaturtle!\nHaving a look at the tutorial, is this the part you are trying to convert into a KerasLayer:\ndev = qml.device(\"default.qubit\", wires=4)\n# Random circuit parameters\nrand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n\n@qml.qnode(dev)\ndef circuit(phi=None):\n    # Encoding of 4 classical input values\n    for j in range(4):\n        qml.RY(np.pi * phi[j], wires=j)\n\n    # Random quantum circuit\n    RandomLayers(rand_params, wires=list(range(4)))\n\n    # Measurement producing 4 classical output values\n    return [qml.expval(qml.PauliZ(j)) for j in range(4)]\n\n(feel free to share any code you have and we can take a closer look)\nFor the above, you first need to update the signature of circuit() so that the phi argument is changed to inputs, since KerasLayer requires an argument of this name to pass input data to. You should also remove the =None part to make the gradient accessible with respect to the input data. You can then add rand_params as an argument to circuit.\nThen, we need to tell KerasLayer the shape of rand_params so they can be initialized within KerasLayer. To do this, you can define:\nweight_shapes = {\"rand_params\": (n_layers, 4)}\n\nIt is then simply a case of running\nqml.qnn.KerasLayer(circuit, weight_shapes, output_dim=4)\n\nto convert to a Keras-compatible layer.\nHope this helps!\nTom1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/2"}, "2": {"author": "vijpandaturtle", "date": "1596889432447", "content": "Thank you for the reply @Tom_Bromley\nThe problem with using a keras layer just for the circuit is that i cannot include the actual convolution function (pasted the code below) in the network\ndef quanv(image):\n    \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n    out = np.zeros((14, 14, 4))\n\n    # Loop over the coordinates of the top-left pixel of 2X2 squares\n    for j in range(0, 28, 2):\n        for k in range(0, 28, 2):\n            # Process a squared 2x2 region of the image with a quantum circuit\n            q_results = circuit(\n                phi=[image[j, k, 0], image[j, k + 1, 0], image[j + 1, k, 0], image[j + 1, k + 1, 0]]\n            )\n            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n            for c in range(4):\n                out[j // 2, k // 2, c] = q_results[c]\n    return out\n\ni even tried including the circuit logic inside the convolution function to see if it works\ndef quanv(inputs, conv_params):\n    out = np.zeros((14, 14, 4))\n\n    # Loop over the coordinates of the top-left pixel of 2X2 squares\n        for j in range(0, 28, 2):\n            for k in range(0, 28, 2):\n            # Process a squared 2x2 region of the image with a quantum circuit\n                win=[inputs[j, k, 0], inputs[j, k + 1, 0], inputs[j + 1, k, 0], inputs[j + 1, k + 1, 0]]\n            \n                for j in range(4):\n                    qml.RY(np.pi * win[j], wires=j)\n                RandomLayers(conv_params, wires=list(range(4)))\n                q_results = [qml.expval(qml.PauliZ(j)) for j in range(4)]\n            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n                for c in range(4):\n                    out[j // 2, k // 2, c] = q_results[c]\n    return out\n\nthere may be some issues with the array shapes but this is a rough sketch of what i\u2019m trying to do. I\u2019m probably going wrong in many place please do let me know  ", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/3"}, "3": {"author": "Tom_Bromley", "date": "1597063541862", "content": "Hey @vijpandaturtle,\nAh ok I see, good question! Since the current KerasLayer doesn\u2019t support applying this style of convolution, you could create your own Keras layer that does!\nThis layer could inherit from qml.qnn.KerasLayer and edit the call() method to apply the convolution. I had a quick go at doing this to give you an idea:\nimport tensorflow as tf\n\nclass ConvQLayer(qml.qnn.KerasLayer):\n    \n    def call(self, inputs):\n        \n        batches = inputs.shape[0]\n        out = tf.Variable(tf.zeros((batches, 14, 14, 4)))\n        \n        # Loop over the coordinates of the top-left pixel of 2X2 squares\n        for j in range(0, 28, 2):\n            for k in range(0, 28, 2):\n                # Process a squared 2x2 region of the image with a quantum circuit\n                qnode_inputs = tf.stack([inputs[:, j, k, 0], inputs[:, j, k + 1, 0], inputs[:, j + 1, k, 0], inputs[:, j + 1, k + 1, 0]], axis=1)\n                q_results = super().call(qnode_inputs)\n\n                out[:, j // 2, k // 2].assign(q_results)\n\n        return out\n\nqlayer_conv = ConvQLayer(circuit, weight_shapes, output_dim=4)\nbatches = 2\ninputs = np.random.random((batches, 28, 28, 3))\nout = qlayer_conv(inputs)\n\nThis probably needs a lot more work to double check that it\u2019s functioning as expected, but this would be the general idea!\nThanks,\nTom1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/4"}, "4": {"author": "vijpandaturtle", "date": "1597064363148", "content": "Thank you so much for that !! 1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/5"}, "5": {"author": "vijpandaturtle", "date": "1597689725545", "content": "@Tom_Bromley I am using the same code snippet and it gives me a warnings \u201cgradients do not exist\u201d.  I have set the argument name to \u2018inputs\u2019 and removed the \u2018None\u2019 assigned to it. Is there anything else that needs to be done to make gradients accessible ?\nThanks", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/6"}, "6": {"author": "antalszava", "date": "1597704039731", "content": "Hi @vijpandaturtle,\nCould you help out with posting the code snippet that results in the warnings?  From the PennyLane side, the previous suggestions by Tom should suffice for making the gradients be accessible. These warnings seem to be specific to TensorFlow and in certain cases still without specific resolutions. Having said that, perhaps we could uncover something specific to the example. ", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/7"}, "7": {"author": "vijpandaturtle", "date": "1597760915719", "content": "@antalszava the gradient error seems to go away once i updated the tensorflow version. however, i am still getting errors regarding matrix compatibility\nInvalidArgumentError: Matrix size-incompatible: In[0]: [4,784], In[1]: [4,10] [Op:MatMul]\n\nHow does the keras layer handle batch processing ? I think that might the reason for this issue.\nbatches = inputs.shape[0]\nout = tf.Variable(tf.zeros((batches, 14, 14, 4)))\n        \n", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/8"}, "8": {"author": "antalszava", "date": "1597793541054", "content": "Hi @vijpandaturtle,\nThat\u2019s great news!\nA single KerasLayer object is meant for a single batch. An example of how integrating KerasLayer with a tf.keras.models.Sequential model 7 and specifying the number of batches for the model as a whole can be found in the Additional example section.\nFor the  InvalidArgumentError it indeed seems like there is something going on with the shapes, which could be related to batching (the second component of [4,784] for example seems to be the product of 14, 14, 4).\nUnfortunately, it is challenging to see what exactly could be going wrong without the complete code snippet that yields the error. Could you please post it?1 Reply", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/9"}, "9": {"author": "vijpandaturtle", "date": "1597854709556", "content": "Thank you for the reply @antalszava here are the code snippets\nclass ConvQLayer(qml.qnn.KerasLayer):\n    \n    def call(self, inputs):\n        \n        batches = inputs.shape[0]\n        out = tf.Variable(tf.zeros((batches, 14, 14, 4)))\n        \n        # Loop over the coordinates of the top-left pixel of 2X2 squares\n        for j in range(0, 28, 2):\n            for k in range(0, 28, 2):\n                # Process a squared 2x2 region of the image with a quantum circuit\n                qnode_inputs = tf.stack([inputs[:, j, k, 0], inputs[:, j, k + 1, 0], inputs[:, j + 1, k, 0], inputs[:, j + 1, k + 1, 0]], axis=1)\n                q_results = super().call(qnode_inputs)\n                out[:, j // 2, k // 2, :].assign(q_results)\n        return out\n\nweight_shapes = {'conv_params': (n_layers,4)}\n\ndef MyModel():\n    \"\"\"Initializes and returns a custom Keras model\n    which is ready to be trained.\"\"\"\n    model = keras.models.Sequential([\n        ConvQLayer(circuit, weight_shapes, output_dim=4),\n        keras.layers.Flatten(),\n        keras.layers.Dense(10, activation=\"softmax\")\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\nq_model = MyModel()\n\nq_history = q_model.fit(\n    train_images,\n    train_labels,\n    validation_data=(test_images, test_labels),\n    batch_size=4,\n    epochs=n_epochs,\n    verbose=2,\n)\n\n\n", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/10"}, "10": {"author": "Tom_Bromley", "date": "1597865406229", "content": "Hey @vijpandaturtle,\nI had another look just now and also had a problem with accessing the gradient. Unfortunately it wasn\u2019t clear to me what the problem was  This will likely take some time troubleshooting before we work out the problem. Remember that KerasLayer does not support convolution, so this is definitely a more \u201cresearch\u201d level question. If you\u2019re interested in looking deeper into this and providing some feedback that would be great, otherwise I\u2019d recommend following the established style in the tutorial.", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/11"}, "11": {"author": "vijpandaturtle", "date": "1597869625123", "content": "@Tom_Bromley thank you for the reply. yes i will look into it a little deeper this time. however, do let me know if you find something 1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/12"}, "12": {"author": "Emmanuel_OM", "date": "1599190692558", "content": "Hi @antalszava,\nI have faced the same issue. Have you found a fix to this? It will be very helpful. Thanks", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/13"}, "13": {"author": "Tom_Bromley", "date": "1599238619028", "content": "Hey @Emmanuel_OM, welcome to the forum!\nCould you elaborate on the issue you\u2019re facing? It can also be useful to share your complete code so that we can troubleshoot the issue more directly.\nThanks!", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/14"}, "14": {"author": "Emmanuel_OM", "date": "1599257065312", "content": "Thank you for the reply @Tom_Bromley. I am using the code snippets that @vijpandaturtle report above. The only difference is the optimizer and loss function (used for a binary classification problem)\n    def MyModel():\n    \"\"\"Initializes and returns a custom Keras model\n    which is ready to be trained.\"\"\"\n    model = keras.models.Sequential([\n        ConvQLayer(circuit, weight_shapes, output_dim=4),\n        keras.layers.Flatten(),\n        keras.layers.Dense(1, activation=\"sigmoid\")\n    ])\n\nmodel.compile(\n        optimizer='adam',\n        loss=\"binary_crossentropy\",\n        metrics=[\"binary_accuracy\"],\n    )\n    return model\n\n", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/15"}, "15": {"author": "Tom_Bromley", "date": "1599599070897", "content": "Thanks @Emmanuel_OM. From my side I didn\u2019t have any breakthrough with getting the gradient from ConvQLayer to be accessible. Not sure if @vijpandaturtle had any luck?\nUnfortunately, this approach to a quantum convolutional layer does not work out of the box with KerasLayer and would require some careful work to get the gradients properly accessible. For now, I\u2019d recommend using the core PennyLane approach (i.e., without KerasLayer) that is used in the tutorial 7.\nThanks!", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/16"}, "16": {"author": "dymat", "date": "1605782028448", "content": "Hi there!\nWe\u2019ve implemented a trainable QuanvolutionLayer for PyTorch:\nclass QonvLayer(nn.Module):\n    def __init__(self, stride=2, device=\"default.qubit\", wires=4, circuit_layers=4, n_rotations=8, out_channels=4, seed=None):\n\tsuper(QonvLayer, self).__init__()\n\t\n\t# init device\n\tself.wires = wires\n\tself.dev = qml.device(device, wires=self.wires)\n\t\n\tself.stride = stride\n\tself.out_channels = min(out_channels, wires)\n\t\n\tif seed is None:\n\t    seed = np.random.randint(low=0, high=10e6)\n\t    \n\tprint(\"Initializing Circuit with random seed\", seed)\n\t\n\t# random circuits\n\t@qml.qnode(device=self.dev)\n\tdef circuit(inputs, weights):\n\t    n_inputs=4\n\t    # Encoding of 4 classical input values\n\t    for j in range(n_inputs):\n\t        qml.RY(inputs[j], wires=j)\n\t    # Random quantum circuit\n\t    RandomLayers(weights, wires=list(range(self.wires)), seed=seed)\n\t    \n\t    # Measurement producing 4 classical output values\n\t    return [qml.expval(qml.PauliZ(j)) for j in range(self.out_channels)]\n\t\n\tweight_shapes = {\"weights\": [circuit_layers, n_rotations]}\n\tself.circuit = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n    \n    \n    def draw(self):\n\t# build circuit by sending dummy data through it\n\t_ = self.circuit(inputs=torch.from_numpy(np.zeros(4)))\n\tprint(self.circuit.qnode.draw())\n\tself.circuit.zero_grad()\n\t\n    \n    def forward(self, img):\n\tbs, h, w, ch = img.size()\n\tif ch > 1:\n\t    img = img.mean(axis=-1).reshape(bs, h, w, 1)\n\t                \n\tkernel_size = 2        \n\th_out = (h-kernel_size) // self.stride + 1\n\tw_out = (w-kernel_size) // self.stride + 1\n\t\n\t\n\tout = torch.zeros((bs, h_out, w_out, self.out_channels))\n\t\n\t# Loop over the coordinates of the top-left pixel of 2X2 squares\n\tfor b in range(bs):\n\t    for j in range(0, h_out, self.stride):\n\t        for k in range(0, w_out, self.stride):\n\t            # Process a squared 2x2 region of the image with a quantum circuit\n\t            q_results = self.circuit(\n\t                inputs=torch.Tensor([\n\t                    img[b, j, k, 0],\n\t                    img[b, j, k + 1, 0],\n\t                    img[b, j + 1, k, 0],\n\t                    img[b, j + 1, k + 1, 0]\n\t                ])\n\t            )\n\t            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n\t            for c in range(self.out_channels):\n\t                out[b, j // kernel_size, k // kernel_size, c] = q_results[c]\n\t                \n\t         \n\treturn out\n\nExperiment I: Training with 1 Quanvolutional Layer\nNet:\nmodel = torch.nn.Sequential(\n    QonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n    torch.nn.Flatten(),\n    torch.nn.Linear(in_features=14*14*4, out_features=10)\n)\n\nTraining output:\nEpoch: 0 \tStep: 0 \tAccuracy: 0.25 \tLoss: 2.353778839111328\nGradients Layer 0\ntensor([[-4.6585e-03, -1.5023e-01,  1.0962e-17,  3.5731e-18],\n\t[-4.6677e-03,  2.9001e-02, -9.6852e-19,  0.0000e+00]])\nCurrent Circuit:\n 0: \u2500\u2500RY(0.0)\u2500\u2500RZ(0.397)\u2500\u2500\u2500RZ(1.178)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500RZ(4.088)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RZ(3.61)\u2500\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RY(4.173)\u2500\u2500\u2570C\u2500\u2500RY(5.072)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500RZ(1.785)\u2500\u2500\u2500RZ(5.903)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n\n---------------------------------------\nEpoch: 0 \tStep: 1 \tAccuracy: 0.0 \tLoss: 3.284860610961914\nGradients Layer 0\ntensor([[-2.2039e-02, -4.9558e-01,  2.5899e-16, -1.4661e-16],\n\t[-1.2097e-02,  2.3364e-01,  1.9031e-16,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 2 \tAccuracy: 0.25 \tLoss: 2.0575411319732666\nGradients Layer 0\ntensor([[-1.3089e-02, -9.6094e-02,  8.8986e-17,  9.1110e-17],\n\t[-7.3473e-03,  6.8553e-02,  6.4072e-18,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 3 \tAccuracy: 0.25 \tLoss: 3.791848659515381\nGradients Layer 0\ntensor([[-8.5180e-02, -6.7926e-01,  2.9336e-16, -6.0067e-16],\n\t[-5.4367e-02,  3.8367e-01, -2.3463e-16,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 4 \tAccuracy: 0.0 \tLoss: 4.429379463195801\nGradients Layer 0\ntensor([[-5.6071e-02, -9.5350e-01, -4.0188e-16, -5.2387e-16],\n\t[-4.3445e-02,  4.8110e-01,  8.5049e-18,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 5 \tAccuracy: 0.0 \tLoss: 2.415179967880249\nGradients Layer 0\ntensor([[-3.7586e-02, -3.1990e-01,  6.1641e-19, -7.2152e-17],\n\t[-2.7385e-02,  1.1129e-01,  2.5546e-18,  0.0000e+00]])\nCurrent Circuit:\n 0: \u2500\u2500RY(0.0)\u2500\u2500RZ(0.397)\u2500\u2500\u2500RZ(1.178)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500RZ(4.13)\u2500\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RZ(3.653)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RY(4.216)\u2500\u2500\u2570C\u2500\u2500RY(5.03)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500RZ(1.785)\u2500\u2500\u2500RZ(5.903)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n\n---------------------------------------\nEpoch: 0 \tStep: 6 \tAccuracy: 0.25 \tLoss: 2.0272059440612793\nGradients Layer 0\ntensor([[-1.3096e-03, -1.6318e-01, -5.6946e-18,  1.0381e-17],\n\t[-2.0847e-03,  3.5787e-02, -5.7634e-18,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 7 \tAccuracy: 0.25 \tLoss: 3.111910820007324\nGradients Layer 0\ntensor([[-4.7392e-02, -5.0553e-01,  1.0673e-16,  3.0912e-17],\n\t[-3.8066e-02,  2.5993e-01, -1.4139e-16,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 8 \tAccuracy: 0.0 \tLoss: 2.9227261543273926\nGradients Layer 0\ntensor([[-5.8086e-02, -3.5329e-01,  3.0340e-17,  1.1894e-16],\n\t[-4.1156e-02,  1.4573e-01,  1.4530e-16,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 9 \tAccuracy: 0.0 \tLoss: 2.6818065643310547\nGradients Layer 0\ntensor([[-1.1859e-01, -2.2195e-01,  8.9172e-18, -3.7483e-17],\n\t[-8.8680e-02,  2.1465e-01,  4.7048e-18,  0.0000e+00]])\n---------------------------------------\nEpoch: 0 \tStep: 10 \tAccuracy: 0.0 \tLoss: 2.707582950592041\nGradients Layer 0\ntensor([[-6.6730e-03, -3.5080e-01,  9.0117e-18, -9.0980e-19],\n\t[ 4.0098e-03,  6.0043e-02,  1.0396e-17,  0.0000e+00]])\nCurrent Circuit:\n 0: \u2500\u2500RY(0.0)\u2500\u2500RZ(0.397)\u2500\u2500\u2500RZ(1.178)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500RZ(4.172)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RZ(3.695)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RY(4.258)\u2500\u2500\u2570C\u2500\u2500RY(4.991)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500RZ(1.785)\u2500\u2500\u2500RZ(5.903)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n\nTraining takes really long, but at least the net achieves ca. 70%-80% accuracy on MNIST.\nExperiment II: Training with 2 Quanvolutional Layer\nNet:\nmodel = torch.nn.Sequential(\n    QonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n    QonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n    torch.nn.Flatten(),\n    torch.nn.Linear(in_features=7*7*4, out_features=10)\n)\n\nTraining output:\nEpoch: 0 \tStep: 0 \tAccuracy: 0.25 \tLoss: 2.3005757331848145\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-9.2107e-03, -3.4147e-02, -9.6166e-03, -1.6073e-02],\n\t[-9.2107e-03, -3.4147e-02, -3.1461e-03,  7.0380e-18]])\nCurrent Circuit Layer 0:\n 0: \u2500\u2500RY(0.0)\u2500\u2500\u2500RX(3.225)\u2500\u2500\u2500RX(3.592)\u2500\u2500RX(5.593)\u2500\u2500\u2500RX(5.953)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RX(2.63)\u2500\u2500\u2500RY(4.176)\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RX(2.43)\u2500\u2500RY(2.163)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n\nCurrent Circuit Layer 1:\n 0: \u2500\u2500RY(0.0)\u2500\u2500\u2500RY(1.79)\u2500\u2500RY(1.847)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500\u2500RY(2.06)\u2500\u2500RY(1.588)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RZ(2.124)\u2500\u2500\u256dX\u2500\u2500RZ(4.867)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500\u2570C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RY(1.193)\u2500\u2500\u2570C\u2500\u2500RY(3.918)\u2500\u2500\u2524 \u27e8Z\u27e9 \n\n---------------------------------------\nEpoch: 0 \tStep: 1 \tAccuracy: 0.0 \tLoss: 2.500396251678467\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-4.7077e-02, -1.2101e-01, -2.1126e-01,  1.4875e-02],\n\t[-4.7077e-02, -1.2101e-01, -1.1339e-02, -5.2808e-18]])\n---------------------------------------\nEpoch: 0 \tStep: 2 \tAccuracy: 0.25 \tLoss: 2.1083250045776367\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[ 1.0099e-01, -6.7789e-03,  1.0940e-01, -3.1570e-02],\n\t[ 1.0099e-01, -6.7789e-03, -5.4767e-03, -1.1648e-17]])\n---------------------------------------\nEpoch: 0 \tStep: 3 \tAccuracy: 0.25 \tLoss: 2.5666348934173584\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-1.6059e-01, -1.4263e-01, -3.8268e-01,  6.0618e-02],\n\t[-1.6059e-01, -1.4263e-01, -1.1166e-02, -9.7793e-19]])\n---------------------------------------\nEpoch: 0 \tStep: 4 \tAccuracy: 0.0 \tLoss: 2.981722593307495\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-4.0437e-01, -3.6849e-01, -6.9833e-01,  1.3052e-01],\n\t[-4.0437e-01, -3.6849e-01, -6.2979e-03,  1.3407e-17]])\n---------------------------------------\nEpoch: 0 \tStep: 5 \tAccuracy: 0.25 \tLoss: 2.1014046669006348\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[ 1.9919e-03,  1.2177e-02, -2.0729e-02, -1.3597e-02],\n\t[ 1.9919e-03,  1.2177e-02, -8.4981e-03, -2.5948e-19]])\nCurrent Circuit Layer 0:\n 0: \u2500\u2500RY(0.0)\u2500\u2500\u2500RX(3.225)\u2500\u2500\u2500RX(3.592)\u2500\u2500RX(5.593)\u2500\u2500\u2500RX(5.953)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RX(2.63)\u2500\u2500\u2500RY(4.176)\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RX(2.43)\u2500\u2500RY(2.163)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dC\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570X\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n\nCurrent Circuit Layer 1:\n 0: \u2500\u2500RY(0.0)\u2500\u2500\u2500RY(1.81)\u2500\u2500\u2500RY(1.867)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 1: \u2500\u2500RY(0.0)\u2500\u2500\u2500RY(2.099)\u2500\u2500RY(1.627)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u27e8Z\u27e9 \n 2: \u2500\u2500RY(0.0)\u2500\u2500\u256dX\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RZ(2.116)\u2500\u2500\u256dX\u2500\u2500RZ(4.867)\u2500\u2500\u2524 \u27e8Z\u27e9 \n 3: \u2500\u2500RY(0.0)\u2500\u2500\u2570C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500RY(1.223)\u2500\u2500\u2570C\u2500\u2500RY(3.964)\u2500\u2500\u2524 \u27e8Z\u27e9 \n\n---------------------------------------\nEpoch: 0 \tStep: 6 \tAccuracy: 0.25 \tLoss: 2.009097099304199\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[ 5.2628e-02,  2.8042e-02,  1.7530e-01, -4.5965e-02],\n\t[ 5.2628e-02,  2.8042e-02, -2.1108e-03, -8.3491e-18]])\n---------------------------------------\nEpoch: 0 \tStep: 7 \tAccuracy: 0.0 \tLoss: 2.7671358585357666\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-4.1022e-01, -2.2605e-01, -4.9493e-01,  1.0710e-01],\n\t[-4.1022e-01, -2.2605e-01,  1.3541e-02,  2.0550e-17]])\n---------------------------------------\nEpoch: 0 \tStep: 8 \tAccuracy: 0.0 \tLoss: 2.595287799835205\nGradients Layer 0:\nNone\nGradients Layer 1:\ntensor([[-2.7742e-01, -1.4687e-01, -4.3393e-01,  1.1318e-01],\n\t[-2.7742e-01, -1.4687e-01, -3.8294e-03,  1.2389e-17]])\n---------------------------------------\n\nAs you can see, only the Quanvolutional Layer 1 receives gradients. Layer 0 does not get any gradients and hence is not updated by the optimizer.\nNow my question is: why? (i.e. What am I missing? What am I doing wrong? Or am I facing a bug?)\nThanks in advance!\nDenny\nPS: we are using PyTorch 1.4.0 with PennyLane v0.12.0.1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/17"}, "17": {"author": "vijpandaturtle", "date": "1605788066435", "content": "@dymat this is awesome ! Thanks so much for posting this here.\nSince you are training MNIST dataset, can you share a simple but complete snippet which i can run myself (if that\u2019s okay with you)? I can have a look at it and see if i find the source of your problem.\nAgain thanks, i hit a roadblock on this, happy to see a way forward ", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/18"}, "18": {"author": "dymat", "date": "1605789283106", "content": "Hi @vijpandaturtle,\nthanks for your reply. Since new users (like me) cannot attach files to threads, I am posting it right here.\nI hope you can help me find out why gradients are not propagated through the quantum layer.\nLooking forward to hear from you,\n~dymat\nUPDATED CODE (FIXES WRONG INDENTATIONS)\n# coding: utf-8\n\n# In[2]:\n\n\nimport torch\nfrom torch import nn\n\nimport torchvision\n\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.templates import RandomLayers\n\nfrom sklearn.metrics import accuracy_score\n\n\n# In[3]:\n\n\nclass QonvLayer(nn.Module):\n    def __init__(self, stride=2, device=\"default.qubit\", wires=4, circuit_layers=4, n_rotations=8, out_channels=4, seed=None):\n\tsuper(QonvLayer, self).__init__()\n\t\n\t# init device\n\tself.wires = wires\n\tself.dev = qml.device(device, wires=self.wires)\n\t\n\tself.stride = stride\n\tself.out_channels = min(out_channels, wires)\n\t\n\tif seed is None:\n\t    seed = np.random.randint(low=0, high=10e6)\n\t    \n\tprint(\"Initializing Circuit with random seed\", seed)\n\t\n\t# random circuits\n\t@qml.qnode(device=self.dev)\n\tdef circuit(inputs, weights):\n\t    n_inputs=4\n\t    # Encoding of 4 classical input values\n\t    for j in range(n_inputs):\n\t        qml.RY(inputs[j], wires=j)\n\t    # Random quantum circuit\n\t    RandomLayers(weights, wires=list(range(self.wires)), seed=seed)\n\t    \n\t    # Measurement producing 4 classical output values\n\t    return [qml.expval(qml.PauliZ(j)) for j in range(self.out_channels)]\n\t\n\tweight_shapes = {\"weights\": [circuit_layers, n_rotations]}\n\tself.circuit = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n    \n    \n    def draw(self):\n\t# build circuit by sending dummy data through it\n\t_ = self.circuit(inputs=torch.from_numpy(np.zeros(4)))\n\tprint(self.circuit.qnode.draw())\n\tself.circuit.zero_grad()\n\t\n    \n    def forward(self, img):\n\tbs, h, w, ch = img.size()\n\tif ch > 1:\n\t    img = img.mean(axis=-1).reshape(bs, h, w, 1)\n\t                \n\tkernel_size = 2        \n\th_out = (h-kernel_size) // self.stride + 1\n\tw_out = (w-kernel_size) // self.stride + 1\n\t\n\t\n\tout = torch.zeros((bs, h_out, w_out, self.out_channels))\n\t\n\t# Loop over the coordinates of the top-left pixel of 2X2 squares\n\tfor b in range(bs):\n\t    for j in range(0, h_out, self.stride):\n\t        for k in range(0, w_out, self.stride):\n\t            # Process a squared 2x2 region of the image with a quantum circuit\n\t            q_results = self.circuit(\n\t                inputs=torch.Tensor([\n\t                    img[b, j, k, 0],\n\t                    img[b, j, k + 1, 0],\n\t                    img[b, j + 1, k, 0],\n\t                    img[b, j + 1, k + 1, 0]\n\t                ])\n\t            )\n\t            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n\t            for c in range(self.out_channels):\n\t                out[b, j // kernel_size, k // kernel_size, c] = q_results[c]\n\t                \n\t         \n\treturn out\n\n\n# In[16]:\n\n\nqonv = QonvLayer(circuit_layers=2, n_rotations=4, out_channels=4, stride=2)\nqonv.draw()\nx = torch.rand(size=(10,28,28,1))\nqonv(x).shape\n\n\n# In[17]:\n\n\ndef transform(x):\n    x = np.array(x)\n    x = x/255.0\n    \n    return torch.from_numpy(x).float()\n\n\n# In[18]:\n\n\ntrain_set = torchvision.datasets.MNIST(root='./mnist', train=True, download=True, transform=transform)\ntest_set = torchvision.datasets.MNIST(root='./mnist', train=False, download=True, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=4)\n\n\n# # Experiment I (one Quanvolutional Layer)\n\n# In[49]:\n\n\ndef training_experiment_1():\n    print(\"Starting Experiment I\")\n\n    model = torch.nn.Sequential(\n\tQonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n\ttorch.nn.Flatten(),\n\ttorch.nn.Linear(in_features=14*14*4, out_features=10)\n    )\n\n    model.train()\n\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(1):\n\tfor i, (x, y) in enumerate(train_loader):\n\n\t    # prepare inputs and labels\n\t    x = x.view(-1, 28, 28, 1)\n\t    y = y.long()\n\n\t    # reset optimizer\n\t    optimizer.zero_grad()\n\n\t    # engage\n\t    y_pred = model(x)\n\n\t    # error, gradients and optimization\n\t    loss = criterion(y_pred, y)  \n\t    loss.backward()\n\t    optimizer.step()\n\n\t    # output\n\t    acc = accuracy_score(y, y_pred.argmax(-1).numpy())       \n\n\t    print(\"Epoch:\", epoch, \"\\tStep:\", i, \"\\tAccuracy:\", acc, \"\\tLoss:\", loss.item())\n\t    print(\"Gradients Layer 0:\")\n\t    print(model[0].circuit.weights.grad)\n\n\t    if i % 5 == 0:\n\t        model[0].draw()\n\t    \n\t    print(\"---------------------------------------\")\n\t    \n\t    # early break\n\t    if i > 0 and i % 10 == 0:\n\t        break\n\t    \n    return model\n\n\n# # Experiment II (two stacked Quanvolutional Layers)\n\n# In[48]:\n\n\ndef training_experiment_2():\n    print(\"Starting Experiment II\")\n\n    model = torch.nn.Sequential(\n\tQonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n\tQonvLayer(stride=2, circuit_layers=2, n_rotations=4, out_channels=4),\n\ttorch.nn.Flatten(),\n\ttorch.nn.Linear(in_features=7*7*4, out_features=10)\n    )\n\n    model.train()\n\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(50):\n\tfor i, (x, y) in enumerate(train_loader):\n\n\t    # prepare inputs and labels\n\t    x = x.view(-1, 28, 28, 1)\n\t    y = y.long()\n\n\t    # reset optimizer\n\t    optimizer.zero_grad()\n\n\t    # engage\n\t    y_pred = model(x)\n\n\t    # error, gradients and optimization\n\t    loss = criterion(y_pred, y)  \n\t    loss.backward()\n\t    optimizer.step()\n\n\n\t    # output\n\t    acc = accuracy_score(y, y_pred.argmax(-1).numpy())  \n\n\t    print(\"Epoch:\", epoch, \"\\tStep:\", i, \"\\tAccuracy:\", acc, \"\\tLoss:\", loss.item())\n\t    print(\"Gradients Layer 0:\")\n\t    print(model[0].circuit.weights.grad)\n\t    print(\"Gradients Layer 1:\")\n\t    print(model[1].circuit.weights.grad)\n\n\t    if i % 5 == 0:\n\t        print(\"Current Circuit Layer 0:\")\n\t        model[0].draw()\n\t        print(\"Current Circuit Layer 1:\")\n\t        model[1].draw()\n\n\t    print(\"---------------------------------------\")\n\t    \n\t    # early break\n\t    if i > 0 and i % 10 == 0:\n\t        break\n\t    \n    return model\n\n\n# In[ ]:\n\n\nif __name__ == \"__main__\":\n    training_experiment_1()\n    training_experiment_2()", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/19"}, "19": {"author": "glassnotes", "date": "1605823108532", "content": "Hi @dymat, welcome, and thanks for your question!\nI\u2019d like to run your code and reproduce the results. The indentation doesn\u2019t seem to have been preserved when it was pasted into the code block though. To make sure I am running it exactly as intended, could you please reformat it? Thanks very much!1 Reply", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/20"}, "20": {"author": "dymat", "date": "1605861599724", "content": "Hi @glassnotes,\nI updated the code in the post above post above.\nIt should work now out of the box. If not, check out my repo at github: https://github.com/dymat/TrainableQuanvolution 18\nBest regards,\n~dymat", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/21"}, "21": {"author": "vijpandaturtle", "date": "1605866178791", "content": "@dymat thank you for posting the code I will try to run it and get back to you asap 1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/22"}, "22": {"author": "Tom_Bromley", "date": "1605912276172", "content": "Thanks @dymat, it\u2019s very cool that you had some positive results training on MNIST (at least for the 1 quantum layer). If you\u2019re interested, I think this would make a great demo! We have just launched a community space 5 for demos. Since you already have a Jupyter Notebook, is should just be a case of creating an issue here 1 (see instructions).\nOn to your question about the gradient for two layers, I had a quick look and unfortunately nothing jumped out as a problem. I might have a chance to dig a bit deeper next week, however for now I would suggest some debugging. For example, what if we swapped out the quantum circuit (self.circuit) with a simple torch function 3? If you still aren\u2019t getting a gradient in the first layer, it\u2019s probably an issue with the big for loop in forward(). Otherwise, we can look at if it\u2019s an issue on the PL side.\nHope this helps a bit!\nTom2 Replies", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/23"}, "23": {"author": "vijpandaturtle", "date": "1606109572161", "content": "@dymat went through your code and ran it a few times with some changes. I haven\u2019t found the source of the gradient problem yet. But will still keep looking and update if i find something!", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/24"}, "24": {"author": "dymat", "date": "1606119484556", "content": "Hi @Tom_Bromley,\npublishing our notebook as a demo sounds cool. I would use the one-layer example. Can we change the demo notebook later on, if we find the source of the gradient problem in two-layer networks?\nBest regards,\nD", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/25"}, "25": {"author": "dymat", "date": "1606141766602", "content": "\n\n\n dymat:\n\nCan we change the demo notebook later on, if we find the source of the gradient problem in two-layer networks?\n\n\nOk, I\u2019ve  checked the instructions and see that my question is obsolete, since the code lives in my repo.1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/26"}, "26": {"author": "glassnotes", "date": "1606166308949", "content": "Hi @dymat, awesome, we\u2019re looking forward to seeing your demo! Let us know if you have any further questions while getting it ready.", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/27"}, "27": {"author": "dymat", "date": "1606170723455", "content": "Hi @glassnotes, the demo request is submitted (https://github.com/PennyLaneAI/qml/issues/165 15).\nBest regards,\nD2", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/28"}, "28": {"author": "gojiita_ku", "date": "1676648755085", "content": "Hi @dymat @Tom_Bromley , has this issue of quantum layer 0 not getting any gradients been solved? Any update?", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/29"}, "29": {"author": "CatalinaAlbornoz", "date": "1677260403503", "content": "Hi @gojiita_ku, thank you for your question and sorry for the delay in responding!\nI haven\u2019t seen gradient issues recently. Are you experiencing a similar issue?\nIf so, please share with us a minimal version of your code, the output of qml.about(), and your full error traceback.\nIf you run our quanvolutional neural networks demo 1 as-is, does it work properly?\nI\u2019ll be looking out for your answer!1 Reply1", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/30"}, "30": {"author": "gojiita_ku", "date": "1677431208333", "content": "\n\n\n Tom_Bromley:\n\nThanks @dymat, it\u2019s very cool that you had some positive results training on MNIST (at least for the 1 quantum layer). If you\u2019re interested, I think this would make a great demo! We have just launched a community space for demos. Since you already have a Jupyter Notebook, is should just be a case of creating an issue here (see instructions).\nOn to your question about the gradient for two layers, I had a quick look and unfortunately nothing jumped out as a problem. I might have a chance to dig a bit deeper next week, however for now I would suggest some debugging. For example, what if we swapped out the quantum circuit (self.circuit) with a simple torch function? If you still aren\u2019t getting a gradient in the first layer, it\u2019s probably an issue with the big for loop in forward(). Otherwise, we can look at if it\u2019s an issue on the PL side.\nHope this helps a bit!\nTom\n\n\nHi @Tom_Bromley and @CatalinaAlbornoz,\nCould you please give a code example in this case about how to swap out the quantum circuit (self.circuit ) with a simple torch function as suggested by Tom?\n\nFor example, what if we swapped out the quantum circuit (self.circuit ) with a simple torch function ?\n\nMany thanks!", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/31"}, "31": {"author": "gojiita_ku", "date": "1677491357956", "content": "Hi @CatalinaAlbornoz. I run this notebook 1 and found parameters in both of two QonvLayers were updated successfully every epoch. But if I change the code a little bit (e.g. build the model using nn.Module instead of nn.Sequential), I found parameters in the first QonvLayer were not updated. I don\u2019t know the reason. I\u2019ve uploaded my notebook file. Could you please take a look if you have free time? Many thanks!\nStackedQuanvolutions.py (5.3 KB)\nHere is the output of qml.about():\nName: PennyLane\nVersion: 0.28.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /Users/chenyixiong/opt/anaconda3/lib/python3.9/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, retworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning, PennyLane-qiskit, pennylane-qulacs\nPlatform info:           macOS-10.16-x86_64-i386-64bit\nPython version:          3.9.13\nNumpy version:           1.21.5\nScipy version:           1.9.1\nInstalled devices:\n- lightning.qubit (PennyLane-Lightning-0.28.2)\n- qiskit.aer (PennyLane-qiskit-0.28.0)\n- qiskit.basicaer (PennyLane-qiskit-0.28.0)\n- qiskit.ibmq (PennyLane-qiskit-0.28.0)\n- qiskit.ibmq.circuit_runner (PennyLane-qiskit-0.28.0)\n- qiskit.ibmq.sampler (PennyLane-qiskit-0.28.0)\n- qulacs.simulator (pennylane-qulacs-0.28.0)\n- default.gaussian (PennyLane-0.28.0)\n- default.mixed (PennyLane-0.28.0)\n- default.qubit (PennyLane-0.28.0)\n- default.qubit.autograd (PennyLane-0.28.0)\n- default.qubit.jax (PennyLane-0.28.0)\n- default.qubit.tf (PennyLane-0.28.0)\n- default.qubit.torch (PennyLane-0.28.0)\n- default.qutrit (PennyLane-0.28.0)\n- null.qubit (PennyLane-0.28.0)\n", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/32"}, "32": {"author": "CatalinaAlbornoz", "date": "1687381942274", "content": "Hi @gojiita_ku,\nI\u2019m really sorry I somehow didn\u2019t see your question when you asked it. Are you still having the same problem? Do you still need our help?", "link": "https://discuss.pennylane.ai//t/making-quanvolutional-neural-net-weights-trainable/509/33"}}