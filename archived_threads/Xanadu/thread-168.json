{"0": {"author": "ToMago", "date": "1661422243363", "content": "Hi everyone,\nI have a question regarding lightning.gpu when building hybrid models.\nI\u2019m using keras to build a Sequential model with a quantum layer, e.g. something like:\nqlayer = qml.qnnKerasLayer(circuit, weight_shapes, output_dim=16)\n\nclayer = tf.keras.Conv2D(10, 3, strides=2, padding='valid', activation='relu')\nflatten = tf.keras.Flatten()\ndense = tf.keras.layers.Dense(81)\nreshape = tf.keras.layers.Reshape((9,9,1))\nout = tf.keras.layers.Conv2D(1, 2, strides=1, padding='same', activation='sigmoid')\n\nmodel = tf.keras.models.Sequential([clayer, flatten, qlayer, dense, reshape, out])\n\nmodel.compile(opt, loss=\"bce)\nmodel.train(x_train, x_train, epochs=5)\n\nWhere circuit is some quantum circuit.\nWhen I use adjoint differentiation for the circuit with lightning.qubit everything works fine.\nWhen I use parameter-shift with lightning.gpu it works as well.\nBut when I try to use adjoint differentiation with lightning.gpu I get the following error:\n\n\nError Trace\n\nPLException                               Traceback (most recent call last)\n/tmp/ipykernel_176419/2056065315.py in \n1 es = tf.keras.callbacks.EarlyStopping(monitor=\u2018val_loss\u2019, patience=2,min_delta=0.0001)\n----> 2 fitting = model.fit(x_train_small, x_train_small, epochs=20, batch_size=50, steps_per_epoch=50, validation_data=(x_test_small, x_test_small))\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, kwargs)\n65     except Exception as e:  # pylint: disable=broad-except\n66       filtered_tb = _process_traceback_frames(e.traceback)\n\u2014> 67       raise e.with_traceback(filtered_tb) from None\n68     finally:\n69       del filtered_tb\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in call(self, inputs)\n300             reconstructor = \n301             for x in tf.unstack(inputs):\n \u2192 302                 reconstructor.append(self.call(x))\n303             return tf.stack(reconstructor)\n304\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in call(self, inputs)\n303             return tf.stack(reconstructor)\n304\n \u2192 305         return self._evaluate_qnode(inputs)\n306\n307     def _evaluate_qnode(self, x):\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in _evaluate_qnode(self, x)\n318             {k: 1.0 * w for k, w in self.qnode_weights.items()},\n319         }\n \u2192 320         return self.qnode(**kwargs)\n321\n322     def compute_output_shape(self, input_shape):\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnode.py in call(self, *args, **kwargs)\n665             gradient_kwargs=self.gradient_kwargs,\n666             override_shots=override_shots,\n \u2192 667             **self.execute_kwargs,\n668         )\n669\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/interfaces/execution.py in execute(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n442\n443     res = _execute(\n \u2192 444         tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n=1, max_diff=max_diff, mode=_mode\n445     )\n446\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/interfaces/tensorflow.py in execute(tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n, max_diff, mode)\n87     with qml.tape.Unwrap(*tapes):\n88         # Forward pass: execute the tapes\n\u2014> 89         res, jacs = execute_fn(tapes, **gradient_kwargs)\n90\n91     for i, tape in enumerate(tapes):\n~/miniconda3/envs/tfqf/lib/python3.7/contextlib.py in inner(*args, **kwds)\n72         def inner(*args, **kwds):\n73             with self._recreate_cm():\n\u2014> 74                 return func(*args, **kwds)\n75         return inner\n76\nTom Magorsch, [25.08.22 11:58]\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/_device.py in execute_and_gradients(self, circuits, method, **kwargs)\n552             # gradient computation (if applicable).\n553             res.append(self.batch_execute([circuit])[0])\n \u2192 554             jacs.append(gradient_method(circuit, **kwargs))\n555\n556         return res, jacs\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane_lightning_gpu/lightning_gpu.py in adjoint_jacobian(self, tape, starting_state, use_device_state)\n299             tp_shift = [i - 1 for i in tp_shift]\n300\n \u2192 301         jac = adj.adjoint_jacobian(self._gpu_state, obs_serialized, ops_serialized, tp_shift)\n302         jac = np.array(jac)  # only for parameters differentiable with the adjoint method\n303         jac = jac.reshape(-1, len(tp_shift))\nPLException: Exception encountered when calling layer \u201ckeras_layer\u201d (type KerasLayer).\n[/pennylane-lightning-gpu/pennylane_lightning_gpu/src/simulator/StateVectorCudaManaged.hpp][Line:200][Method:StateVectorCudaManaged]: Error in PennyLane Lightning: custatevec not initialized\nCall arguments received:\n\u2022 inputs=tf.Tensor(shape=(50, 81), dtype=float64)\n\n\nTraining the same circuit with adjoint diff. on lightning.gpu but without embedding it in a keras hybrid model things work as well, so I suppose the problem should not lie in the circuit but in the keras integration.\nAny ideas would be greatly appreciated!\nGreetings\nTom\n\n\n Solved by mlxd in post #7 \n\n\n                Hi @ToMago \nThanks for posting the above info. I have attempted to run your earlier script using adjoint with lightning.gpu v0.25.0 and was able to run it to completion. \n [image] \nThe GPU used for this was a 40GB A100, which is the general target GPU for our cuQuantum backed workloads, along with t\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1661549760403", "content": "Hi @ToMago!\nI see that you\u2019re trying to use lightning.gpu. Please make sure that you have a compatible device and that you have the cuQuantum SDK installed. You can learn more in the docs.\nOtherwise, I\u2019m finding a few errors.\n\nWhen you call \u201cqml.qnnKerasLayer\u201d it actually requires a dot so it\u2019s qml.qnn.KerasLayer\n\nIn the \u201cmodel.compile(opt, loss=\u201cbce\u201d)\u201d line you\u2019re missing the closing quotation marks.\nYou haven\u2019t share your full code so I\u2019m assuming you\u2019re using the same code as in our qnn demo. If that\u2019s not the case then please share a minimal working example of your code.\ntf.keras.Conv2D should actually be \u201ctf.keras.layers.Conv2D\u201d\ntf.keras.Flatten should be \u201ctf.keras.layers.Flatten\u201d\n\nOther than this, it\u2019s likely that you will have a dimension or shape mismatch depending on what data you\u2019re using. Please share this data too so that I can try to reproduce your problem.\nHopefully this can help you get going.", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/2"}, "2": {"author": "ToMago", "date": "1661600963803", "content": "Hi @CatalinaAlbornoz and thanks for your answer!\nAs I mentioned, my code is running fine when using lightning.gpu with parameter-shift differentiation and also when training circuits without qml.nn.keraslayer.\nTherefore I assume, that cuQuantum is configured correctly.\nFurthermore the code is also working with lightning.qubit therefore I think the dimensions should just work fine.\nThe code for the circuit and my data is a bit lengthy but of course I can provide a minimal working example with MNIST:\nimport pennylane as qml\nimport tensorflow as tf\n\ntf.keras.backend.set_floatx('float64')\n\nDATA_QBITS = 16\nINPUT_SIZE = 12\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train/255.0, x_test/255.0\nx_train = \nx_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\nx_test = \nx_test.reshape((x_test.shape[0],x_test.shape[1],x_test.shape[2],1))\nx_train_small = tf.image.resize(x_train[:100], \n(INPUT_SIZE,INPUT_SIZE)).numpy()\nx_test_small = tf.image.resize(x_test[:100], (INPUT_SIZE,INPUT_SIZE)).numpy()\n\ndev1 = qml.device('lightning.gpu', wires=DATA_QBITS)\n\n@qml.qnode(dev1, diff_method=\"adjoint\")\ndef circuit(inputs, weights):\n\n    qml.AngleEmbedding(inputs, wires=range(DATA_QBITS))\n    qml.BasicEntanglerLayers(weights, wires=range(DATA_QBITS))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(DATA_QBITS)]\n\nweight_shapes = {\"weights\": (3,DATA_QBITS)}\nqlayer = qml.qnn.KerasLayer(circuit, weight_shapes, \noutput_dim=DATA_QBITS)\n\ninputs = tf.keras.layers.Input(shape=(12,12,1))\nclayer_1 = tf.keras.layers.Conv2D(10, 2, strides=3, padding='valid', activation=\"relu\")\nclayer_2 = tf.keras.layers.Conv2D(1, 2, strides=1, padding='same', activation=\"relu\")\ndress1 = tf.keras.layers.Flatten()\n\ndress2 = tf.keras.layers.Dense(16)\nre = tf.keras.layers.Reshape((4,4,1))\nclayer_3 = tf.keras.layers.Conv2DTranspose(10, 3, strides=3, \npadding='valid', activation=\"relu\")\nclayer_4 = tf.keras.layers.Conv2DTranspose(10, 2, strides=1, \npadding='same', activation=\"relu\")\nout_layer = tf.keras.layers.Conv2D(1, 2, strides=1, padding='same', activation=\"sigmoid\")\n\nmodel = tf.keras.models.Sequential([inputs, clayer_1, clayer_2, dress1, qlayer, dress2, re, clayer_3, clayer_4, out_layer])\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(opt, loss=\"bce\")\n\nhist = model.fit(x_train_small, x_train_small, epochs=5, batch_size=20, validation_data=(x_test_small, x_test_small))\n\nThis code throws the error Error in PennyLane Lightning: custatevec not initialized mentioned in the original post:\n\n\nError trace\n\n\nPLException                               Traceback (most recent call last)\n/tmp/ipykernel_192120/3504964397.py in \n44 model.compile(opt, loss=\u201cbce\u201d)\n45\n\u2014> 46 hist = model.fit(x_train_small, x_train_small, epochs=5, batch_size=20, validation_data=(x_test_small, x_test_small))\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, kwargs)\n65     except Exception as e:  # pylint: disable=broad-except\n66       filtered_tb = _process_traceback_frames(e.traceback)\n\u2014> 67       raise e.with_traceback(filtered_tb) from None\n68     finally:\n69       del filtered_tb\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in call(self, inputs)\n300             reconstructor = \n301             for x in tf.unstack(inputs):\n \u2192 302                 reconstructor.append(self.call(x))\n303             return tf.stack(reconstructor)\n304\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in call(self, inputs)\n303             return tf.stack(reconstructor)\n304\n \u2192 305         return self._evaluate_qnode(inputs)\n306\n307     def _evaluate_qnode(self, x):\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnn/keras.py in _evaluate_qnode(self, x)\n318             {k: 1.0 * w for k, w in self.qnode_weights.items()},\n319         }\n \u2192 320         return self.qnode(**kwargs)\n321\n322     def compute_output_shape(self, input_shape):\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/qnode.py in call(self, *args, **kwargs)\n665             gradient_kwargs=self.gradient_kwargs,\n666             override_shots=override_shots,\n \u2192 667             **self.execute_kwargs,\n668         )\n669\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/interfaces/execution.py in execute(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n442\n443     res = _execute(\n \u2192 444         tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n=1, max_diff=max_diff, mode=_mode\n445     )\n446\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/interfaces/tensorflow.py in execute(tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n, max_diff, mode)\n87     with qml.tape.Unwrap(*tapes):\n88         # Forward pass: execute the tapes\n\u2014> 89         res, jacs = execute_fn(tapes, **gradient_kwargs)\n90\n91     for i, tape in enumerate(tapes):\n~/miniconda3/envs/tfqf/lib/python3.7/contextlib.py in inner(*args, **kwds)\n72         def inner(*args, **kwds):\n73             with self._recreate_cm():\n\u2014> 74                 return func(*args, **kwds)\n75         return inner\n76\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane/_device.py in execute_and_gradients(self, circuits, method, **kwargs)\n552             # gradient computation (if applicable).\n553             res.append(self.batch_execute([circuit])[0])\n \u2192 554             jacs.append(gradient_method(circuit, **kwargs))\n555\n556         return res, jacs\n~/miniconda3/envs/tfqf/lib/python3.7/site-packages/pennylane_lightning_gpu/lightning_gpu.py in adjoint_jacobian(self, tape, starting_state, use_device_state)\n299             tp_shift = [i - 1 for i in tp_shift]\n300\n \u2192 301         jac = adj.adjoint_jacobian(self._gpu_state, obs_serialized, ops_serialized, tp_shift)\n302         jac = np.array(jac)  # only for parameters differentiable with the adjoint method\n303         jac = jac.reshape(-1, len(tp_shift))\nPLException: Exception encountered when calling layer \u201ckeras_layer\u201d (type KerasLayer).\n[/pennylane-lightning-gpu/pennylane_lightning_gpu/src/simulator/StateVectorCudaManaged.hpp][Line:200][Method:StateVectorCudaManaged]: Error in PennyLane Lightning: custatevec not initialized\nCall arguments received:\n\u2022 inputs=tf.Tensor(shape=(20, 16), dtype=float64)\n\n\nNote again, that for me this works with lightning.qubit and also with lightning.gpu with parameter-shift as diff_method.\nThanks!\nTom", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/3"}, "3": {"author": "CatalinaAlbornoz", "date": "1661817147557", "content": "Thanks for the clarification @ToMago!\nI will get back to you tomorrow with an official answer but it does look like you\u2019ve run into an unsupported case.", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/4"}, "4": {"author": "CatalinaAlbornoz", "date": "1661863072625", "content": "Hi @ToMago, can you please post the output of qml.about()? Adjoint with lightning.gpu is in fact supported in the latest version of PennyLane.", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/5"}, "5": {"author": "ToMago", "date": "1661866436562", "content": "Hi,\nglad to hear its supported!\nHere is the output of qml.about()\nName: PennyLane\nVersion: 0.25.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /home/tom/miniconda3/envs/tfqf/lib/python3.7/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, retworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning, PennyLane-Lightning-GPU\n\nPlatform info:           Linux-5.18.0-1-amd64-x86_64-with-debian-bookworm-sid\nPython version:          3.7.13\nNumpy version:           1.21.6\nScipy version:           1.7.3\nInstalled devices:\n- lightning.qubit (PennyLane-Lightning-0.25.1)\n- default.gaussian (PennyLane-0.25.1)\n- default.mixed (PennyLane-0.25.1)\n- default.qubit (PennyLane-0.25.1)\n- default.qubit.autograd (PennyLane-0.25.1)\n- default.qubit.jax (PennyLane-0.25.1)\n- default.qubit.tf (PennyLane-0.25.1)\n- default.qubit.torch (PennyLane-0.25.1)\n- default.qutrit (PennyLane-0.25.1)\n- lightning.gpu (PennyLane-Lightning-GPU-0.25.0)", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/6"}, "6": {"author": "mlxd", "date": "1661868201069", "content": "Hi @ToMago\nThanks for posting the above info. I have attempted to run your earlier script using adjoint with lightning.gpu v0.25.0 and was able to run it to completion.\n\nimage957\u00d7237 12.1 KB\n\nThe GPU used for this was a 40GB A100, which is the general target GPU for our cuQuantum backed workloads, along with the other Tesla-grade V100.\nThe error you reported is likely due to there being insufficient GPU memory available to initialize the CUDA contexts necessary for cuQuantum, and so fails to complete. Can you indicate which GPU you are running this on, as it may help us to track down the problem.\nAlso, while running the script, it may be worth examining the GPU memory use through nvidia-smi to help idnetify if this is the root cause. I prefer to poll this every second using watch -n 1 nvidia-smi to keep track over the script execution time.Solution1", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/7"}, "7": {"author": "ToMago", "date": "1661958956426", "content": "Hi @mlxd,\nyou are indeed right, the error is due to a lack of memory.\nThanks for uncovering this!\nGreetings\nTom", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/8"}, "8": {"author": "mlxd", "date": "1661868201069", "content": "Hi @ToMago\nThanks for posting the above info. I have attempted to run your earlier script using adjoint with lightning.gpu v0.25.0 and was able to run it to completion.\n\nimage957\u00d7237 12.1 KB\n\nThe GPU used for this was a 40GB A100, which is the general target GPU for our cuQuantum backed workloads, along with the other Tesla-grade V100.\nThe error you reported is likely due to there being insufficient GPU memory available to initialize the CUDA contexts necessary for cuQuantum, and so fails to complete. Can you indicate which GPU you are running this on, as it may help us to track down the problem.\nAlso, while running the script, it may be worth examining the GPU memory use through nvidia-smi to help idnetify if this is the root cause. I prefer to poll this every second using watch -n 1 nvidia-smi to keep track over the script execution time.Solution1", "link": "https://discuss.pennylane.ai//t/keras-hybrid-model-lightning-gpu-with-adjoint-differentiation/2114/9"}}