{"0": {"author": "NikSchet", "date": "1606555664251", "content": "Hello all!\nSo Variational classifier using angle encoding fails to capture patterns in simple datasets (i.e. moons) easily captured by classical Support vector machines. The main reason for this in my opinion is the data embedding (angle encoding).\nOn the other hand Data reuploading classifier captures easily patterns when variational classifier fails. And of course this is (in my opinion) due to the fact that it is a more advanced data embedding technique.\nMy question is whether it is possible to make a hybrid network using\ndata reuploading classifier instead of variational classifier.\nThanks in advance", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/1"}, "1": {"author": "NikSchet", "date": "1606557561005", "content": "Moreover, i noticed in the HYBRID you use default LINEAR activation function in the classical layers (before quantum node) instead of Tanh or Relu etc. Is there a reason for that? Also in my understanding this choice will have to do with the data embedding option. For example since angle encoding uses rotations and maps data using sine it makes sense to use a trigonometric activation function instead of RELU or default LINEAR.", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/2"}, "2": {"author": "josh", "date": "1606569228854", "content": "Hi @NikSchet!\n\nMy question is whether it is possible to make a hybrid network using\ndata reuploading classifier instead of variational classifier.\n\nIt definitely should be possible! Have you tried downloading the data re-uploading tutorial 6, and modifying it to classify simple datasets (such as moons?).\nI believe your intuition regarding data re-uploading being more expressive is correct; this tutorial on expressivity of quantum models 4 delves into this topic in more detail (and is based on the paper The effect of data encoding on the expressive power of variational quantum machine learning models by Schuld, Sweke, and Meyer 1).\n\nMoreover, i noticed in the HYBRID you use default LINEAR activation function in the classical layers (before quantum node) instead of Tanh or Relu etc. Is there a reason for that?\n\nCould you point me to the specific tutorial/example you are referring to here?", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/3"}, "3": {"author": "NikSchet", "date": "1606569910312", "content": "Thank you for your fast reply and suggestions:\n\n\nYes i have benchmarked data reuploading classifier VS variational classifier (with angle encoding) in a a variety of such 2D  and 3D artificial datasets (moons, circles, squares etc). Always the data reuploading classifier captures the pattern while variational classifier struggles in certain datasets.\n\n\nI haven\u2019t tried to implement a hybrid network with data reuploading classifier yet. So i am asking if there is a reason why it wouldnt work or maybe a hint on how to do this.\n\n\nRegarding activation functions: I am reffering to the tutorial Turning quantum nodes into Keras Layers\n\n\n\n\n\npennylane.ai\n\n\n\nTurning quantum nodes into Keras Layers \u2014 PennyLane\nLearn how to create hybrid ML models in PennyLane using Keras\n\n\n\n\n\nThe first classical layer is a dense 2 layer. When you dont specify the activation function then by default it is a linear activation function.", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/4"}, "4": {"author": "NikSchet", "date": "1606570648075", "content": "Also correct me if i am wrong \nThe data reuploading classifier is an extension to  the code in Quantum models as Fourier series . so essentially they are based on the same idea. right?", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/5"}, "5": {"author": "josh", "date": "1606572108429", "content": "\nYes i have benchmarked data reuploading classifier VS variational classifier (with angle encoding) in a a variety of such 2D and 3D artificial datasets (moons, circles, squares etc). Always the data reuploading classifier captures the pattern while variational classifier struggles in certain datasets.\n\nAwesome! Are interested in submitting your benchmarking demo to the QML website? It would make a very interesting addition!\n\nI haven\u2019t tried to implement a hybrid network with data reuploading classifier yet. So i am asking if there is a reason why it wouldnt work or maybe a hint on how to do this.\n\nI can\u2019t think of any reasons why it might not work, so I would recommend giving it a shot and seeing what happens \n\nRegarding activation functions: I am reffering to the tutorial Turning quantum nodes into Keras Layers\n\nPerhaps @Tom_Bromley can provide more details here.1", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/6"}, "6": {"author": "NikSchet", "date": "1606644594556", "content": "I want to make it a fair comparison between Variational Classifier and data-reupload classifier so i want to better understand the data embedding for VC. So looking just the template embeddings i am not sure any such embedding will work better. so probably i need to go to custom trainable embeddings1 Reply", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/7"}, "7": {"author": "josh", "date": "1607084368722", "content": "\n\n\n NikSchet:\n\nso probably i need to go to custom trainable embeddings\n\n\nNo worries @NikSchet, let us know if you have any questions as you explore this1", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/8"}, "8": {"author": "NikSchet", "date": "1607100549779", "content": "Thank you very much.\nSo in the tutorial Quantum models as Fourier series\nthe idea is similar as in the data-reuploading classifier.\nIn my hybrid model i use a qnode sandwitched between two classical layers:\n@qml.qnode(device)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n   \n   return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nSo what if i repeat the templates inside the qnode many times , wouldnt that be similar to data reuploading? for example\n @qml.qnode(device)\n    def qnode(inputs, weights):\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n       \n        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nMaybe it make sense to use different \u201cinput\u201d for every repeating layer?!?\nThanks in advance", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/9"}, "9": {"author": "Tom_Bromley", "date": "1607369744737", "content": "Hey @NikSchet,\nThis is indeed an interesting thing to think about, i.e., taking @Maria_Schuld\u2019s paper and exploring how our understanding of circuit expressivity applies to a simple ML problem like the moons dataset.\nThe code block you shared appears to be along the right lines! However, each repetition of StronglyEntanglingLayers should have a different set of weights. For example:\nimport pennylane as qml\nimport numpy as np\nimport tensorflow as tf\n\nqml.enable_tape()\n\nn_qubits = 4\n\ndev = qml.device(\"default.qubit.tf\", wires=n_qubits)\n\n@qml.qnode(dev, interface=\"tf\", grad_method=\"backprop\")\ndef qnode(inputs, weights):\n    for i in range(blocks):\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights[i], wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nlayers_per_block = 1\nblocks = 2\nweights_shape = (blocks, layers_per_block, n_qubits, 3)\n\nweights = tf.Variable(np.random.random(weights_shape))\ninputs = tf.constant(np.random.random(n_qubits))\n\nprint(\"Output of QNode:\", qnode(inputs, weights).numpy())\n\n# Optionally convert to Keras layer:\ntf.keras.backend.set_floatx(\"float64\")\n\nweight_shapes = {\"weights\": weights_shape}\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, n_qubits)\n\nbatch_size = 10\ninputs = tf.constant(np.random.random((batch_size, n_qubits)))\nprint(\"Output of quantum layer:\\n\", qlayer(inputs).numpy())\n\nUsing the terminology here 2 (see second diagram), we apply an embedding of the same input among multiple blocks. Each block also has a trainable element with independent weights.\nWe can treat AngleEmbedding and StronglyEntanglingLayers as one block, although need to be careful of the terminology between \u201clayers\u201d and \u201cblocks\u201d. In the code above, there can be multiple \u201clayers\u201d of StronglyEntanglingLayers per block, set by the layers_per_block variable. We can then vary the number of blocks (see blocks variable) and see how well the circuit can learn. Of course, in the blocks = 1 case, we recover the tutorial here 4.\nOnce set up, I see no reason why the above couldn\u2019t be combined into a hybrid with other classical layers and applied to, e.g., the moons dataset. What would be really cool is to plot the accuracy as a function of blocks and/or layers_per_block. As @josh mentioned, if you have any luck with this then it would make an awesome community demo (instructions here).\nAs a side note, when I plot the decision boundary for the KerasLayer 4 demo, it appears to be a straight line. This is probably linked to having just one block and hence limited expressivity. It would be interesting to check out the boundary if blocks is higher.\nAlso, to answer an earlier question:\n\nMoreover, i noticed in the HYBRID you use default LINEAR activation function in the classical layers (before quantum node) instead of Tanh or Relu etc. Is there a reason for that?\n\nNot really! Quite possibly we could have used another activation and it may have trained as well or better. Something like a tanh or sigmoid activation that is nicely bounded in an interval makes sense. Indeed, this is linked to the choice of activation and we could also have rescaled to [-\\pi, \\pi].", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/10"}, "10": {"author": "NikSchet", "date": "1607433751419", "content": "I would like to thank you for your detailed answer. Yes i succeeded in using your code cobined with other classical layers. some remarks\n\n\nI do not need the initial feeding classical layer to get high accuracy. Even with just the quantum node and a final classical layer i get excellent results\n\n\n\neach repetition of  StronglyEntanglingLayers  should have a  different  set of weights.\n-> Interestingly, even without different set of weights (see my previous code) i get very good results.\n\n\n\nincreasing blocks significantly improves the prediction grid (Using 3 blocks  ,2 layers is sufficient to get amazing results in just 10 epochs)\n\n\nI think adding the qml.enable_tape() line greatly improves the running time\n\n\nI am in the process of making such a demo thanks!\n\n\n", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/11"}, "11": {"author": "Tom_Bromley", "date": "1607434918215", "content": "Thanks @NikSchet, sounds great and some interesting observations. Let us know if there\u2019s anything else we can help with!2 Replies1", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/12"}, "12": {"author": "NikSchet", "date": "1607435074741", "content": "Feel free to check the code here. https://github.com/nsansen/Quantum-Machine-Learning 4\n\nDemo v5. Just plot prediction grid for a given setup\nDemo v5 - feature exploring , prints prediction grids for different blocks , so you can see the evolution of the grid\n\nFor database construction code i use code  written by Adri\u00e1n P\u00e9rez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and Jos\u00e9 I. Latorre. I think it make more sense to have these databases as reference1", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/13"}, "13": {"author": "NikSchet", "date": "1607501936290", "content": "So i am not sure if the code in github makes a useful demo or not, a presentation with results might be more meaniningful.\nIn any case thank you very much for your help1 Reply", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/14"}, "14": {"author": "josh", "date": "1607504388849", "content": "\n\n\n NikSchet:\n\nSo i am not sure if the code in github makes a useful demo or not, a presentation with results might be more meaniningful.\n\n\nHi @NikSchet - it\u2019s up to you how you would like to structure a demo submission, but demos we\u2019ve found quite useful in the past are ones that tell a story.\nThat is, they start off with a question that is going to be explored, space out the code cells with text explanations, and finish by presenting some results. Even if the results are negative, the demo as a whole could still tell an interesting story!1", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/15"}, "15": {"author": "Tom_Bromley", "date": "1607369744737", "content": "Hey @NikSchet,\nThis is indeed an interesting thing to think about, i.e., taking @Maria_Schuld\u2019s paper and exploring how our understanding of circuit expressivity applies to a simple ML problem like the moons dataset.\nThe code block you shared appears to be along the right lines! However, each repetition of StronglyEntanglingLayers should have a different set of weights. For example:\nimport pennylane as qml\nimport numpy as np\nimport tensorflow as tf\n\nqml.enable_tape()\n\nn_qubits = 4\n\ndev = qml.device(\"default.qubit.tf\", wires=n_qubits)\n\n@qml.qnode(dev, interface=\"tf\", grad_method=\"backprop\")\ndef qnode(inputs, weights):\n    for i in range(blocks):\n        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n        qml.templates.StronglyEntanglingLayers(weights[i], wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nlayers_per_block = 1\nblocks = 2\nweights_shape = (blocks, layers_per_block, n_qubits, 3)\n\nweights = tf.Variable(np.random.random(weights_shape))\ninputs = tf.constant(np.random.random(n_qubits))\n\nprint(\"Output of QNode:\", qnode(inputs, weights).numpy())\n\n# Optionally convert to Keras layer:\ntf.keras.backend.set_floatx(\"float64\")\n\nweight_shapes = {\"weights\": weights_shape}\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, n_qubits)\n\nbatch_size = 10\ninputs = tf.constant(np.random.random((batch_size, n_qubits)))\nprint(\"Output of quantum layer:\\n\", qlayer(inputs).numpy())\n\nUsing the terminology here 2 (see second diagram), we apply an embedding of the same input among multiple blocks. Each block also has a trainable element with independent weights.\nWe can treat AngleEmbedding and StronglyEntanglingLayers as one block, although need to be careful of the terminology between \u201clayers\u201d and \u201cblocks\u201d. In the code above, there can be multiple \u201clayers\u201d of StronglyEntanglingLayers per block, set by the layers_per_block variable. We can then vary the number of blocks (see blocks variable) and see how well the circuit can learn. Of course, in the blocks = 1 case, we recover the tutorial here 4.\nOnce set up, I see no reason why the above couldn\u2019t be combined into a hybrid with other classical layers and applied to, e.g., the moons dataset. What would be really cool is to plot the accuracy as a function of blocks and/or layers_per_block. As @josh mentioned, if you have any luck with this then it would make an awesome community demo (instructions here).\nAs a side note, when I plot the decision boundary for the KerasLayer 4 demo, it appears to be a straight line. This is probably linked to having just one block and hence limited expressivity. It would be interesting to check out the boundary if blocks is higher.\nAlso, to answer an earlier question:\n\nMoreover, i noticed in the HYBRID you use default LINEAR activation function in the classical layers (before quantum node) instead of Tanh or Relu etc. Is there a reason for that?\n\nNot really! Quite possibly we could have used another activation and it may have trained as well or better. Something like a tanh or sigmoid activation that is nicely bounded in an interval makes sense. Indeed, this is linked to the choice of activation and we could also have rescaled to [-\\pi, \\pi][\u2212\u03c0,\u03c0].", "link": "https://discuss.pennylane.ai//t/data-re-uploading-classifier-in-hybrid/699/16"}}