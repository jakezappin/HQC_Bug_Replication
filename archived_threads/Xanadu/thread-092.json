{"0": {"author": "_risto", "date": "1624223180188", "content": "Hi.\nWhen I try to run https://pennylane.ai/qml/demos/tutorial_quantum_transfer_learning.html 4 I get RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat2 in method wrapper_mm)\nThis has not happened on previous computer, which had no GPU and no CUDA.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/1"}, "1": {"author": "nathan", "date": "1624300563700", "content": "Hi @_risto,\nThis is an interesting one we haven\u2019t seen before. Would you be able to a minimal code example that generated this error message for you, as well as the output of qml.about()? That will better help us diagnose. Thanks!", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/2"}, "2": {"author": "_risto", "date": "1624306478665", "content": "When running this line: model_hybrid = train_model(\nmodel_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n)\nI get:\nTraining started:\nC:\\Users\\risto\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  \u2026\\c10/core/TensorImpl.h:1156.)\nreturn torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\nRuntimeError                              Traceback (most recent call last)\n in \n----> 1 model_hybrid = train_model(\n2     model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n3 )\n in train_model(model, criterion, optimizer, scheduler, num_epochs)\n37                     loss = criterion(outputs, labels)\n38                     if phase == \u201ctrain\u201d:\n\u2014> 39                         loss.backward()\n40                         optimizer.step()\n41\n~\\anaconda3\\lib\\site-packages\\torch_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n253                 create_graph=create_graph,\n254                 inputs=inputs)\n\u2013> 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n256\n257     def register_hook(self, hook):\n~\\anaconda3\\lib\\site-packages\\torch\\autograd_init_.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n145         retain_graph = create_graph\n146\n\u2013> 147     Variable.execution_engine.run_backward(\n148         tensors, grad_tensors, retain_graph, create_graph, inputs,\n149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n~\\anaconda3\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\n85     def apply(self, *args):\n86         # _forward_cls is defined by derived class\n\u2014> 87         return self._forward_cls.backward(self, *args)  # type: ignore[attr-defined]\n88\n89\n~\\anaconda3\\lib\\site-packages\\pennylane\\interfaces\\torch.py in backward(ctx, dy)\n173         \u201c\u201d\u201cImplements the backwards pass QNode vector-Jacobian product\u201d\"\"\n174         ctx.dy = dy\n\u2013> 175         vjp = dy.view(1, -1) @ ctx.jacobian.apply(ctx, *ctx.saved_tensors)\n176         vjp = torch.unbind(vjp.view(-1))\n177         return (None,) + tuple(vjp)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat2 in method wrapper_mm)", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/3"}, "3": {"author": "_risto", "date": "1624331267616", "content": "I have also tried to run some random network from github and I notice the error occuring, while running the following:\ndevice_name = \u201ccuda:0:\u201d if torch.cuda.is_available() else \u201ccpu\u201d\ndevice = torch.device(device_name)\nRuntimeError: Invalid device string: \u2018cuda:0:\u2019", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/4"}, "4": {"author": "christina", "date": "1624369485205", "content": "If your priority is just getting the code working, you could try\ndevice_name = \"cpu\"\n\nThis won\u2019t have a potential GPU speedup, but it should at least work.\nLet me know if that helps ", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/5"}, "5": {"author": "_risto", "date": "1624370263763", "content": "Hi @christina\nYes, I get the same error: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat2 in method wrapper_mm).\nI also get same error when running: device = torch.device(\u201ccuda:0\u201d)\nIs it because I use RTX 3060 and those are not compatible with pennylane?\nI would like to scale the code for larger data an more complex resNet, thus would like to use it with GPU, if it would be possible.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/6"}, "6": {"author": "christina", "date": "1624377994482", "content": "Looks like there\u2019s a bug in computing the jacobian with a GPU. Thanks so much for bringing this problem up. \nA bugfix is currently in the works, so stay tuned.1", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/7"}, "7": {"author": "_risto", "date": "1624378886790", "content": "Happy to be of any help. This company & group is doing really amazing things.2", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/8"}, "8": {"author": "_risto", "date": "1625021610274", "content": "Hi @christina\nHow is the bugfix going?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/9"}, "9": {"author": "glassnotes", "date": "1625054813247", "content": "Hi @_risto,\nThe bug fix has now been merged into the master branch of PennyLane - try installing directly from there, and let us know how it goes. Just a heads up, depending on your setup, you might need to use the most recent stable version of torch, which is 1.9 (those of us on the team who tested the fix both have a GTX 1060, and had some card-related issues with 1.8.x that are resolved with 1.9).", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/10"}, "10": {"author": "_risto", "date": "1625064990993", "content": "\n\n\n glassnotes:\n\ntry installing directly from there\n\n\nWhat do you mean by that?\nI was using 1.9 stable version of torch all along while I got the error.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/11"}, "11": {"author": "theodor", "date": "1625072408562", "content": "Hi @_risto,\nThe bug was in PennyLane, but has now been merged into the master branch of PennyLane (so you\u2019d need to install it from there, which can be done by executing pip install git+https://github.com/PennyLaneAI/pennylane.git  or by downloading/cloning the repository and installing it directly from the downloaded folder using pip install .).\nIt\u2019s recommended that you use torch v1.9 as well, since that version also resolved some issues that some members of the team had. ", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/12"}, "12": {"author": "_risto", "date": "1625094267899", "content": "Hi @theodor\nDid the installations of Pennylane. I have been using torch 1.9 all along. Still getting the error.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/13"}, "13": {"author": "glassnotes", "date": "1625146555223", "content": "Hi @_risto,\nCould you please post your error output? Is it exactly the same as before, or is the traceback pointing to a different part of the code now?\nAlso, could please post the output of running qml.about()?\nThanks!", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/14"}, "14": {"author": "_risto", "date": "1625150024975", "content": "Hi @glassnotes\nSo, first Powershell tells me pennylane 17 is installed\n\nimage756\u00d7227 20.5 KB\n\nBut then when I run the demo I get that I am using pennylane 16\n\nimage948\u00d7702 52.7 KB\n\nAnd then this error\n\nimage736\u00d7720 244 KB\n\nI don\u2019t understand why it says that pennylane 17 is installed, but then it shows 16 while running the code. \nWhen I try to install it via conda install git+https://github.com/PennyLaneAI/pennylane.git I get:\n\nimage1084\u00d7594 28.7 KB\n", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/15"}, "15": {"author": "_risto", "date": "1625186835291", "content": "Hi @glassnotes\nI have solved the issue. I have uninstalled anaconda all together and have installed and run jupyter notebook from pip. But I still get:\n\nimage1276\u00d7247 12.5 KB\n\nWhat does that mean?\nIn addition, are the following parameters from the demo set to the optimal training performance?\nn_qubits = 4                # Number of qubits\nstep = 0.0004               # Learning rate\nbatch_size = 4              # Number of samples for each training step\nnum_epochs = 30              # Number of training epochs\nq_depth = 6                 # Depth of the quantum circuit (number of variational layers)\ngamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\nq_delta = 0.01              # Initial spread of random quantum weights\nstart_time = time.time()    # Start of the computation timer", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/16"}, "16": {"author": "antalszava", "date": "1625264521858", "content": "Hi @_risto,\nThe warning seems to be something that Torch raises internally when using functions like max_pool1d. The Torch team has been made aware of this happening and they have fixed it in their master version. See their message here from a week ago. According to the Torch team, it will be in the next release. It should be safe to be ignored until then.\nAs for the hyper-parameters, these were reported in the original paper to provide high enough accuracy for transfer learning. Would you be interested in optimal training performance in terms of accuracy or execution time?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/17"}, "17": {"author": "_risto", "date": "1625281932400", "content": "Hi @antalszava\nThank you for your answer.\nYes, I would be interested in setup for optimal performance. I am playing and changing parameters, but I don\u2019t know how practical that would be.\nI am also trying to run some other models from torch (f.e.: resnext101_32x8d).\nBut then it says Training started, but nothing is happening. However, my GPU usage is at 100%.\n\n\nimage1101\u00d7130 7.5 KB\n\nWhat does that mean?\nAlso, I have managed to run the algorithm on resnet152, both on a quantum and a classical one, with same parameters (batch size, epochs, etc.), on the same data and the classical one has better test/val acc. as well as better speed (the quantum model barely uses my GPU). So what would be an advantage of using the hybrid quantum model in compare to the classical one?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/18"}, "18": {"author": "antalszava", "date": "1625523356683", "content": "Hi @_risto,\nThe parameters in the paper present a scenario where the training seemed to work well, but there may be other configurations that also work nicely or even better. It\u2019s worth exploring, but there aren\u2019t many trivial approaches as hyperparameter optimization is an established problem in itself. There are some techniques that can be worth explored (e.g., doing a grid search of the parameters).\nWas that something that came up specifically for resnext101_32x8d? Naively, it would be an indication that the computation is still ongoing. Did that not occur when using only CPUs?\n\nSo what would be an advantage of using the hybrid quantum model in compare to the classical one?\n\nThe advantage of using quantum machine learning algorithms compared to similar classical machine learning algorithms is an open research question. The paper on transfer learning presents proof of concept ideas and verifies that they produce accurate results on quantum hardware too.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/19"}, "19": {"author": "_risto", "date": "1625533951955", "content": "Hi @antalszava\nI have run resnet152 both as classical and modified to this quantum demo with same parameters and same data. The result: classical has better validation / test accuracy and much faster (it took classical 2 min, whereas quantum one 20 min). Too bad, was hoping for an advantage of the quantum one.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/20"}, "20": {"author": "_risto", "date": "1625281932400", "content": "Hi @antalszava\nThank you for your answer.\nYes, I would be interested in setup for optimal performance. I am playing and changing parameters, but I don\u2019t know how practical that would be.\nI am also trying to run some other models from torch (f.e.: resnext101_32x8d).\nBut then it says Training started, but nothing is happening. However, my GPU usage is at 100%.\n\n\nimage1101\u00d7130 7.5 KB\n\nWhat does that mean?\nAlso, I have managed to run the algorithm on resnet152, both on a quantum and a classical one, with same parameters (batch size, epochs, etc.), on the same data and the classical one has better test/val acc. as well as better speed (the quantum model barely uses my GPU). So what would be an advantage of using the hybrid quantum model in compare to the classical one?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/21"}, "21": {"author": "josh", "date": "1625564626751", "content": "@_risto interesting results!\n\nToo bad, was hoping for an advantage of the quantum one.\n\nYes, this is often the case when researching and playing around with QML models ", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/22"}, "22": {"author": "_risto", "date": "1625755705256", "content": "Hi @josh\nI have also run the model on resnext101, got the same results - classical performs better in regard to validation / test accuracy and time of training.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/23"}, "23": {"author": "Tom_Bromley", "date": "1625777567471", "content": "Hey @_risto, as @josh mentioned this may often be the case for such prototypical models. It\u2019d be interesting to scale up the width (number of qubits) and depth of the quantum element and see how things compare, but this unfortunately becomes a challenge for simulators.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/24"}, "24": {"author": "_risto", "date": "1625783256293", "content": "Hi @Tom_Bromley\nYes. I agree. I tried scaling up, but the simulator just can\u2019t take it. The maximum I could punch is 16 qbits and this was very slow. And also with actual q-computers things might be different. Time will tell.\nBtw. is Xanadu team planing to do some demos on the topic of quantum reservoir computing - QRC (https://www.nature.com/articles/s41534-019-0149-8/)? I believe the future of quantum computing is to manipulate quantum data in quantum algorithms, rather than just using quantum algorithms on classical data. QRC seems promising in that area, although the coding skills are far beyond my humble knowledge.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/25"}, "25": {"author": "Tom_Bromley", "date": "1625833404309", "content": "\nBtw. is Xanadu team planing to do some demos on the topic of quantum reservoir computing\n\nThanks for sharing, I\u2019ll check it out. I\u2019m not aware of any demos on this topic in the pipeline, though if you end up prototyping a solution yourself then please consider submitting it as a community demo 1 - it\u2019d be a great way to get people interested.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/26"}, "26": {"author": "_risto", "date": "1626122340623", "content": "Hi @Tom_Bromley\nCan I ask, would there be a way to send someone my two examples of resnet152 (classical & quantum version) just to check if all the parameters are actually the same (except the last layer)? Perhaps I have made a mistake and am doing injustice to the developers and the whole Xanadu team?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/27"}, "27": {"author": "Tom_Bromley", "date": "1626189551311", "content": "Hey @_risto!\nOne option would be to turn your examples into nicely-presented notebooks and submit them as community demos 1 (instructions here), if you think the content would be sufficiently different to the transfer learning demo. In my opinion, it doesn\u2019t matter too much if the end result shows a fully classical network performing better, just making the comparison and perhaps a discussion on why they are different is still of interest.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/28"}, "28": {"author": "_risto", "date": "1626882305459", "content": "Hi @Tom_Bromley\nI have noticed, that when using quantum simulator, almost all of the work is done on CPU and not GPU, which kinda slows the process of training / validation. When running a classical network, GPU is used exclusively. Is there any explanation for this? I am playing with changing different parameters to see the results, but due to CPU usage it takes much longer.\nFor example, if I run it on 32 qbits, my CPU usage shows up to 50% and GPU 0%. I don\u2019t understand, why the simulator doesn\u2019t use the GPU.\nWhen I try 16 gbits, it starts, but then freezes at \u201cPhase: train Epoch: 1/30 Iter: 8/62 Batch time: 143.9889\u201d and CPU shows 0% and GPU 0%.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/29"}, "29": {"author": "Tom_Bromley", "date": "1626899431121", "content": "Hey @_risto!\n\nI have noticed, that when using quantum simulator, almost all of the work is done on CPU and not GPU, which kinda slows the process of training / validation. When running a classical network, GPU is used exclusively. Is there any explanation for this?\n\nWhen using the torch interface or TorchLayer, the backend calculations for the quantum circuit are all performed using NumPy and then converted into Torch tensors, along with the gradient. The reason this is done is because Torch only recently introduced support for complex numbers.\nWe have a WIP update 1 that will allow the full calculation pipeline to remain within PyTorch. Doing so would allow for Torch tensors to live on the GPU and potentially utilize it more.\nOn the other hand, even when the full pipeline can remain on GPU, it is not guaranteed that the device will use the GPU as efficiently as possible. Using the full potential of the GPU is something that we\u2019re thinking about for lightning.qubit, but I can\u2019t give a firm timeline on when that would become available.\n\nif I run it on 32 qbits\n\nI\u2019m impressed that you are able to push a quantum circuit to 32 qubits! This will be quite challenging and you may want to consider lowering the number of qubits while prototyping the model.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/30"}, "30": {"author": "_risto", "date": "1626903624149", "content": "Thank you @Tom_Bromley\nI changed the pagefile size to maximum possible, after that I did not get error after running 32 qbits, however it seems the system freezes.\nJust today I was lookng at Pytorch Lightning, is this what you mean by lightning.qubit?\nAlso I have noticed in classical Resnet the training accuracy is slightly bigger than validation, but in quantum it is other way around. Does that imply that by it nature the quantum model is better in \u201cguessing\u201d?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/31"}, "31": {"author": "Tom_Bromley", "date": "1626969541872", "content": "Hey @_risto!\n\nJust today I was lookng at Pytorch Lightning, is this what you mean by lightning.qubit ?\n\nI am referring to our PennyLane-Lightning plugin, which is a C++ based device intended for high performance. It doesn\u2019t currently support GPUs, but that is on the agenda.\n\nAlso I have noticed in classical Resnet the training accuracy is slightly bigger than validation, but in quantum it is other way around. Does that imply that by it nature the quantum model is better in \u201cguessing\u201d?\n\nPerhaps for that specific model it could be said that the quantum element is helping to avoid overfitting, though I\u2019m not aware that this is a phenomenon that holds in general.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/32"}, "32": {"author": "_risto", "date": "1626969976819", "content": "Hi @Tom_Bromley\n\n\n\n Tom_Bromley:\n\nI am referring to our PennyLane-Lightning plugin\n\n\nWhen I try to run it I get DeviceError: Device does not exist. Make sure the required plugin is installed. How do i install that?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/33"}, "33": {"author": "Tom_Bromley", "date": "1626970170336", "content": "Hey @_risto!\nHere 3 are the installation instructions - once installed make sure to restart your kernel.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/34"}, "34": {"author": "_risto", "date": "1626971680663", "content": "I tried, but get ERROR: Failed building wheel for pennylane-lightning and then a bunch of red lines and no installed package.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/35"}, "35": {"author": "Tom_Bromley", "date": "1626974039202", "content": "@_risto, apologies that it\u2019s not working for you! Could you share which system you\u2019re running on? One reliable way to share this info is to copy the output of:\nimport pennylane as qml\nqml.about()\n", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/36"}, "36": {"author": "_risto", "date": "1626975130515", "content": "@Tom_Bromley\nHere it is:\n\nimage791\u00d7743 37.5 KB\n", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/37"}, "37": {"author": "Tom_Bromley", "date": "1626979742622", "content": "Hey @_risto! We do not yet have pre-built binaries for Python 3.9 available through pip install. Would you be able to create a new environment with Python 3.8? If you use Conda, you can follow the instructions here 2.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/38"}, "38": {"author": "_risto", "date": "1627307213691", "content": "Hi @Tom_Bromley\nI get the error, when I want to install the package\n\nimage1492\u00d7714 29 KB\n\nIn addition, is it possible to run the demo on pytorch-lightning?", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/39"}, "39": {"author": "Tom_Bromley", "date": "1627332098875", "content": "Hey @_risto! Did you try to change your version of Python to 3.8? Unfortunately we do not yet have support for 3.9 and it looks like that is the version you are using.\n\nIn addition, is it possible to run the demo on pytorch-lightning?\n\nIt may well be possible to interface PennyLane with PyTorch-Lightning. Indeed, we have an open issue on GitHub discussing how we can provide a nice example, with transfer learning a likely candidate use case. However, we do not have a walkthrough just yet, so I\u2019d be interested to see how it goes if you do try.\nAlso wanted emphasize (e.g., to other readers of this post) that lightning.qubit is designed to be a fast backend for PennyLane and isn\u2019t intended specifically as a complementary feature to PyTorch-Lightning.", "link": "https://discuss.pennylane.ai//t/transfer-learning-error/1134/40"}}