{"0": {"author": "akatief", "date": "1688647259251", "content": "Hello! As suggested in another thread, I\u2019m asking for advice on a more complicated use case. I\u2019d like to build a TorchLayer that performs two separate amplitude embeddings on different parts of its input. These inputs are 2D tensors of size (batch_size, embedding_dim).\nTo make it work, I\u2019m using merge_amplitude_embedding. The issue lies in the fact that merge_amplitude_embedding performs a Kronecker product under the hood, also expanding the batch dimension instead of just the embedding one, resulting in an intermediate tensor of size (batch_size^2, embedding_dim^2).\nIs there some workaround to prevent this behavior?\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=4)\n\n@qml.qnode(dev, interface=\"torch\")\n@qml.transforms.merge_amplitude_embedding\ndef qnode(inputs, weight):\n    inputs1 = inputs[:,:4]\n    inputs2 = inputs[:,4:]\n    qml.AmplitudeEmbedding(inputs1, wires=[0, 1], normalize=True, pad_with=0.0)\n    qml.AmplitudeEmbedding(inputs2, wires=[2, 3], normalize=True, pad_with=0.0)\n    return qml.state()\n\nweight_shapes = {'weight' : 1}\nx = torch.ones(9*2*4).reshape((9,2, 4))\nx = torch.flatten(x, start_dim=1)\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\noutput = qlayer(x) # Will crash with RuntimeError: shape '[9, 16]' is invalid for input of size 1296\nprint(output.shape)\nprint(output)\n\nMight also be useful to look at this line:\n\ngithub.com\n\n\nPennyLaneAI/pennylane/blob/49d325f21fe6cdf20d8a53e64b29c682b430a707/pennylane/transforms/optimization/merge_amplitude_embedding.py#L93\n\n      \n              input_vectors.append(current_gate.parameters[0])\n              list_copy.pop(0)\n              visited_wires = visited_wires.union(wires_set)\n          \n          if len(input_wires) > 0:\n              final_wires = input_wires[0]\n              final_vector = input_vectors[0]\n          \n              # Merge all parameters and qubits into a single one.\n              for w, v in zip(input_wires[1:], input_vectors[1:]):\n                  final_vector = kron(final_vector, v)\n                  final_wires = final_wires + w\n          \n              AmplitudeEmbedding(final_vector, wires=final_wires)\n          \n          for gate in not_amplitude_embedding:\n              apply(gate)\n          \n          # Queue the measurements normally\n          for m in tape.measurements:\n              apply(m)\n      \n    \n\n\n\n\n\nRunning on:\ntorch==2.0.1\ntorchsummary==1.5.1\ntorchvision==0.15.2\nPennyLane==0.31.0\nPennyLane-Lightning==0.31.0\nPennyLane-qiskit==0.31.0\n\nThanks for your help!", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/1"}, "1": {"author": "isaacdevlugt", "date": "1688654660123", "content": "Hey @akatief!\nThis looks like a good candidate for a feature request issue 2. Maybe einsum might be better here so that we can be more careful? Anyway \u2014 let\u2019s see what our dev team says . Great catch!", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/2"}, "2": {"author": "akatief", "date": "1688717048091", "content": "Thanks! Opening the request right away \n\n\ngithub.com/PennyLaneAI/pennylane\n\n\n\n\n\n\n\n\nMultiple batched amplitude embedding\n\n\n\n        opened \n\n\n\nJul 7, 2023\n\n\n\n\n\n          akatief\n        \n\n\n\n\n          enhancement \n\n\n\n\n\n### Feature details\n\nMake merge_amplitude_embedding work with batches.\n\nWhen f\u2026eeding 2D tensors to merge_amplitude_embedding, it performs a Kronecker product under the hood, also expanding the batch dimension instead of just the embedding one, resulting in an intermediate tensor of size (batch_size^2, embedding_dim^2). This makes any circuit relying on separate amplitude embeddings fail in a batch context.\n\n### Implementation\n\nThe merge_amplitude_embedding function could be changed to call einsum instead of kron.\n\nhttps://github.com/PennyLaneAI/pennylane/blob/34d2fb218f38d88d01f4b742a915f3149cd0c6ac/pennylane/transforms/optimization/merge_amplitude_embedding.py#L93\n\nThis is a snippet implementing a similar behavior in PyTorch.\n`# a and b are the two 2D tensors you want to encode separately\ntorch.einsum('nk,nl->nkl',a,b).reshape(a.shape[0],-1)`\n\n### How important would you say this feature is?\n\n2: Somewhat important. Needed this quarter.\n\n### Additional information\n\n_No response_\n\n\n\n\n\n\nIn the meantime I found a workaround that consists in building the tensor product yourself and feeding it directly to a circuit with a single AmplitudeEmbedding. When performing the tensor product you need to be careful with padding, I think this is the correct way to do it:\ndef pad_to_next_power_of_two(tensor):\n    curr_size = tensor.size(-1)\n    next_power_of_two = 2 ** (curr_size - 1).bit_length()\n    padding_size = next_power_of_two - curr_size\n\n    padded_tensor = torch.nn.functional.pad(tensor, (0, padding_size))\n    return padded_tensor\n\n# a and b are the two 2D tensors you want to encode separately\na = pad_to_next_power_of_two(a)\nb = pad_to_next_power_of_two(b)\n# Performs tensor product along only one axis\nc = torch.einsum('nk,nl->nkl',a,b).reshape(a.shape[0],-1)\ncircuit(c) # Contains a single amplitude encoding\n\nHope this is useful to someone!", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/3"}, "3": {"author": "isaacdevlugt", "date": "1688736838192", "content": "Interesting! Do you mind linking the feature request that you made to the PL github? ", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/4"}, "4": {"author": "akatief", "date": "1688647259251", "content": "Hello! As suggested in another thread, I\u2019m asking for advice on a more complicated use case. I\u2019d like to build a TorchLayer that performs two separate amplitude embeddings on different parts of its input. These inputs are 2D tensors of size (batch_size, embedding_dim).\nTo make it work, I\u2019m using merge_amplitude_embedding. The issue lies in the fact that merge_amplitude_embedding performs a Kronecker product under the hood, also expanding the batch dimension instead of just the embedding one, resulting in an intermediate tensor of size (batch_size^2, embedding_dim^2).\nIs there some workaround to prevent this behavior?\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=4)\n\n@qml.qnode(dev, interface=\"torch\")\n@qml.transforms.merge_amplitude_embedding\ndef qnode(inputs, weight):\n    inputs1 = inputs[:,:4]\n    inputs2 = inputs[:,4:]\n    qml.AmplitudeEmbedding(inputs1, wires=[0, 1], normalize=True, pad_with=0.0)\n    qml.AmplitudeEmbedding(inputs2, wires=[2, 3], normalize=True, pad_with=0.0)\n    return qml.state()\n\nweight_shapes = {'weight' : 1}\nx = torch.ones(9*2*4).reshape((9,2, 4))\nx = torch.flatten(x, start_dim=1)\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\noutput = qlayer(x) # Will crash with RuntimeError: shape '[9, 16]' is invalid for input of size 1296\nprint(output.shape)\nprint(output)\n\nMight also be useful to look at this line:\n\ngithub.com\n\n\nPennyLaneAI/pennylane/blob/49d325f21fe6cdf20d8a53e64b29c682b430a707/pennylane/transforms/optimization/merge_amplitude_embedding.py#L93\n\n      \n              input_vectors.append(current_gate.parameters[0])\n              list_copy.pop(0)\n              visited_wires = visited_wires.union(wires_set)\n          \n          if len(input_wires) > 0:\n              final_wires = input_wires[0]\n              final_vector = input_vectors[0]\n          \n              # Merge all parameters and qubits into a single one.\n              for w, v in zip(input_wires[1:], input_vectors[1:]):\n                  final_vector = kron(final_vector, v)\n                  final_wires = final_wires + w\n          \n              AmplitudeEmbedding(final_vector, wires=final_wires)\n          \n          for gate in not_amplitude_embedding:\n              apply(gate)\n          \n          # Queue the measurements normally\n          for m in tape.measurements:\n              apply(m)\n      \n    \n\n\n\n\n\nRunning on:\ntorch==2.0.1\ntorchsummary==1.5.1\ntorchvision==0.15.2\nPennyLane==0.31.0\nPennyLane-Lightning==0.31.0\nPennyLane-qiskit==0.31.0\n\nThanks for your help!", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/5"}, "5": {"author": "akatief", "date": "1688717048091", "content": "Thanks! Opening the request right away \n\n\ngithub.com/PennyLaneAI/pennylane\n\n\n\n\n\n\n\n\nMultiple batched amplitude embedding\n\n\n\n        opened \n\n\n\nJul 7, 2023\n\n\n\n\n\n          akatief\n        \n\n\n\n\n          enhancement \n\n\n\n\n\n### Feature details\n\nMake merge_amplitude_embedding work with batches.\n\nWhen f\u2026eeding 2D tensors to merge_amplitude_embedding, it performs a Kronecker product under the hood, also expanding the batch dimension instead of just the embedding one, resulting in an intermediate tensor of size (batch_size^2, embedding_dim^2). This makes any circuit relying on separate amplitude embeddings fail in a batch context.\n\n### Implementation\n\nThe merge_amplitude_embedding function could be changed to call einsum instead of kron.\n\nhttps://github.com/PennyLaneAI/pennylane/blob/34d2fb218f38d88d01f4b742a915f3149cd0c6ac/pennylane/transforms/optimization/merge_amplitude_embedding.py#L93\n\nThis is a snippet implementing a similar behavior in PyTorch.\n`# a and b are the two 2D tensors you want to encode separately\ntorch.einsum('nk,nl->nkl',a,b).reshape(a.shape[0],-1)`\n\n### How important would you say this feature is?\n\n2: Somewhat important. Needed this quarter.\n\n### Additional information\n\n_No response_\n\n\n\n\n\n\nIn the meantime I found a workaround that consists in building the tensor product yourself and feeding it directly to a circuit with a single AmplitudeEmbedding. When performing the tensor product you need to be careful with padding, I think this is the correct way to do it:\ndef pad_to_next_power_of_two(tensor):\n    curr_size = tensor.size(-1)\n    next_power_of_two = 2 ** (curr_size - 1).bit_length()\n    padding_size = next_power_of_two - curr_size\n\n    padded_tensor = torch.nn.functional.pad(tensor, (0, padding_size))\n    return padded_tensor\n\n# a and b are the two 2D tensors you want to encode separately\na = pad_to_next_power_of_two(a)\nb = pad_to_next_power_of_two(b)\n# Performs tensor product along only one axis\nc = torch.einsum('nk,nl->nkl',a,b).reshape(a.shape[0],-1)\ncircuit(c) # Contains a single amplitude encoding\n\nHope this is useful to someone!", "link": "https://discuss.pennylane.ai//t/multiple-batched-amplitude-embedding/3153/6"}}