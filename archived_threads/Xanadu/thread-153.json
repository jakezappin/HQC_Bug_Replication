{"0": {"author": "trifire", "date": "1663189452292", "content": "I am trying to use the code from pennylane\u2019s QNSPSA tutorial linked here 3 to run a variational quantum eigensolver.\nHowever, if I use a template for my ansatz such as qml.templates.AllSinglesDoubles\nI get an error: AdjointUndefinedError.\nI have stepped through my code, and I have found that the issue is that BasisState does not have a defined adjoint.\nIs there a good way to fix this?\nFor reference, this is my full code:\nfrom types import FunctionType\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport math\n\nimport random\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom scipy.linalg import sqrtm\nimport warnings\nfrom functools import partial\n\nclass QNSPSA:\n\"\"\"Quantum natural SPSA optimizer. Refer to https://quantum-journal.org/papers/q-2021-10-20-567/\nfor a detailed description of the methodology. When disable_metric_tensor\nis set to be True, the metric tensor estimation is disabled, and QNSPSA is\nreduced to be a SPSA optimizer.\nFrom: https://github.com/PennyLaneAI/qml/blob/master/demonstrations/qnspsa.py\nArgs:\n    stepsize (float): The learn rate.\n    regularization (float): Regularitzation term to the Fubini-Study\n        metric tensor for numerical stability.\n    finite_diff_step (float): step size to compute the finite difference\n        gradient and the Fubini-Study metric tensor.\n    resamplings (int): The number of samples to average for each parameter\n        update.\n    blocking (boolean): When set to be True, the optimizer only accepts\n        updates that lead to a loss value no larger than the loss value\n        before update, plus a tolerance. The tolerance is set with the\n        parameter history_length.\n    history_length (int): When blocking is True, the tolerance is set to be\n        the average of the cost values in the last history_length steps.\n    disable_metric_tensor (boolean): When set to be True, the optimizer is\n        reduced to be a (1st-order) SPSA optimizer.\n    seed (int): Seed for the random sampling.\n\"\"\"\n\ndef __init__(\n    self,\n    stepsize=1e-3,\n    regularization=1e-3,\n    finite_diff_step=1e-2,\n    resamplings=1,\n    blocking=True,\n    history_length=5,\n    disable_metric_tensor=False,\n    seed=None,\n):\n    self.stepsize = stepsize\n    self.reg = regularization\n    self.finite_diff_step = finite_diff_step\n    self.metric_tensor = None\n    self.k = 1\n    self.resamplings = resamplings\n    self.blocking = blocking\n    self.last_n_steps = np.zeros(history_length)\n    self.history_length = history_length\n    self.disable_metric_tensor = disable_metric_tensor\n    random.seed(seed)\n    return\n\ndef step(self, cost, params):\n    \"\"\"Update trainable arguments with one step of the optimizer.\n    .. warning::\n        When blocking is set to be True, use step_and_cost instead, as loss\n        measurements are required for the updates for the case.\n    Args:\n        cost (qml.QNode): the QNode wrapper for the objective function for\n        optimization\n        params (np.array): Parameter before update.\n    Returns:\n        np.array: The new variable values after step-wise update.\n    \"\"\"\n    if self.blocking:\n        warnings.warn(\n            \"step_and_cost() instead of step() is called when \"\n            \"blocking is turned on, as the step-wise loss value \"\n            \"is required by the algorithm.\",\n            stacklevel=2,\n        )\n        return self.step_and_cost(cost, params)[0]\n\n    if self.disable_metric_tensor:\n        return self.__step_core_first_order(cost, params)\n    return self.__step_core(cost, params)\n\ndef step_and_cost(self, cost, params):\n    \"\"\"Update trainable parameters with one step of the optimizer and return\n    the corresponding objective function value after the step.\n    Args:\n        cost (qml.QNode): the QNode wrapper for the objective function for\n            optimization\n        params (np.array): Parameter before update.\n    Returns:\n        tuple[np.array, float]: the updated parameter and the objective\n            function output before the step.\n    \"\"\"\n    params_next = (\n        self.__step_core_first_order(cost, params)\n        if self.disable_metric_tensor\n        else self.__step_core(cost, params)\n    )\n\n    if not self.blocking:\n        loss_curr = cost(params)\n        return params_next, loss_curr\n    params_next, loss_curr = self.__apply_blocking(cost, params, params_next)\n    return params_next, loss_curr\n\ndef __step_core(self, cost, params):\n    # Core function that returns the next parameters before applying blocking.\n    grad_avg = np.zeros(params.shape)\n    tensor_avg = np.zeros((params.size, params.size))\n    for i in range(self.resamplings):\n        grad_tapes, grad_dir = self.__get_spsa_grad_tapes(cost, params)\n        metric_tapes, tensor_dirs = self.__get_tensor_tapes(cost, params)\n        raw_results = qml.execute(grad_tapes + metric_tapes, cost.device, None)\n        grad = self.__post_process_grad(raw_results[:2], grad_dir)\n        metric_tensor = self.__post_process_tensor(raw_results[2:], tensor_dirs)\n        grad_avg = grad_avg * i / (i + 1) + grad / (i + 1)\n        tensor_avg = tensor_avg * i / (i + 1) + metric_tensor / (i + 1)\n\n    self.__update_tensor(tensor_avg)\n    return self.__get_next_params(params, grad_avg)\n\ndef __step_core_first_order(self, cost, params):\n    # Reduced core function that returns the next parameters with SPSA rule.\n    # Blocking not applied.\n    grad_avg = np.zeros(params.shape)\n    for i in range(self.resamplings):\n        grad_tapes, grad_dir = self.__get_spsa_grad_tapes(cost, params)\n        raw_results = qml.execute(grad_tapes, cost.device, None)\n        grad = self.__post_process_grad(raw_results, grad_dir)\n        grad_avg = grad_avg * i / (i + 1) + grad / (i + 1)\n    return params - self.stepsize * grad_avg\n\ndef __post_process_grad(self, grad_raw_results, grad_dir):\n    # With the quantum measurement results from the 2 gradient tapes,\n    # compute the estimated gradient. Returned gradient is a tensor\n    # of the same shape with the input parameter tensor.\n    loss_forward, loss_backward = grad_raw_results\n    grad = (loss_forward - loss_backward) / (2 * self.finite_diff_step) * grad_dir\n    return grad\n\ndef __post_process_tensor(self, tensor_raw_results, tensor_dirs):\n    # With the quantum measurement results from the 4 metric tensor tapes,\n    # compute the estimated raw metric tensor. Returned raw metric tensor\n    # is a tensor of shape (d x d), d being the dimension of the input parameter\n    # to the ansatz.\n    tensor_finite_diff = (\n        tensor_raw_results[0][0][0]\n        - tensor_raw_results[1][0][0]\n        - tensor_raw_results[2][0][0]\n        + tensor_raw_results[3][0][0]\n    )\n    metric_tensor = (\n        -(\n            np.tensordot(tensor_dirs[0], tensor_dirs[1], axes=0)\n            + np.tensordot(tensor_dirs[1], tensor_dirs[0], axes=0)\n        )\n        * tensor_finite_diff\n        / (8 * self.finite_diff_step * self.finite_diff_step)\n    )\n    return metric_tensor\n\ndef __get_next_params(self, params, gradient):\n    grad_vec, params_vec = gradient.reshape(-1), params.reshape(-1)\n    new_params_vec = np.linalg.solve(\n        self.metric_tensor,\n        (-self.stepsize * grad_vec + np.matmul(self.metric_tensor, params_vec)),\n    )\n    return new_params_vec.reshape(params.shape)\n\ndef __get_perturbation_direction(self, params):\n    param_number = len(params) if isinstance(params, list) else params.size\n    sample_list = random.choices([-1, 1], k=param_number)\n    direction = np.array(sample_list).reshape(params.shape)\n    return direction\n\ndef __get_spsa_grad_tapes(self, cost, params):\n    # Returns the 2 tapes along with the sampled direction that will be\n    # used to estimate the gradient per optimization step. The sampled\n    # direction is of the shape of the input parameter.\n    direction = self.__get_perturbation_direction(params)\n    cost.construct([params + self.finite_diff_step * direction], {})\n    tape_forward = cost.tape.copy(copy_operations=True)\n    cost.construct([params - self.finite_diff_step * direction], {})\n    tape_backward = cost.tape.copy(copy_operations=True)\n    return [tape_forward, tape_backward], direction\n\ndef __update_tensor(self, tensor_raw):\n    tensor_avg = self.__get_tensor_moving_avg(tensor_raw)\n    tensor_regularized = self.__regularize_tensor(tensor_avg)\n    self.metric_tensor = tensor_regularized\n    self.k += 1\n\ndef __get_tensor_tapes(self, cost, params):\n    # Returns the 4 tapes along with the 2 sampled directions that will be\n    # used to estimate the raw metric tensor per optimization step. The sampled\n    # directions are 1d vectors of the length of the input parameter dimension.\n    dir1 = self.__get_perturbation_direction(params)\n    dir2 = self.__get_perturbation_direction(params)\n    perturb1 = dir1 * self.finite_diff_step\n    perturb2 = dir2 * self.finite_diff_step\n    dir_vecs = dir1.reshape(-1), dir2.reshape(-1)\n\n    tapes = [\n        self.__get_overlap_tape(cost, params, params + perturb1 + perturb2),\n        self.__get_overlap_tape(cost, params, params + perturb1),\n        self.__get_overlap_tape(cost, params, params - perturb1 + perturb2),\n        self.__get_overlap_tape(cost, params, params - perturb1),\n    ]\n    return tapes, dir_vecs\n\ndef __get_overlap_tape(self, cost, params1, params2):\n    op_forward = self.__get_operations(cost, params1)\n    op_inv = self.__get_operations(cost, params2)\n\n    with qml.tape.QuantumTape() as tape:\n        for op in op_forward:\n            qml.apply(op)\n        for op in reversed(op_inv):\n            print(op)\n            print(op.adjoint())\n            op.adjoint()\n        qml.probs(wires=cost.tape.wires.labels)\n    return tape\n\ndef __get_operations(self, cost, params):\n    # Given a QNode, returns the list of operations before the measurement.\n    cost.construct([params], {})\n    return cost.tape.operations\n\ndef __get_tensor_moving_avg(self, metric_tensor):\n    # For numerical stability: averaging on the Fubini-Study metric tensor.\n    if self.metric_tensor is None:\n        self.metric_tensor = np.identity(metric_tensor.shape[0])\n    return self.k / (self.k + 1) * self.metric_tensor + 1 / (self.k + 1) * metric_tensor\n\ndef __regularize_tensor(self, metric_tensor):\n    # For numerical stability: Fubini-Study metric tensor regularization.\n    tensor_reg = np.real(sqrtm(np.matmul(metric_tensor, metric_tensor)))\n    return (tensor_reg + self.reg * np.identity(metric_tensor.shape[0])) / (1 + self.reg)\n\ndef __apply_blocking(self, cost, params_curr, params_next):\n    # For numerical stability: apply the blocking condition on the parameter update.\n    cost.construct([params_curr], {})\n    tape_loss_curr = cost.tape.copy(copy_operations=True)\n    cost.construct([params_next], {})\n    tape_loss_next = cost.tape.copy(copy_operations=True)\n\n    loss_curr, loss_next = qml.execute([tape_loss_curr, tape_loss_next], cost.device, None)\n    # self.k has been updated earlier.\n    ind = (self.k - 2) % self.history_length\n    self.last_n_steps[ind] = loss_curr\n\n    tol = (\n        2 * self.last_n_steps.std()\n        if self.k > self.history_length\n        else 2 * self.last_n_steps[: self.k - 1].std()\n    )\n\n    if loss_curr + tol < loss_next:\n        params_next = params_curr\n    return params_next, loss_curr\n\n\ngeo_file = \"h2.xyz\"\n\nsymbols, coordinates = qml.qchem.read_structure(geo_file)\nhamiltonian, qubits = qml.qchem.molecular_hamiltonian(symbols, coordinates)\n\nprint(coordinates)\n\nprint(hamiltonian)\n\nprint(\"Number of qubits = \", qubits)\ndev = qml.device(\"default.qubit\", wires=qubits, shots = 1000)\n\ndef AllSinglesDoubles(init_state, singles = [], doubles =[]):\ndef circuit(params, wires):\n    qml.templates.AllSinglesDoubles(weights = params, wires = wires,\n                    hf_state = init_state, singles = singles, doubles = doubles)\n\nreturn circuit, (len(singles) + len(doubles))\n\nhf_state = np.array([1, 1, 0, 0], requires_grad=True)\nsingles, doubles = qml.qchem.excitations(2, 4)\n\nansatz, num_params = AllSinglesDoubles(init_state = hf_state, singles = singles, doubles = doubles)\nprint(num_params)\nparameters = np.random.uniform(low=0, high=2 * np.pi, size=num_params, requires_grad=True)\nprint(parameters)\n\n@qml.qnode(dev)\ndef cost(params):\nansatz(params, wires = range(qubits))\nreturn qml.expval(hamiltonian)\n\nopt = QNSPSA()\nqngd_cost = []\nparameters, energy = opt.step_and_cost(cost, parameters)\nqngd_cost.append(energy)\n\nmax_iterations = 200\nconv_tol = 10e-6\nexact_value = -1.136189454088\n\nwith qml.Tracker(cost.device) as tracker:\nfor n in range(max_iterations):\n    parameters, energy = opt.step_and_cost(cost, parameters)\n    conv = np.abs(energy - qngd_cost[-1])\n    qngd_cost.append(energy)\n    \n\n    if n % 4 == 0:\n        print(\n        \"Iteration = {:},  Energy = {:.8f} Ha\".format(n, energy)\n        )\n\n    if conv <= conv_tol:\n        break\n\n\nprint(\"\\nFinal convergence parameter = {:.8f} Ha\".format(conv))\nprint(\"Number of iterations = \", n)\nprint(\"Final value of the ground-state energy = {:.8f} Ha\".format(energy))\nprint(\"Accuracy with respect to the FCI energy: {:.8f} Ha ({:.8f} kcal/mol)\".format(\n    np.abs(energy - exact_value), np.abs(energy - exact_value) * 627.503))\nprint()\nprint(\"Final circuit parameters = \\n\", parameters)\nprint(tracker.totals)'", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1663276576896", "content": "Hi @trifire,\nUnfortunately I\u2019m not being able to run your code due to some indentation errors. If you could send a minimal example that shows the error you\u2019re getting?", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/2"}, "2": {"author": "Crol", "date": "1684160408171", "content": "Here is hotfix.\nwrap around Operator that has AdjointUndefinedError into two adjoints\nExample\nqml.adjoint(qml.adjoint(qml.AmplitudeEmbedding())", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/3"}, "3": {"author": "Lucas_Tecot", "date": "1690265137995", "content": "I\u2019m having the same issue with RandomLayers, works fine with a custom defined model but not with the random layers ansatz.", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/4"}, "4": {"author": "Ivana_at_Xanadu", "date": "1690280289895", "content": "Hi @Lucas_Tecot , do you mind sharing a minimal example of your code, so I understand exactly what you mean? ", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/5"}, "5": {"author": "Lucas_Tecot", "date": "1690337859115", "content": "import pennylane as qml\nfrom pennylane import numpy as plnp\n\nnum_qubits = 2\ndev = qml.device(\"default.qubit\", wires=num_qubits)\n@qml.qnode(dev)\ndef cost(params):\n    qml.RandomLayers(weights=params, wires=num_qubits, seed=42)\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1))\n\nparams = plnp.random.normal(0, plnp.pi, (2, 4))\nopt = qml.QNSPSAOptimizer(stepsize=5e-2)\nfor i in range(51):\n    params, loss = opt.step_and_cost(cost, params)\n    if i % 10 == 0:\n        print(f\"Step {i}: cost = {loss:.4f}\")\n\nError message:\nTraceback (most recent call last):\n  File \"/home/lucastecot/development/quantum_optimization/sandbox.py\", line 19, in <module>\n    params, loss = opt.step_and_cost(cost, params)\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 185, in step_and_cost\n    params_next = self._step_core(cost, args, kwargs)\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 215, in _step_core\n    metric_tapes, tensor_dirs = self._get_tensor_tapes(cost, args, kwargs)\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 396, in _get_tensor_tapes\n    tapes = [\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 397, in <listcomp>\n    self._get_overlap_tape(cost, args, args_finite_diff, kwargs)\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 411, in _get_overlap_tape\n    new_ops = op_forward + [op.adjoint() for op in reversed(op_inv)]\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/optimize/qnspsa.py\", line 411, in <listcomp>\n    new_ops = op_forward + [op.adjoint() for op in reversed(op_inv)]\n  File \"/home/lucastecot/miniconda3/envs/quantum_optimization/lib/python3.10/site-packages/pennylane/operation.py\", line 1381, in adjoint\n    raise AdjointUndefinedError\npennylane.operation.AdjointUndefinedError\n", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/6"}, "6": {"author": "Ivana_at_Xanadu", "date": "1690383955608", "content": "Thanks, @Lucas_Tecot !\nHmm, this seems a bit unusual.  I can spot that your call for qml.RandomLayers should actually look like this:\nqml.RandomLayers(weights=params, wires=range(num_qubits), seed=42)\n\n\u2026 but let me check what\u2019s going on here with the optimizer \u2014 I\u2019ll get back to you soon!", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/7"}, "7": {"author": "Ivana_at_Xanadu", "date": "1690387878130", "content": "Hey @Lucas_Tecot, this is definitely a bug! Congrats on spotting it and thank you for letting us know! \nWould you like to quickly open an issue in the PennyLane repository with this example? We can sort out the fix for you there.", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/8"}, "8": {"author": "Ivana_at_Xanadu", "date": "1690892119935", "content": "Update:\nWe\u2019ve now solved the problem with qml.ONSPSAOptimizer, which should fix the AdjointUndefinedError that was popping up in this case.\n\nI\u2019ve now gone ahead and made a bug report 1 in the PennyLane repository.\n@Lucas_Tecot, you can let me know if you want to be tagged for credit, and I\u2019ll share an update here as soon as we fix it. ", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/9"}, "9": {"author": "trifire", "date": "1663189452292", "content": "I am trying to use the code from pennylane\u2019s QNSPSA tutorial linked here 3 to run a variational quantum eigensolver.\nHowever, if I use a template for my ansatz such as qml.templates.AllSinglesDoubles\nI get an error: AdjointUndefinedError.\nI have stepped through my code, and I have found that the issue is that BasisState does not have a defined adjoint.\nIs there a good way to fix this?\nFor reference, this is my full code:\nfrom types import FunctionType\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport math\n\nimport random\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom scipy.linalg import sqrtm\nimport warnings\nfrom functools import partial\n\nclass QNSPSA:\n\"\"\"Quantum natural SPSA optimizer. Refer to https://quantum-journal.org/papers/q-2021-10-20-567/\nfor a detailed description of the methodology. When disable_metric_tensor\nis set to be True, the metric tensor estimation is disabled, and QNSPSA is\nreduced to be a SPSA optimizer.\nFrom: https://github.com/PennyLaneAI/qml/blob/master/demonstrations/qnspsa.py\nArgs:\n    stepsize (float): The learn rate.\n    regularization (float): Regularitzation term to the Fubini-Study\n        metric tensor for numerical stability.\n    finite_diff_step (float): step size to compute the finite difference\n        gradient and the Fubini-Study metric tensor.\n    resamplings (int): The number of samples to average for each parameter\n        update.\n    blocking (boolean): When set to be True, the optimizer only accepts\n        updates that lead to a loss value no larger than the loss value\n        before update, plus a tolerance. The tolerance is set with the\n        parameter history_length.\n    history_length (int): When blocking is True, the tolerance is set to be\n        the average of the cost values in the last history_length steps.\n    disable_metric_tensor (boolean): When set to be True, the optimizer is\n        reduced to be a (1st-order) SPSA optimizer.\n    seed (int): Seed for the random sampling.\n\"\"\"\n\ndef __init__(\n    self,\n    stepsize=1e-3,\n    regularization=1e-3,\n    finite_diff_step=1e-2,\n    resamplings=1,\n    blocking=True,\n    history_length=5,\n    disable_metric_tensor=False,\n    seed=None,\n):\n    self.stepsize = stepsize\n    self.reg = regularization\n    self.finite_diff_step = finite_diff_step\n    self.metric_tensor = None\n    self.k = 1\n    self.resamplings = resamplings\n    self.blocking = blocking\n    self.last_n_steps = np.zeros(history_length)\n    self.history_length = history_length\n    self.disable_metric_tensor = disable_metric_tensor\n    random.seed(seed)\n    return\n\ndef step(self, cost, params):\n    \"\"\"Update trainable arguments with one step of the optimizer.\n    .. warning::\n        When blocking is set to be True, use step_and_cost instead, as loss\n        measurements are required for the updates for the case.\n    Args:\n        cost (qml.QNode): the QNode wrapper for the objective function for\n        optimization\n        params (np.array): Parameter before update.\n    Returns:\n        np.array: The new variable values after step-wise update.\n    \"\"\"\n    if self.blocking:\n        warnings.warn(\n            \"step_and_cost() instead of step() is called when \"\n            \"blocking is turned on, as the step-wise loss value \"\n            \"is required by the algorithm.\",\n            stacklevel=2,\n        )\n        return self.step_and_cost(cost, params)[0]\n\n    if self.disable_metric_tensor:\n        return self.__step_core_first_order(cost, params)\n    return self.__step_core(cost, params)\n\ndef step_and_cost(self, cost, params):\n    \"\"\"Update trainable parameters with one step of the optimizer and return\n    the corresponding objective function value after the step.\n    Args:\n        cost (qml.QNode): the QNode wrapper for the objective function for\n            optimization\n        params (np.array): Parameter before update.\n    Returns:\n        tuple[np.array, float]: the updated parameter and the objective\n            function output before the step.\n    \"\"\"\n    params_next = (\n        self.__step_core_first_order(cost, params)\n        if self.disable_metric_tensor\n        else self.__step_core(cost, params)\n    )\n\n    if not self.blocking:\n        loss_curr = cost(params)\n        return params_next, loss_curr\n    params_next, loss_curr = self.__apply_blocking(cost, params, params_next)\n    return params_next, loss_curr\n\ndef __step_core(self, cost, params):\n    # Core function that returns the next parameters before applying blocking.\n    grad_avg = np.zeros(params.shape)\n    tensor_avg = np.zeros((params.size, params.size))\n    for i in range(self.resamplings):\n        grad_tapes, grad_dir = self.__get_spsa_grad_tapes(cost, params)\n        metric_tapes, tensor_dirs = self.__get_tensor_tapes(cost, params)\n        raw_results = qml.execute(grad_tapes + metric_tapes, cost.device, None)\n        grad = self.__post_process_grad(raw_results[:2], grad_dir)\n        metric_tensor = self.__post_process_tensor(raw_results[2:], tensor_dirs)\n        grad_avg = grad_avg * i / (i + 1) + grad / (i + 1)\n        tensor_avg = tensor_avg * i / (i + 1) + metric_tensor / (i + 1)\n\n    self.__update_tensor(tensor_avg)\n    return self.__get_next_params(params, grad_avg)\n\ndef __step_core_first_order(self, cost, params):\n    # Reduced core function that returns the next parameters with SPSA rule.\n    # Blocking not applied.\n    grad_avg = np.zeros(params.shape)\n    for i in range(self.resamplings):\n        grad_tapes, grad_dir = self.__get_spsa_grad_tapes(cost, params)\n        raw_results = qml.execute(grad_tapes, cost.device, None)\n        grad = self.__post_process_grad(raw_results, grad_dir)\n        grad_avg = grad_avg * i / (i + 1) + grad / (i + 1)\n    return params - self.stepsize * grad_avg\n\ndef __post_process_grad(self, grad_raw_results, grad_dir):\n    # With the quantum measurement results from the 2 gradient tapes,\n    # compute the estimated gradient. Returned gradient is a tensor\n    # of the same shape with the input parameter tensor.\n    loss_forward, loss_backward = grad_raw_results\n    grad = (loss_forward - loss_backward) / (2 * self.finite_diff_step) * grad_dir\n    return grad\n\ndef __post_process_tensor(self, tensor_raw_results, tensor_dirs):\n    # With the quantum measurement results from the 4 metric tensor tapes,\n    # compute the estimated raw metric tensor. Returned raw metric tensor\n    # is a tensor of shape (d x d), d being the dimension of the input parameter\n    # to the ansatz.\n    tensor_finite_diff = (\n        tensor_raw_results[0][0][0]\n        - tensor_raw_results[1][0][0]\n        - tensor_raw_results[2][0][0]\n        + tensor_raw_results[3][0][0]\n    )\n    metric_tensor = (\n        -(\n            np.tensordot(tensor_dirs[0], tensor_dirs[1], axes=0)\n            + np.tensordot(tensor_dirs[1], tensor_dirs[0], axes=0)\n        )\n        * tensor_finite_diff\n        / (8 * self.finite_diff_step * self.finite_diff_step)\n    )\n    return metric_tensor\n\ndef __get_next_params(self, params, gradient):\n    grad_vec, params_vec = gradient.reshape(-1), params.reshape(-1)\n    new_params_vec = np.linalg.solve(\n        self.metric_tensor,\n        (-self.stepsize * grad_vec + np.matmul(self.metric_tensor, params_vec)),\n    )\n    return new_params_vec.reshape(params.shape)\n\ndef __get_perturbation_direction(self, params):\n    param_number = len(params) if isinstance(params, list) else params.size\n    sample_list = random.choices([-1, 1], k=param_number)\n    direction = np.array(sample_list).reshape(params.shape)\n    return direction\n\ndef __get_spsa_grad_tapes(self, cost, params):\n    # Returns the 2 tapes along with the sampled direction that will be\n    # used to estimate the gradient per optimization step. The sampled\n    # direction is of the shape of the input parameter.\n    direction = self.__get_perturbation_direction(params)\n    cost.construct([params + self.finite_diff_step * direction], {})\n    tape_forward = cost.tape.copy(copy_operations=True)\n    cost.construct([params - self.finite_diff_step * direction], {})\n    tape_backward = cost.tape.copy(copy_operations=True)\n    return [tape_forward, tape_backward], direction\n\ndef __update_tensor(self, tensor_raw):\n    tensor_avg = self.__get_tensor_moving_avg(tensor_raw)\n    tensor_regularized = self.__regularize_tensor(tensor_avg)\n    self.metric_tensor = tensor_regularized\n    self.k += 1\n\ndef __get_tensor_tapes(self, cost, params):\n    # Returns the 4 tapes along with the 2 sampled directions that will be\n    # used to estimate the raw metric tensor per optimization step. The sampled\n    # directions are 1d vectors of the length of the input parameter dimension.\n    dir1 = self.__get_perturbation_direction(params)\n    dir2 = self.__get_perturbation_direction(params)\n    perturb1 = dir1 * self.finite_diff_step\n    perturb2 = dir2 * self.finite_diff_step\n    dir_vecs = dir1.reshape(-1), dir2.reshape(-1)\n\n    tapes = [\n        self.__get_overlap_tape(cost, params, params + perturb1 + perturb2),\n        self.__get_overlap_tape(cost, params, params + perturb1),\n        self.__get_overlap_tape(cost, params, params - perturb1 + perturb2),\n        self.__get_overlap_tape(cost, params, params - perturb1),\n    ]\n    return tapes, dir_vecs\n\ndef __get_overlap_tape(self, cost, params1, params2):\n    op_forward = self.__get_operations(cost, params1)\n    op_inv = self.__get_operations(cost, params2)\n\n    with qml.tape.QuantumTape() as tape:\n        for op in op_forward:\n            qml.apply(op)\n        for op in reversed(op_inv):\n            print(op)\n            print(op.adjoint())\n            op.adjoint()\n        qml.probs(wires=cost.tape.wires.labels)\n    return tape\n\ndef __get_operations(self, cost, params):\n    # Given a QNode, returns the list of operations before the measurement.\n    cost.construct([params], {})\n    return cost.tape.operations\n\ndef __get_tensor_moving_avg(self, metric_tensor):\n    # For numerical stability: averaging on the Fubini-Study metric tensor.\n    if self.metric_tensor is None:\n        self.metric_tensor = np.identity(metric_tensor.shape[0])\n    return self.k / (self.k + 1) * self.metric_tensor + 1 / (self.k + 1) * metric_tensor\n\ndef __regularize_tensor(self, metric_tensor):\n    # For numerical stability: Fubini-Study metric tensor regularization.\n    tensor_reg = np.real(sqrtm(np.matmul(metric_tensor, metric_tensor)))\n    return (tensor_reg + self.reg * np.identity(metric_tensor.shape[0])) / (1 + self.reg)\n\ndef __apply_blocking(self, cost, params_curr, params_next):\n    # For numerical stability: apply the blocking condition on the parameter update.\n    cost.construct([params_curr], {})\n    tape_loss_curr = cost.tape.copy(copy_operations=True)\n    cost.construct([params_next], {})\n    tape_loss_next = cost.tape.copy(copy_operations=True)\n\n    loss_curr, loss_next = qml.execute([tape_loss_curr, tape_loss_next], cost.device, None)\n    # self.k has been updated earlier.\n    ind = (self.k - 2) % self.history_length\n    self.last_n_steps[ind] = loss_curr\n\n    tol = (\n        2 * self.last_n_steps.std()\n        if self.k > self.history_length\n        else 2 * self.last_n_steps[: self.k - 1].std()\n    )\n\n    if loss_curr + tol < loss_next:\n        params_next = params_curr\n    return params_next, loss_curr\n\n\ngeo_file = \"h2.xyz\"\n\nsymbols, coordinates = qml.qchem.read_structure(geo_file)\nhamiltonian, qubits = qml.qchem.molecular_hamiltonian(symbols, coordinates)\n\nprint(coordinates)\n\nprint(hamiltonian)\n\nprint(\"Number of qubits = \", qubits)\ndev = qml.device(\"default.qubit\", wires=qubits, shots = 1000)\n\ndef AllSinglesDoubles(init_state, singles = [], doubles =[]):\ndef circuit(params, wires):\n    qml.templates.AllSinglesDoubles(weights = params, wires = wires,\n                    hf_state = init_state, singles = singles, doubles = doubles)\n\nreturn circuit, (len(singles) + len(doubles))\n\nhf_state = np.array([1, 1, 0, 0], requires_grad=True)\nsingles, doubles = qml.qchem.excitations(2, 4)\n\nansatz, num_params = AllSinglesDoubles(init_state = hf_state, singles = singles, doubles = doubles)\nprint(num_params)\nparameters = np.random.uniform(low=0, high=2 * np.pi, size=num_params, requires_grad=True)\nprint(parameters)\n\n@qml.qnode(dev)\ndef cost(params):\nansatz(params, wires = range(qubits))\nreturn qml.expval(hamiltonian)\n\nopt = QNSPSA()\nqngd_cost = []\nparameters, energy = opt.step_and_cost(cost, parameters)\nqngd_cost.append(energy)\n\nmax_iterations = 200\nconv_tol = 10e-6\nexact_value = -1.136189454088\n\nwith qml.Tracker(cost.device) as tracker:\nfor n in range(max_iterations):\n    parameters, energy = opt.step_and_cost(cost, parameters)\n    conv = np.abs(energy - qngd_cost[-1])\n    qngd_cost.append(energy)\n    \n\n    if n % 4 == 0:\n        print(\n        \"Iteration = {:},  Energy = {:.8f} Ha\".format(n, energy)\n        )\n\n    if conv <= conv_tol:\n        break\n\n\nprint(\"\\nFinal convergence parameter = {:.8f} Ha\".format(conv))\nprint(\"Number of iterations = \", n)\nprint(\"Final value of the ground-state energy = {:.8f} Ha\".format(energy))\nprint(\"Accuracy with respect to the FCI energy: {:.8f} Ha ({:.8f} kcal/mol)\".format(\n    np.abs(energy - exact_value), np.abs(energy - exact_value) * 627.503))\nprint()\nprint(\"Final circuit parameters = \\n\", parameters)\nprint(tracker.totals)'", "link": "https://discuss.pennylane.ai//t/basisstate-raises-adjoint-undefined-error/2160/10"}}