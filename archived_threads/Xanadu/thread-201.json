{"0": {"author": "MUSHKAN_SUREKA", "date": "1655815217513", "content": "Hello everyone,\nThe two questions that I would want to ask you from your paper,  \u201cMeasuring the similarity of graphs with a Gaussian boson sampler\u201d  are as follows:-\n\nThe probability of measuring a given photon-counting event given in the paper (Page 2 and equation 3) gives results greater than 1 when I do a manual calculation for many cases, for example, if I have a 9 node graph and want to detect one photon in the first and second mode of the GBS, Hafnian results in 1 and n! is 1 again, the det(Q) is coming less than 1 and anything divided by something less than 1 results in greater than 1. However, I did use the code on Xanadu for graph similarity and that did give a probability of less than 1. I am sure I might be missing something. I hope you could clear that up.\nI do not understand constructing a similarity measure or kernel that computes the similarity between two graphs G and G\u2019. Is it a dot product between the two feature vectors? I hope you could elaborate on the use of linear and radial basis kernels since the Xanadu code documentation does not show how the similarity was measured in the Python programming language. Also if you could elaborate on the term  \u201csimilarity\u201d . I am unsure as to what similarity means in this context.\n\nFor your convenience, I have attached the summary of the question I am talking about in this message.\nIn the document, I mistakenly added n! inside the square root, it would be outside and I have taken n_100 in the introduction section but it would be n_m. Apologies for this error, however, the examples are using the right formula.\nLooking forward to your reply.\n\n35%20PM2414\u00d71662 315 KB\n\n\n\n Solved by Sebastian_Duque_Mesa in post #13 \n\n\n                Let me give a shot at your question: \nProbabilities of photon counting events\nOn the examples listed on the document it seems that you\u2019re missing the normalization constant when calculating \\bar A. From the paper you cited: \n\nthe rescaling constant c is chosen so that 0 <\nc < 1/s_{max}, and s_{max} \u2026\n              \n1 Reply", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/1"}, "1": {"author": "ziofil", "date": "1655844994332", "content": "n! is not under the square root.\nAlso, note that the A tilde matrix is rescaled so that it has max singular value of 1 (or slightly below 1).\nLet us know if that fixes your issues!2 Replies", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/2"}, "2": {"author": "MUSHKAN_SUREKA", "date": "1655976061757", "content": "Thank you for your help but if the A tilde matrix has a max singular value of 1 (or slightly below) then the det|Q| will always have a value less than 1 since the A tilde has 1 entry, X and I also have 0\u2019s and 1\u2019s as singular entry. Hence the probability is coming more than 1.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/3"}, "3": {"author": "MUSHKAN_SUREKA", "date": "1655976215911", "content": "Also, I understand that n! is outside the square root, it is a printing mistake from my side but I am anyway using a sample where I need one photon detection at each mode so n! effectively comes to 1.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/4"}, "4": {"author": "ziofil", "date": "1656097934977", "content": "now I\u2019m confused too. the hafnian function from the walrus evaluates to zero on that graph ", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/5"}, "5": {"author": "MUSHKAN_SUREKA", "date": "1656099535640", "content": "Hey!\nSo you are getting hafnian of the graph as 0 because you might be taking the adjacency matrix of the whole graph. According to the paper, \u201cEquation (3) does not depend on the Hafnian of the adjacency matrix A, but on a matrix An. An contains nj duplicates of the jth row and column in A. If nj = 0, the jth row/column in A does not appear in An.\u201d\nThis matrix would result in hafnian as 1.1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1656361589294", "content": "Thanks for this clarification @MUSHKAN_SUREKA!\nHas your question been resolved? Or is it still standing?", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/7"}, "7": {"author": "MUSHKAN_SUREKA", "date": "1656363495843", "content": "Hello,\nUnfortunately, it is still standing ", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1656507538674", "content": "Oh, I\u2019ve asked another colleague to take a look at your question. It\u2019s not an easy question though!", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/9"}, "9": {"author": "MUSHKAN_SUREKA", "date": "1656508254789", "content": "Haha yeah! It has been bugging me for a while and I really hope it gets resolved. \nAny idea when can I hear back from your colleague?", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/10"}, "10": {"author": "Sebastian_Duque_Mesa", "date": "1656519728110", "content": "Hi @MUSHKAN_SUREKA! Happy to help here. I\u2019ll get back to you in short.1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/11"}, "11": {"author": "Sebastian_Duque_Mesa", "date": "1656534966402", "content": "Let me give a shot at your question:\nProbabilities of photon counting events\nOn the examples listed on the document it seems that you\u2019re missing the normalization constant when calculating \\bar A. From the paper you cited:\n\nthe rescaling constant c is chosen so that 0 <\nc < 1/s_{max}, and s_{max} is the maximum singular value of\nA.\n\nPrecisely, the rescaling constant is chosen to be 1/(s_{max}+10^{-8})\nThe following code calculates the prefactor 1/\\sqrt{\\det Q} accordingly for both examples on the attached file\nimport numpy as np\n\n# Example 1\n    # A = np.array([[0,1,1],\n    #              [1,0,1],\n    #              [1,1,0]])\n\n# Example 2\nA = np.array([[0,0,1,0,1],\n             [0,0,0,1,1],\n             [1,0,0,1,1],\n             [0,1,1,0,0],\n             [1,1,1,0,0]])\n\n\n# calculate max singular value and rescaling constant\n_, s, _ = np.linalg.svd(A, full_matrices=True)\nc = 1 / ( np.max(s) + 1e-8 )\nprint(f\"Max singular value: {np.max(s)}\")\nprint(f\"Rescaling constant value: {c:.2E}\\n\")\n\n# calculate Q and its determinant\nAb = c * np.block([\n    [A, np.zeros(A.shape)],\n    [np.zeros(A.shape), A]\n])\n\nX = np.block([\n    [np.zeros(A.shape), np.identity(A.shape[0])],\n    [np.identity(A.shape[0]), np.zeros(A.shape)]\n])\n\nI = np.identity(Ab.shape[0])\nQ = np.linalg.inv(I - X @ Ab)\ndetQ = np.linalg.det(Q)\n\nprint(f\"det(Q) = {detQ:.2E}\")\nprint(f\"1/sqrt(det(Q)) = {1/np.sqrt(detQ):.2E}\")\n\nFor example 1\nMax singular value: 2.0\nRescaling constant value: 5.00E-01\n\ndet(Q) = 1.78E+08\n1/sqrt(det(Q)) = 7.50E-05\n\nand example 2\nMax singular value: 2.481194304092015\nRescaling constant value: 4.03E-01\n\ndet(Q) = 4.94E+08\n1/sqrt(det(Q)) = 4.50E-05\n\nAs seen in the results the prefactors are below one. In particular for example 2, the probability of observing the sample [1,1,1,1,0] is p=4.5\\times 10^{-5} as the Hafnian equals one as well as \\boldsymbol n!.\nSimilarity measures\nThere are many different notions of similarity and they depend on which property of the graph is exploited to compare one graph to another.\nThe idea of the paper is to use the properties of the samples from the GBS machine to assign to each graph a vector in \\mathbb{R}^N. This vectors are called feature vectors because they encode the properties of each graph in a familiar space: one in which we can define a measure of \u201ccloseness\u201d or \u201csimilarity\u201d.\nThe idea is the following, in \\mathbb{R}^N we say two vectors \\boldsymbol f and \\boldsymbol f' are similar if the dot product\n\\langle \\boldsymbol f, \\boldsymbol f'\\rangle between them is zero, or close to each other (pointing to a similar direction) if close to zero. If the encoding proposed is capable of generating a vector \\boldsymbol f in \\mathbb{R}^N for each graph G then, by association, we can say that two graphs are similar if the vectors that represent them are close to each other. Since we already know that the dot product is a good measure of similarity, then we can use that measure to compare different graphs when represented as vectors:\n\n\\kappa(G, G') = \\langle \\boldsymbol f, \\boldsymbol f'\\rangle.\n\nThe above means that we can express the similarity of two graphs as the inner product of two vectors \u2014 thanks to the feature map.\nHowever, the kernel defined above is linear and might not be able to learn non-linear decision boundaries on classification tasks. In that case one can define a different kernel, like the radial basis function kernel \\kappa_{rbf} mentioned in the paper.\n\nHope this helps to solve your question! 2 RepliesSolution1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/12"}, "12": {"author": "MUSHKAN_SUREKA", "date": "1656536804496", "content": "\n\n\n MUSHKAN_SUREKA:\n\nMeasuring the similarity of graphs with a Gaussian boson sample\n\n\nAh okay! Got it! This was really helpful. Thankyou!1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/13"}, "13": {"author": "MUSHKAN_SUREKA", "date": "1677135423900", "content": "Hello! Sorry to disturb you again but according to the paper, there is a formula given for the covariance matrix. Now I am trying to calculate the mean-photon number at the output state of the GBS using the trace of the covariance matrix but I am getting a huge number : 248119457.09197953\nThis is classically intractable and no GBS can accommodate that many photons. Could you please look into this?\n-Thanks!", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/14"}, "14": {"author": "CatalinaAlbornoz", "date": "1677268904668", "content": "Hi @MUSHKAN_SUREKA,\nCould you please share the code that you\u2019re using? I would like to see if I can replicate your problem.\nThanks!", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/15"}, "15": {"author": "MUSHKAN_SUREKA", "date": "1677269099238", "content": "Sure! here you go\nA = np.array([[0,0,1,0,1],\n         [0,0,0,1,1],\n\n         [1,0,0,1,1],\n\n         [0,1,1,0,0],\n\n         [1,1,1,0,0]])\n\n\ncalculate max singular value and rescaling constant\n_, s, _ = np.linalg.svd(A1, full_matrices=True)\nc = 1 / ( np.max(s) + 1e-8 )\nprint(f\"Max singular value: {np.max(s)}\")\nprint(f\"Rescaling constant value: {c:.2E}\\n\")\n\ncalculate Q and its determinant\nAb = c * np.block([\n[A1, np.zeros(A1.shape)],\n\n[np.zeros(A1.shape), A1]\n\n])\nX = np.block([\n[np.zeros(A1.shape), np.identity(A1.shape[0])],\n\n[np.identity(A1.shape[0]), np.zeros(A1.shape)]\n\n])\nI = np.identity(Ab.shape[0])\nQ = np.linalg.inv(I - X @ Ab)\nV= Q-(I/2)\ndetQ = np.linalg.det(Q)\nn = np.trace(V)/2 - 0.5\n#print(V)\n#print(f\"det(Q) = {detQ:.2E}\")\n#print(f\"1/sqrt(det(Q)) = {1/np.sqrt(detQ):.2E}\")\nprint(n)", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/16"}, "16": {"author": "CatalinaAlbornoz", "date": "1677271487297", "content": "Thanks @MUSHKAN_SUREKA. I\u2019m guessing A and A1 the same.\nI can replicate your result but I don\u2019t know if the math is right.\nAs Nathan mentioned on Slack 1 it\u2019s probably best to experiment with different values of the mean photon number.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/17"}, "17": {"author": "MUSHKAN_SUREKA", "date": "1677272294653", "content": "Yep A and A1 are the same, apologies for the confusion.\nI see, could you please ask any of your colleagues about the relation between the output covariance matrix and the mean photon number? I just wanted to be sure about the formula.\nThanks!", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/18"}, "18": {"author": "CatalinaAlbornoz", "date": "1677528852759", "content": "Hi @MUSHKAN_SUREKA, sure let me look around for an answer.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/19"}, "19": {"author": "CatalinaAlbornoz", "date": "1677531733208", "content": "Hi @MUSHKAN_SUREKA , what is exactly the formula that you\u2019re referring to?1 Reply", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/20"}, "20": {"author": "Sebastian_Duque_Mesa", "date": "1656534966402", "content": "Let me give a shot at your question:\nProbabilities of photon counting events\nOn the examples listed on the document it seems that you\u2019re missing the normalization constant when calculating \\bar A\u00afA. From the paper you cited:\n\nthe rescaling constant cc is chosen so that 0 <\nc < 1/s_{max}0<c<1/smax, and s_{max}smax is the maximum singular value of\nAA.\n\nPrecisely, the rescaling constant is chosen to be 1/(s_{max}+10^{-8})1/(smax+10\u22128)\nThe following code calculates the prefactor 1/\\sqrt{\\det Q}1/\u221adetQ accordingly for both examples on the attached file\nimport numpy as np\n\n# Example 1\n    # A = np.array([[0,1,1],\n    #              [1,0,1],\n    #              [1,1,0]])\n\n# Example 2\nA = np.array([[0,0,1,0,1],\n             [0,0,0,1,1],\n             [1,0,0,1,1],\n             [0,1,1,0,0],\n             [1,1,1,0,0]])\n\n\n# calculate max singular value and rescaling constant\n_, s, _ = np.linalg.svd(A, full_matrices=True)\nc = 1 / ( np.max(s) + 1e-8 )\nprint(f\"Max singular value: {np.max(s)}\")\nprint(f\"Rescaling constant value: {c:.2E}\\n\")\n\n# calculate Q and its determinant\nAb = c * np.block([\n    [A, np.zeros(A.shape)],\n    [np.zeros(A.shape), A]\n])\n\nX = np.block([\n    [np.zeros(A.shape), np.identity(A.shape[0])],\n    [np.identity(A.shape[0]), np.zeros(A.shape)]\n])\n\nI = np.identity(Ab.shape[0])\nQ = np.linalg.inv(I - X @ Ab)\ndetQ = np.linalg.det(Q)\n\nprint(f\"det(Q) = {detQ:.2E}\")\nprint(f\"1/sqrt(det(Q)) = {1/np.sqrt(detQ):.2E}\")\n\nFor example 1\nMax singular value: 2.0\nRescaling constant value: 5.00E-01\n\ndet(Q) = 1.78E+08\n1/sqrt(det(Q)) = 7.50E-05\n\nand example 2\nMax singular value: 2.481194304092015\nRescaling constant value: 4.03E-01\n\ndet(Q) = 4.94E+08\n1/sqrt(det(Q)) = 4.50E-05\n\nAs seen in the results the prefactors are below one. In particular for example 2, the probability of observing the sample [1,1,1,1,0] is p=4.5\\times 10^{-5}p=4.5\u00d710\u22125 as the Hafnian equals one as well as \\boldsymbol n!n!.\nSimilarity measures\nThere are many different notions of similarity and they depend on which property of the graph is exploited to compare one graph to another.\nThe idea of the paper is to use the properties of the samples from the GBS machine to assign to each graph a vector in \\mathbb{R}^NRN. This vectors are called feature vectors because they encode the properties of each graph in a familiar space: one in which we can define a measure of \u201ccloseness\u201d or \u201csimilarity\u201d.\nThe idea is the following, in \\mathbb{R}^NRN we say two vectors \\boldsymbol ff and \\boldsymbol f'f\u2032 are similar if the dot product\n\\langle \\boldsymbol f, \\boldsymbol f'\\rangle\u27e8f,f\u2032\u27e9 between them is zero, or close to each other (pointing to a similar direction) if close to zero. If the encoding proposed is capable of generating a vector \\boldsymbol ff in \\mathbb{R}^NRN for each graph GG then, by association, we can say that two graphs are similar if the vectors that represent them are close to each other. Since we already know that the dot product is a good measure of similarity, then we can use that measure to compare different graphs when represented as vectors:\n\n\\kappa(G, G') = \\langle \\boldsymbol f, \\boldsymbol f'\\rangle.\n\u03ba(G,G\u2032)=\u27e8f,f\u2032\u27e9.\nThe above means that we can express the similarity of two graphs as the inner product of two vectors \u2014 thanks to the feature map.\nHowever, the kernel defined above is linear and might not be able to learn non-linear decision boundaries on classification tasks. In that case one can define a different kernel, like the radial basis function kernel \\kappa_{rbf}\u03barbf mentioned in the paper.\n\nHope this helps to solve your question! 2 RepliesSolution1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/21"}, "21": {"author": "MUSHKAN_SUREKA", "date": "1677534241407", "content": "The mean photon number per mode = Trace(Covariance matrix)/N - 0.5", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/22"}, "22": {"author": "MUSHKAN_SUREKA", "date": "1677534271274", "content": "where N is the number of modes", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/23"}, "23": {"author": "Sebastian_Duque_Mesa", "date": "1677540402071", "content": "Hi @MUSHKAN_SUREKA,\nI can\u2019t seem to find the formula you mention. Where in the paper is it located?3 Replies", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/24"}, "24": {"author": "MUSHKAN_SUREKA", "date": "1677606502539", "content": "Hello Sebastian,\nEven I cannot find a relevant paper but I asked my professor and he told me about this. Th most relevant link I could find is continuous variable - How to write the covariance matrix of a quantum gaussian state as a function of photon numbers? - Quantum Computing Stack Exchange.\nKindly look into it and I would update you if I find a paper which mentions this.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/25"}, "25": {"author": "MUSHKAN_SUREKA", "date": "1677607463111", "content": "a small proof of that can be:-\nn = a\u207aa = (q \u2011 ip)(q + ip) = q\u00b2 + i (qp \u2013 pq) + p\u00b2 = q\u00b2 + p\u00b2 \u2013 \u210f/2\nHence, for a single mode of 2\u00d72 covariance matrix V and displacement 0, \u27e8n\u27e9= \u27e8q\u00b2\u27e9 + \u27e8p\u00b2\u27e9 \u2013 \u210f/2 = Tr V \u2013 \u210f/2", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/26"}, "26": {"author": "MUSHKAN_SUREKA", "date": "1677618326835", "content": "\n\ndrive.google.com\n\n\n\n2013-phd-thesis-joachim-schaefer-v2-2.pdf 3\nGoogle Drive file.\n\n\n\n\n\nI uploaded a reference on the drive. Eq 5.9 on page 50 of the thesis should help you.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/27"}, "27": {"author": "CatalinaAlbornoz", "date": "1677860065169", "content": "Hi @MUSHKAN_SUREKA, we\u2019re taking a look at this. It is possible that the result is correct and indeed a large number of photons is needed, but since the paper was just showing a proof of concept this was not relevant for them.\nWe\u2019ll look into this and come back with a more definite answer.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/28"}, "28": {"author": "nathan", "date": "1677863449138", "content": "Hi @MUSHKAN_SUREKA, yes you are correct. The mean photon number (in the case of zero displacement) is given by the trace of the covariance matrix minus \u210f/2. Note that by default in SF we use \u210f=2.\nIf you have multiple modes, the formula still works, except you have to subtract out the vacuum energy of all modes: \\bar{n} = \\textit{Tr}(V) - N  \u210f/2\u00afn=Tr(V)\u2212N\u210f/2, where N is the number of modes.2 Replies1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/29"}, "29": {"author": "MUSHKAN_SUREKA", "date": "1677869937537", "content": "Hello Nathan, yeah however even after that the mean photon number is too high", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/30"}, "30": {"author": "MUSHKAN_SUREKA", "date": "1678043319759", "content": "Hello Nathan,\nhttps://arxiv.org/pdf/2301.01232.pdf 2 Appendix A section 2 might be useful. Could you please confirm if equation A7 is correct? Since it is nowhere mentioned in Xanadu/s graph similarity paper.\nThanks!", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/31"}, "31": {"author": "CatalinaAlbornoz", "date": "1678123716491", "content": "Hi @MUSHKAN_SUREKA,\nIf all of the formulas are correct and you still get that you need a lot of photons then maybe it means that you really need a lot of photons.\nI think the latest paper that you mentioned was written by @Amanuel. @Amanuel, do you have any thoughts you could share with @MUSHKAN_SUREKA?", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/32"}, "32": {"author": "Amanuel", "date": "1678125196501", "content": "Thanks for tagging me @CatalinaAlbornoz, and thank you @MUSHKAN_SUREKA for the interest in our paper. Formula A7 is actually taken from a paper written by some Xanadu folks 3. It is Eq. 14. That paper along with this one 3 were the most helpful when learning about the encoding of a graph into a GBS device. The latter I believe is where the detailed theory was first developed. This paper 2 may also be of interest.1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/33"}, "33": {"author": "MUSHKAN_SUREKA", "date": "1678157754052", "content": "Thank you Amanuel! Unfortunately, I never came across the detailed paper you mentioned. It seems like that direct formula is correct. @CatalinaAlbornoz yeah I guess practically, one does need a lot of photons for the actual embedding. I was just wondering if that is possible or not on hardware.\nThank you for your help and apologies for constantly pinging you haha.", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/34"}, "34": {"author": "CatalinaAlbornoz", "date": "1678301968681", "content": "Hi @Amanuel, thank you for your response!\n@MUSHKAN_SUREKA, the paper on Gaussian Boson Sampling for perfect matchings of arbitrary graphs 1 is the one where Amanuel mentions he found a lot of details. Given the large number of photons needed it would seem that it\u2019s not possible to run this on current hardware. Thank you for your questions! There\u2019s no need to apologize for them. I\u2019m glad we can help.1", "link": "https://discuss.pennylane.ai//t/about-graph-similarity-paper/1987/35"}}