{"0": {"author": "imakash", "date": "1692196123193", "content": "Hello I am working on a quantum-classical model which has got the following structure : [Classical] \u2192 [QVC] . Following is the minimal working code that is representative of my actual model.\nimport math\nimport pennylane as qml\nimport torch\nimport torch.nn as nn\nimport numpy as np\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev, interface = 'torch')\ndef simple_qubit_circuit(inputs, theta):\n    qml.RX(inputs, wires=0)\n    qml.RY(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\nclass QNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        quantum_weights = np.random.normal(0, np.pi)\n        self.quantum_weights = nn.parameter.Parameter(torch.tensor(quantum_weights,\\\n                                    dtype=torch.float32,requires_grad=True))\n        shapes = {\n            \"theta\": 1\n        }\n        self.q = qml.qnn.TorchLayer(simple_qubit_circuit, shapes)\n    \n    def forward(self, input_value):\n        return self.q(input_value)\n\nclass Q_PPO1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.shared_network = nn.Linear(5, 1)\n        self.value_network = QNet()\n\n    def forward(self, obs):\n        features = self.shared_network(obs).squeeze(1).to(torch.float64)\n        value = self.value_network(features)\n        return value\n\nPPO_model = Q_PPO1()\nx_train = torch.rand(10, 5) # Batch of 10 input vectors of dimension 5\ny_train = torch.rand(10, 1)\ny_out = PPO_model(x_train)\nloss = (y_out - y_train).mean()\nloss.backward()\n\nloss.backward() returns the following error message.\n\nRuntimeError: Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [1]\n\nNote that in the forward() method I had to convert features vector to torch64 datatype. If I don\u2019t do this, I get the following error in forward propagation.\nValueError: probabilities do not sum to 1\n\n\nI am not sure what\u2019s wrong here. Following are the details of qml.info()\nName: PennyLane\nVersion: 0.31.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/PennyLaneAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: c:\\users\\aksi01\\appdata\\roaming\\python\\python38\\site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml, typing-extensions\nRequired-by: PennyLane-Lightning, PennyLane-qiskit\nPlatform info:           Windows-10-10.0.19045-SP0\nPython version:          3.8.15\nNumpy version:           1.23.5\nScipy version:           1.10.1\nInstalled devices:\n- default.gaussian (PennyLane-0.31.1)\n- default.mixed (PennyLane-0.31.1)\n- default.qubit (PennyLane-0.31.1)\n- default.qubit.autograd (PennyLane-0.31.1)\n- default.qubit.jax (PennyLane-0.31.1)\n- default.qubit.tf (PennyLane-0.31.1)\n- default.qubit.torch (PennyLane-0.31.1)\n- default.qutrit (PennyLane-0.31.1)\n- null.qubit (PennyLane-0.31.1)\n- lightning.qubit (PennyLane-Lightning-0.31.0)\n- qiskit.aer (PennyLane-qiskit-0.29.0)\n- qiskit.basicaer (PennyLane-qiskit-0.29.0)\n- qiskit.ibmq (PennyLane-qiskit-0.29.0)\n- qiskit.ibmq.circuit_runner (PennyLane-qiskit-0.29.0)\n- qiskit.ibmq.sampler (PennyLane-qiskit-0.29.0)\n\n", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1692303986310", "content": "Hi @imakash, thank you for your question!\nI ran the code just as you shared it and I don\u2019t get any errors. I\u2019m running on Python 3.10.12\nWould you be able to create a new virtual environment with Python 3.10 and try again to see if this might be causing the issue? Let me know if you need guidance on how to create an environment. Alternatively you could also use Google Colab.\nLet me know if this solves your issue!", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/2"}, "2": {"author": "imakash", "date": "1692305107801", "content": "Hello @CatalinaAlbornoz ,\nThanks for your response.\nI can try out a new environment with an upgraded Python version. The only problem is that I am currently using Pennylane with other libraries that support only Python 3.8.\nIf we could make this work for Python v3.8, it would save me a lot of effort.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/3"}, "3": {"author": "CatalinaAlbornoz", "date": "1692368897398", "content": "Hi @imakash,\nUnfortunately our next release of PennyLane (v0.32) will no longer support Python 3.8 so you will need to upgrade your Python version to use the newest PennyLane features. You can still choose to stay with an older version of PennyLane if this is the only option that works for you, but I would strongly suggest updating your Python version if you can.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/4"}, "4": {"author": "imakash", "date": "1692378139602", "content": "Hello @CatalinaAlbornoz ,\nI am using CARLA which has their latest version (0.9.14) compatible only with Python versions 3.7 and 3.8.  As a result, I can\u2019t upgrade my Python version until they upgrade their build. It would very helpful of you if you could suggest me a fix to make the code work with Python 3.8.1 Reply", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/5"}, "5": {"author": "imakash", "date": "1692379564084", "content": "I have another question. In the circuit specified in this question, I am wondering how are the gradient for the parameters of classical layer is calculated if the diff_method is set as \u201cparamter_shift\u201d for the Qnode.\n@qml.qnode(dev, interface='torch', diff_method=\"parameter-shift\")\n\nDoes the gradient calculation for the classical layer parameters happen through parameter shift rule?1 Reply", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1692390726040", "content": "I understand @imakash.\nI just ran your code above with Python 3.8 and the latest PennyLane version and I don\u2019t get any errors so the problem wasn\u2019t the Python version after all. I would recommend that you create a new Python environment and install only the needed packages there to avoid any conflicts.\nIn the future it may become tricky to provide you with support since you will need to keep using the last PennyLane version that supports Python 3.8, which would be PennyLane v0.31.1. However you can follow the following tips to try to find a solution:\n\nDecouple your programs as much as possible so that it\u2019s easier to debug (like the example code you sent before).\nIf you need our support please provide the full context, a minimal but self-contained version of your code that can allow us to replicate the error, and the full error traceback. This makes it easier to uncover the source of the error.\nStart with simple examples and use print statements to verify that you\u2019re getting the expected outputs. For example, your error message says that you\u2019ve probably got a dimension mismatch somewhere so you can verify this by printing the outputs you\u2019re getting.\n\nI hope this can help you move forward. Please let me know if you have any other questions.1 Reply", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/7"}, "7": {"author": "CatalinaAlbornoz", "date": "1692392950520", "content": "Regarding your question on gradients @imakash,\nPennyLane will calculate the gradients for the quantum part (QNode) using the method you specify in diff_method. We give Torch the vjps for the quantum component, and Torch handles tracking the classical component and combining the two together.\nBasically we give gradients to Torch in a format that it can accept, and Torch combines it with its own backprop.\nLet me know if you have any further questions!1", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/8"}, "8": {"author": "imakash", "date": "1692437553351", "content": "Hello @CatalinaAlbornoz ,\nThank you for your responses. Can you please explain what is \u201cvjps\u201d? Also, if possible, can you please share references to help me understand how \u201cparameter shift\u201d works in Hybrid QVC?1 Reply", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/9"}, "9": {"author": "imakash", "date": "1692448026665", "content": "I have already tried out the third step. There is no problem with forward propagation. The problem is caused by backward propagation, which I cannot debug  . Anyway, thanks for your help.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/10"}, "10": {"author": "kevinkawchak", "date": "1692494592705", "content": "Hello @imakash,\nVjps could refer to:\nhttps://pytorch.org/functorch/nightly/generated/functorch.vjp.html 5\nThis document has a good explanation of parameter shift rule: Parameter-shift rules \u2014 PennyLane 1\nrequires_grad=True is for backward passes of the optimization process (OpenAI), and can not work correctly if there is an issue with other parts of the code. The Quanvolutional Neural Networks and Quantum transfer learning Demos have examples of requires_grad being used.1", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/11"}, "11": {"author": "isaacdevlugt", "date": "1692649065611", "content": "\n\n\n imakash:\n\nCan you please explain what is \u201cvjps\u201d? Also, if possible, can you please share references to help me understand how \u201cparameter shift\u201d works in Hybrid QVC\n\n\nHey @imakash! VJP = vector-jacobian product. I think this article in the Jax documentation does a great job of explaining things: The Autodiff Cookbook \u2014 JAX documentation 1\nFor parameter-shift rule information, we have a number of resources. However, I recommend checkout out our YT video: https://youtu.be/CRafKy6wsbY 3 (there are two parts).\nLet us know if that helps!1 Reply2", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/12"}, "12": {"author": "CatalinaAlbornoz", "date": "1692654230648", "content": "You\u2019ve got great answers @imakash!\nRegarding your original question, since your code works for me with Python v3.8 and PennyLane v0.31.1 then I would still encourage you to run this section of the code until you manage to get it working in your system.\nLet us know if you manage to make it work.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/13"}, "13": {"author": "imakash", "date": "1695393358745", "content": "Hey @isaacdevlugt @CatalinaAlbornoz ,\nI have a fundamental question regarding the training of a hybrid quantum classical model. Imagine there are two cases. In case 1, I train the entire model using backpropagation, and in case 2, I use the parameter-shift rule to calculate the gradients of the parameters of the QVC while the gradient for the rest of the model is calculated using backpropagation. If the weight initialization is done using the same seed, should I expect to see an identical loss curve for both cases?", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/14"}, "14": {"author": "kevinkawchak", "date": "1695401734413", "content": "Hello @imakash,\nThe \u2018Quantum gradients with backpropagation\u2019 demo shows different outputs of the same circuit for backpropagation vs. parameter-shift rule examples seen in \u2018print(circuit(params))\u2019. The loss curves would be different as the way circuits are calculated are different, I believe.\nReference:\n\nQuantum gradients with backpropagation | PennyLane Demos 1\n", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/15"}, "15": {"author": "imakash", "date": "1695409739201", "content": "Hello @kevinkawchak ,\nThanks for your response. I understand that the methods are different. However, if the loss curve is different then it means that the value of gradients calculated by the two approaches are different because everything else in the two cases is the same. I don\u2019t understand why should this be the case.\nAccording to the this paper and also the paper by Dr. Schuld on the parameter shift rule, the parameter-shift rule gives the exact gradient of the QVC if the ansatz is composed of Rx, Ry, and Rz gates(basically gates represented by unitaries having just 2 eigenvalues). I believe, gradient calculation by backpropagation also gives the exact value and doesn\u2019t make any approximation.\nTo conclude, my concern is why should the gradient values be different in the two approaches.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/16"}, "16": {"author": "CatalinaAlbornoz", "date": "1695698016074", "content": "Hi @imakash, the parameter-shift rule indeed gives exact gradients. Did you find a case where this gives you a different result from backpropagation? In the demo on Quantum gradients with backpropagation 1 the graphs indicate a comparison in time, not loss.\nPlease let me know if this answers your question!", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/17"}, "17": {"author": "imakash", "date": "1695715833622", "content": "Hello @CatalinaAlbornoz ,\nIn my work, I tried backpropagation as well as the parameter shift rule for gradient calculation and I got different results even for the same seed value (weight initialization). Therefore, I asked. My question is not related to the demo.\nLooking at the curves, I think that parameter shift is suffering from the overshooting problem due to a large step size; a similar problem that exists with naive gradient descent without using Adam/RMS optimizer. Therefore, I think that even though the gradients are the same, probably, optimizers are probably different in both cases (Even though I explicitly define the optimizer as Adam optimizer in both cases). I am not sure though about my hypothesis and therefore would need someone from Pennylane to comment on this.", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/18"}, "18": {"author": "CatalinaAlbornoz", "date": "1695817759400", "content": "Hi @imakash, if you define the optimizer as Adam then that\u2019s the optimizer being used. If you share a minimal but self-contained version of your code showing the different gradient for parameter-shift and backprop then I can try to replicate your error. Here\u2019s a small example showing that the gradients are indeed the same:\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev, interface = 'torch', diff_method='backprop')\ndef simple_qubit_circuit(inputs, theta):\n    qml.RX(inputs, wires=0)\n    qml.RY(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ninputs = torch.tensor(0.5, requires_grad=False)\ntheta = torch.tensor(0.2, requires_grad=True)\nresult = simple_qubit_circuit(inputs, theta)\n\nprint(result)\n\nresult.backward()\nprint(theta.grad)\n\n@qml.qnode(dev, interface = 'torch', diff_method='parameter-shift')\ndef simple_qubit_circuit(inputs, theta):\n    qml.RX(inputs, wires=0)\n    qml.RY(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ninputs = torch.tensor(0.5, requires_grad=False)\ntheta = torch.tensor(0.2, requires_grad=True)\nresult = simple_qubit_circuit(inputs, theta)\n\nprint(result)\n\nresult.backward()\nprint(theta.grad)\n\nMy guess is that in your code the random parameters are being initialized differently even though you have a seed, or something is being coded differently in the optimization, or the comparison you\u2019re doing is not correct. You can learn more about using the Torch interface here.\nI hope this helps!", "link": "https://discuss.pennylane.ai//t/issues-with-backpropagation-when-using-parameter-broadcasting-with-pytorch/3333/19"}}