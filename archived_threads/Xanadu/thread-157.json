{"0": {"author": "wojtek", "date": "1573677970082", "content": "Hello,\nThe tutorial:\nhttps://pennylane.ai/qml/tutorial/tutorial_advanced_usage.html 1\nmentions the difference between positional and keyword arguments i.e. output will not be differentiated with respect to the keyword arguments.\nDoes the same pattern hold for pytorch interface or is the requires_grad used instead? I.e. if I pass into my quantum circuit some tensors which do not require grad (e.g. the \u2018data\u2019) and some tensors which do require grad (learnable parameters) will pennylane figure it out to only autodifferentiate with respect to inputs which have requires_grad=True? Or do I need to apply the same pattern of positional and keyword arguments?\nThanks,\nWojtek\n\n\n Solved by wojtek in post #2 \n\n\n                Answering myself: \nThe pattern of positional and keyword arguments does seem to matter. \nHere is a module (based on the qbit rotation tutorial) encapsulating a qnode with two (dummy) arguments and two learnable parameters: \nfrom torch.nn.parameter import Parameter\n\n@qml.qnode(qpu,interface='torch')\n\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/pytorch-interface-and-positional-vs-keyword-arguments/266/1"}, "1": {"author": "wojtek", "date": "1573690415077", "content": "Answering myself:\nThe pattern of positional and keyword arguments does seem to matter.\nHere is a module (based on the qbit rotation tutorial) encapsulating a qnode with two (dummy) arguments and two learnable parameters:\nfrom torch.nn.parameter import Parameter\n\n@qml.qnode(qpu,interface='torch')\ndef circuit(params,inputs=None):\n    qml.RX(inputs[0],wires=0)\n    qml.RZ(inputs[0],wires=0)\n    qml.RX(params[0],wires=0)\n    qml.RZ(params[0],wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\n\nclass QCircuitNet(torch.nn.Module):\n    def __init__(self):\n        super(QCircuitNet, self).__init__()\n        self.phi=Parameter(torch.tensor(0.0))\n        self.theta=Parameter(torch.tensor(0.0))\n    \n    def forward(self,alpha,beta):\n        return circuit(alpha,beta,self.phi,self.theta)\n\n\n\nmodel=QCircuitNet()\n\ndef cost(value,target):\n    return torch.abs(value-target)**2\n\nvalue=model(alpha,beta)\n#this results in 1 job (about 4.5 for 100 shots on ibmQ)\n\nloss=cost(value,target)\n\nloss.backward()\n#this results in 8 jobs\n\nSo the backward call in the above took 8 jobs sent to the backend\nIf one looks the gradients of the inputs are all None as expected though\nNow if we re-write using positional arguments for the learnable parameters and keyword arguments for inputs/arguments:\n@qml.qnode(qpu,interface='torch')\ndef circuit(params,inputs=None):\n    qml.RX(inputs[0],wires=0)\n    qml.RZ(inputs[0],wires=0)\n    qml.RX(params[0],wires=0)\n    qml.RZ(params[0],wires=0)\n    return qml.expval(qml.PauliZ(0))\n\nclass QCircuitNet(torch.nn.Module):\n    def __init__(self):\n        super(QCircuitNet, self).__init__()\n        self.params=Parameter(torch.zeros(2,dtype=torch.float32))\n    \n    def forward(self,args):\n        return circuit(self.params,inputs=args)\n\nThe call to forward still takes 1 job, but call to backward takes only 4 evaluations. So one is definitely better off using the pattern of positional and keyword arguments. Not sure if this would be considered a bug as it seems the correct behaviour could be achieved by checking the requires_grad field. BTW this is using release 0.6.1.\nLet me know if it seems I\u2019m doing something wrong.\nCheers,\nWojtekSolution1", "link": "https://discuss.pennylane.ai//t/pytorch-interface-and-positional-vs-keyword-arguments/266/2"}, "2": {"author": "josh", "date": "1573705373049", "content": "@wojtek, thanks for you question (and solution!)\nYou\u2019re right, currently the  positional/trainable, keyword/non-trainable split is inherited by the PyTorch and TensorFlow interfaces for historical reasons. Namely, the PennyLane QNode was designed with just Autograd in mind (which does not have a way of marking variables), and the PyTorch and TensorFlow interfaces were added later, on top.\nWe are currently in the process of refactoring the QNode, and as a part of this, we are moving the autograd dependent code out of the QNode and into its own interface. So this should be fixed soon ", "link": "https://discuss.pennylane.ai//t/pytorch-interface-and-positional-vs-keyword-arguments/266/3"}, "3": {"author": "wojtek", "date": "1573677970082", "content": "Hello,\nThe tutorial:\nhttps://pennylane.ai/qml/tutorial/tutorial_advanced_usage.html 1\nmentions the difference between positional and keyword arguments i.e. output will not be differentiated with respect to the keyword arguments.\nDoes the same pattern hold for pytorch interface or is the requires_grad used instead? I.e. if I pass into my quantum circuit some tensors which do not require grad (e.g. the \u2018data\u2019) and some tensors which do require grad (learnable parameters) will pennylane figure it out to only autodifferentiate with respect to inputs which have requires_grad=True? Or do I need to apply the same pattern of positional and keyword arguments?\nThanks,\nWojtek\n\n\n Solved by wojtek in post #2 \n\n\n                Answering myself: \nThe pattern of positional and keyword arguments does seem to matter. \nHere is a module (based on the qbit rotation tutorial) encapsulating a qnode with two (dummy) arguments and two learnable parameters: \nfrom torch.nn.parameter import Parameter\n\n@qml.qnode(qpu,interface='torch')\n\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/pytorch-interface-and-positional-vs-keyword-arguments/266/4"}}