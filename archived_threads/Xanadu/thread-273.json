{"0": {"author": "kannan_v", "date": "1612960415267", "content": "Hi,\nI\u2019m trying to optimise classical fisher information from a gaussian system simulated using default.gaussian backend. I\u2019m  measuring the mean and variance and using that to generate the gaussian probability distribution classically which I then use to calculate the fisher information.\nThe problem arises when I try to optimise the circuit taking the Fisher Information matrix as the cost function. The qml.GradientDescentOptimizer just returns zero as the cost and doesn\u2019t update the parameters. It doesn\u2019t give an error or anything suggesting that the objective function is not differentiable.\nI also tried changing the inital values and learning rate.\nWhat could be the problem here?\nThanks in advance \n-Kannan", "link": "https://discuss.pennylane.ai//t/issue-with-gradient-descent-optimisation-on-cv-gaussian-system/839/1"}, "1": {"author": "jmarrazola", "date": "1612968945601", "content": "Hi @Kannan! Welcome to the Xanadu forum! \nIt\u2019s difficult for me pin down the issue without taking a look at your code. Could you please share it so we can better assist you?\nAs a general piece of advice, the Fisher information matrix is a notably difficult cost function to work with because it can\u2019t easily be expressed as an expectation value of a simple observable. As you noted, it requires taking gradients of individual outcome probabilities. It\u2019s probably wise to be extra careful in this case.\nIn the meantime, you may find this tutorial on quantum metrology helpful. It is described for qubit circuits, but it also makes use of the Fisher information matrix as the cost function.\nBest,\nJuan Miguel", "link": "https://discuss.pennylane.ai//t/issue-with-gradient-descent-optimisation-on-cv-gaussian-system/839/2"}, "2": {"author": "kannan_v", "date": "1613488957789", "content": "Hi @jmarrazola,\nThank you for the suggestion. I figured out the issue, but I thought I\u2019ll share it here so it may help others as well.\nThe problem was with a simple float point precision error. The gradient of the function was extremely small (of the order of 10^-30) compared to the inital step which was 0.1 and the learning rate was just 0.01.\nSo the step of the gradient descent optimization was going like:\n0.1 - (0.01*1e-30)\nwhich is computed to be 0.1 itself.\nSo I think this may be resolved by using an astronomical learnigng rate like 1e29 so that the step is comparable to initial state.\n\nKannan\n1", "link": "https://discuss.pennylane.ai//t/issue-with-gradient-descent-optimisation-on-cv-gaussian-system/839/3"}, "3": {"author": "jmarrazola", "date": "1613494209378", "content": "Hi @kannan_v,\nI\u2019m glad you identified that the gradient is extremely small. However, I\u2019m not sure that using an extremely large learning rate is the way to go. Vanishing gradients, also known as barren plateaus, are a common problem in the optimization of quantum circuits. Are you observing that the model trains properly when setting such a large learning rate?\nIdeally, your model should be such that the gradient is not vanishing. You may also want to experiment with changing the gates in your circuit, using different initial parameters, or even using gradient-free optimizers.\nBest,\nJuan Miguel1", "link": "https://discuss.pennylane.ai//t/issue-with-gradient-descent-optimisation-on-cv-gaussian-system/839/4"}, "4": {"author": "kannan_v", "date": "1612960415267", "content": "Hi,\nI\u2019m trying to optimise classical fisher information from a gaussian system simulated using default.gaussian backend. I\u2019m  measuring the mean and variance and using that to generate the gaussian probability distribution classically which I then use to calculate the fisher information.\nThe problem arises when I try to optimise the circuit taking the Fisher Information matrix as the cost function. The qml.GradientDescentOptimizer just returns zero as the cost and doesn\u2019t update the parameters. It doesn\u2019t give an error or anything suggesting that the objective function is not differentiable.\nI also tried changing the inital values and learning rate.\nWhat could be the problem here?\nThanks in advance \n-Kannan", "link": "https://discuss.pennylane.ai//t/issue-with-gradient-descent-optimisation-on-cv-gaussian-system/839/5"}}