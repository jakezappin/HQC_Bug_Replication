{"0": {"author": "Solomon", "date": "1686212418191", "content": "Hello,\nI am writing some code for processing images in a QNN, where each image is being fed through a full quantum NN.\nIn almost every PennyLane QC example, the quantum circuit returns a tuple of expectation values like so:\nexp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]    \nreturn tuple(exp_vals)                                                         \n\nwhich are then processed in a NN like so: q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0)\nHowever, after debugging for hours trying to understand the source of the following error in my code:\nAttributeError: 'tuple' object has no attribute 'float' \nI came to the realisation that either one of these modifications are required:\nOption 1:\nreturn qml.expval(Tensor(*[qml.PauliZ(i) for i in range(n_qubits)]))  e.g. adding a `Tensor\u2019 type.\nOption 2:\nInside the NN instead of q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0) use q_out_elem = self.pqc(elem, self.q_params)[0].float().unsqueeze(0)\nThese are very subtle differences but I am trying to understand why I need them in my code while every other example does not. Also, what is the fastest way to get the float value of the expectation value?\nThanks.\n[Python version]: 3.9.12 (main, Dec  2 2022, 15:48:07) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]\n[Deep Learning framework, Pytorch (Facebook) version]: 1.12.1\n[Quantum Machine Learning framework (Pennylane) version]: 0.30.0\n", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/1"}, "1": {"author": "Romain_Moyard", "date": "1686235244930", "content": "Hi @Solomon !\nYou can use the NN module from PennyLane for PyTorch check TorchLayer and for TF use KerasLayer. You can easily create quantum layers out of QNode, some post-processing like stacking multiple expectations values is applied automatically. qml.qnn.TorchLayer \u2014 PennyLane 0.30.0 documentation 3\nCould you tell me wher you have found this example using? q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0) it might be an outdate example/demo.\nLet me know if you have more questions!1 Reply", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/2"}, "2": {"author": "Romain_Moyard", "date": "1686236006274", "content": "I found the culprit I think: Quantum transfer learning \u2014 PennyLane documentation\nIt was updated recently to q_out_elem = torch.hstack(quantum_net(elem, self.q_params)).float().unsqueeze(0), so you can apply this change. But I strongly recommend to use the TorchLayer that I linked in my previous message. We will update this demo soon in order to make the use of the TorchLayer module.", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/3"}, "3": {"author": "Solomon", "date": "1686236376411", "content": "Hello Romain,\nThank you for your response. I was unable to find an example that specifically addresses my requirements of processing RGB images, encoding, and training the parameters of a PQC using a quantum circuit alone. Most existing examples tend to involve a classical feature extractor combined with a quantum FC layer, which is not what I am looking for.\nI have written a self-contained example for binary classification using the bees/ants dataset, but  facing an issue where the validation and training accuracy do not change. I believe this issue is not related to overfitting, but rather a problem in my code that I am unable to identify. I would appreciate it if you could take a look at my code and provide guidance on this and also how to address the related \u201cfloat\u201d tuple issue.\nHere is the full code, all you have to do is extract the bees/ants dataset into ./datasets and run the code.\n# Install necessary packages\n# !pip install torch==1.12.1 torchvision==0.13.1 pennylane==0.30.0 efficientnet_pytorch\n\n# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport os\nfrom tqdm import tqdm\nimport sys \nfrom pennylane.operation import Tensor\nimport psutil\nimport matplotlib.pyplot as plt\n\n# from qblocks.qgates import *\n\nprint(\"[Python version]:\", sys.version)\nprint(\"[Deep Learning framework, Pytorch (Facebook) version]:\", torch.__version__)\nprint(\"[Quantum Machine Learning framework (Pennylane) version]:\", qml.__version__)\n\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Set parameters\npatch_size = 3\nimg_size_single = 128\nimg_size_flat = img_size_single ** 2\nRGB_C = 1  # Number of channels in the image (RGB)\nbatch_size = 32\nnum_epochs = 60\nn_qubits = patch_size ** 2 * RGB_C\n\nn_layers = 6\nnum_classes = None\n\n# Note if you forget to wrap your circuit with dev: AttributeError: 'tuple' object has no attribute 'float' \ndev_train = qml.device('default.qubit', wires=n_qubits)   \n\n# Define the quantum circuit\n# @qml.qnode(dev_train, interface=\"torch\")\n\ndef Q_Plot(cirq_0, q_b,q_d):\n    # print(\"Plot Q:\", q_b)\n    fig, ax = qml.draw_mpl(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d))        \n    # print (qml.draw(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d)))\n    # plt.figure(figsize=(5,3))\n    # from pylab import rcParams\n    # rcParams['figure.figsize'] = 3, 6\n    # fig.set_size_inches(12,6)\n    plt.show()\n    fig.show()\n\n\ndef Q_count_parameters(qnn):\n    print(dict(qnn.named_parameters()))\n    for name, param in qnn.named_parameters():\n        param.requires_grad=True\n        # print (name, param.data)\n    return sum(p.numel() for p in qnn.parameters() if p.requires_grad)\n\nimport random\n\ndef circuit(inputs, weights):\n    # print('inputs / weights {}/{}'.format(inputs.shape, weights.shape))\n    for qub in range(n_qubits):\n        qml.Hadamard(wires=qub)\n        qml.RY(inputs[qub], wires=qub)\n\n    for l in range(n_layers):\n        qubit_indices = list(range(n_qubits))\n        random.shuffle(qubit_indices)  # Shuffle the qubit indices to create random pairs\n\n        for i in range(0, n_qubits, 2):\n            control_qubit = qubit_indices[i]\n            target_qubit = qubit_indices[(i + 1) % n_qubits]\n\n            # Apply CRZ gate with conditional RY gate\n            random_num = random.uniform(0, 1)\n            qml.CRZ(weights[control_qubit], wires=[control_qubit, target_qubit])\n            qml.RY(random_num * weights[control_qubit], wires=control_qubit)\n            qml.CNOT(wires=[control_qubit, target_qubit])\n            qml.CZ(wires=[control_qubit, (control_qubit + 2) % n_qubits])  # Additional CZ gate for entanglement\n\n    return qml.expval(Tensor(*[qml.PauliZ(i) for i in range(n_qubits)]))\n\n\n# Define the Quanvolutional Neural Network\nclass QuanvolutionalNeuralNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(n_qubits, num_classes)\n        self.q_params = nn.Parameter(torch.Tensor(n_qubits, n_qubits))\n        self.lr1 = nn.LeakyReLU(0.1)\n        nn.init.xavier_uniform_(self.q_params)\n        self.pqc = qml.QNode(circuit, dev_train, interface = 'torch')\n        Q_Plot(circuit,n_qubits,n_qubits)\n\n    \n    def extract_patches(self, x):\n        patches = []\n        bs, c, w, h = x.size()\n        for i in range(w - patch_size + 1):\n            for j in range(h - patch_size + 1):\n                patch = x[:, :, i:i+patch_size, j:j+patch_size]\n                patches.append(patch)\n        patches = torch.stack(patches, dim=1).view(bs, -1, c * patch_size * patch_size)\n        return patches\n\n    def forward(self, x):\n        assert len(x.shape) == 4  # (bs, c, w, h)\n        bs = x.shape[0]  # batch_size = x.size(0)\n        c = x.shape[1]  # RGB\n        x = x.view(bs, c, img_size_single, img_size_single)\n        q_in = self.extract_patches(x)\n        q_in = q_in.to(device)\n        # print (q_in.shape)\n        # q_out = torch.Tensor(0, n_qubits)\n        q_out = torch.Tensor(0, n_qubits)\n\n        q_out = q_out.to(device)\n        for elem in q_in:\n            # print (elem.shape)\n            # print (self.q_params.shape)\n            q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))        \n        x = self.lr1(q_out.view(-1, n_qubits))\n        x = self.fc1(x)\n\n        return x\n\n\n# Set the data directory and transformations\ndata_dir = 'datasets/hymenoptera/'\n# import splitfolders as sf\n# sf.ratio('datasets/mri/train', 'output', ratio=(0.65, 0.05, 0.3), seed=42)\ndata_transforms = {\n    'train': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n    'val': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n}\n\n\n# Load the image datasets\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\nnum_classes=len(class_names)\n\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n\n\n# Initialize the Quanvolutional Neural Network\nqnn = QuanvolutionalNeuralNetwork(num_classes=num_classes)\nqnn = qnn.to(device)\n# print (\"Total trainable params:\",Q_count_parameters(qnn))\n\n# Set the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(qnn.parameters(), lr=0.001)\n\n# Train the model\nfor epoch in tqdm(range(num_epochs)):\n    # print(\"Epoch {}/{}, Qubits:{}\".format(epoch, num_epochs, n_qubits))\n    cpu_percent = psutil.cpu_percent()\n    mem_usage= psutil.virtual_memory().total / (1024 ** 3)\n    used_ram_gb = psutil.virtual_memory().used / (1024 ** 3)    \n    # print(f\"Epoch{epoch+1}/{num_epochs}, Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C}, IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]},CPU:{cpu_percent},RAM(GB):{used_ram_gb}'/'{mem_usage}\")\n    print(f\"Epoch:[{epoch+1}/{num_epochs}], Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C},IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]} CPU:{cpu_percent}, RAM(GB):{used_ram_gb}/{mem_usage}\")\n    \n    qnn.train()\n    running_loss = 0\n    running_corrects = 0\n    total_samples = 0\n\n    for batch_idx, (data, target) in tqdm(enumerate(dataloaders['train'])):\n        data = data.to(device)\n        target = target.view(-1).to(device)\n        batch_size = data.size(0)  # Get the actual batch size\n\n        optimizer.zero_grad()\n        output = qnn(data)\n\n        # Adjust the output tensor size if necessary\n        if output.size(0) > batch_size:\n            output = output[:batch_size]\n\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(output, 1)\n        running_corrects += torch.sum(predicted == target.data)\n        total_samples += batch_size\n\n    batch_loss = running_loss / len(dataloaders['train'])\n    batch_acc = running_corrects / total_samples\n    print(f'[{epoch + 1}] Training Loss: {batch_loss:.3f}, Training Accuracy: {batch_acc:.3f}')\n\n    running_loss = 0.0\n    running_corrects = 0\n    total_samples = 0\n\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for val_data, val_target in dataloaders['val']:\n            val_data = val_data.to(device)\n            val_target = val_target.to(device)\n            batch_size = val_data.size(0)  # Get the actual batch size\n\n            val_output = qnn(val_data)\n            _, val_predicted = torch.max(val_output.data, 1)\n\n            # Adjust the output and target tensors if necessary\n            if val_predicted.size(0) > batch_size:\n                val_predicted = val_predicted.narrow(0, 0, batch_size)\n                val_target = val_target.narrow(0, 0, batch_size)\n\n            val_total += batch_size\n            val_correct += (val_predicted == val_target).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"[{epoch + 1}] Validation Accuracy: {val_accuracy:.2f}%\")\n\n\nThanks.", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/4"}, "4": {"author": "Romain_Moyard", "date": "1686236520162", "content": "For your example, you remove the Tensor from the measurements and use\nq_out_elem = torch.hstack(self.pqc(elem, self.q_params)).float().unsqueeze(0\n\nLet me know if that helps ", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/5"}, "5": {"author": "Solomon", "date": "1686236831340", "content": "Thank you for your input. I understand that the suggestion you provided is similar to what I have already tried and does not alter the functionality. However, it does not address the larger and more critical issue that I have reported. I would greatly appreciate further assistance in resolving the main problem I am facing.", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/6"}, "6": {"author": "Romain_Moyard", "date": "1686237055049", "content": "Thanks for your feedback, could you create a minimal example that is not training and post your issue on GitHub? Issues \u00b7 PennyLaneAI/pennylane \u00b7 GitHub 2\nThanks!", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/7"}, "7": {"author": "Solomon", "date": "1686242994316", "content": "Here you go:\n\ngithub.com/PennyLaneAI/pennylane\n\n\n\n\n\n\n\n\nIssues with training of a QNN for the binary classification of images   9\n\n\n\n        opened \n\n\n\nJun 8, 2023\n\n\n\n\n\n          BoltzmannEntropy\n        \n\n\n\n\n          enhancement \n\n\n\n\n\n### Feature details\n\nDear team,\nThis is a follow up from here:\nhttps://discuss\u2026.pennylane.ai/t/expectation-values-and-tensors-in-qnn/3024/7 \n\nI was unable to find an example that specifically addresses my requirements of processing RGB images, encoding, and training the parameters of a PQC using a quantum circuit alone. Most existing examples tend to involve a classical feature extractor combined with training of only a quantum FC layer, which is not what I am looking for.\n\nI have written a self-contained example for binary classification using the bees/ants dataset, but facing an issue **where the validation and training accuracy do not change.** I believe this issue is not related to overfitting, but rather a problem in my code that I am unable to identify. \n\nI would appreciate it if you could take a look at my code and provide guidance on this.\n\n### Implementation\n\nHere is the code to reproduce the problem:\n```\n# Install necessary packages\n# !pip install torch==1.12.1 torchvision==0.13.1 pennylane==0.30.0 efficientnet_pytorch\n\n# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport os\nfrom tqdm import tqdm\nimport sys \nfrom pennylane.operation import Tensor\nimport psutil\nimport matplotlib.pyplot as plt\n\n# from qblocks.qgates import *\n\nprint(\"[Python version]:\", sys.version)\nprint(\"[Deep Learning framework, Pytorch (Facebook) version]:\", torch.__version__)\nprint(\"[Quantum Machine Learning framework (Pennylane) version]:\", qml.__version__)\n\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Set parameters\npatch_size = 2\nimg_size_single = 128\nimg_size_flat = img_size_single ** 2\nRGB_C = 3  # Number of channels in the image (RGB)\nbatch_size = 32\nnum_epochs = 60\nn_qubits = patch_size ** 2 * RGB_C\n\nn_layers = 6\nnum_classes = None\n\n# Note if you forget to wrap your circuit with dev: AttributeError: 'tuple' object has no attribute 'float' \ndev_train = qml.device('default.qubit', wires=n_qubits)   \n\n# Define the quantum circuit\n# @qml.qnode(dev_train, interface=\"torch\")\n\ndef Q_Plot(cirq_0, q_b,q_d):\n    # print(\"Plot Q:\", q_b)\n    fig, ax = qml.draw_mpl(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d))        \n    # print (qml.draw(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d)))\n    # plt.figure(figsize=(5,3))\n    # from pylab import rcParams\n    # rcParams['figure.figsize'] = 3, 6\n    # fig.set_size_inches(12,6)\n    plt.show()\n    fig.show()\n\n\ndef Q_count_parameters(qnn):\n    print(dict(qnn.named_parameters()))\n    for name, param in qnn.named_parameters():\n        param.requires_grad=True\n        # print (name, param.data)\n    return sum(p.numel() for p in qnn.parameters() if p.requires_grad)\n\nimport random\n\ndef circuit(inputs, weights):\n    # print('inputs / weights {}/{}'.format(inputs.shape, weights.shape))\n    for qub in range(n_qubits):\n        qml.Hadamard(wires=qub)\n        qml.RY(inputs[qub], wires=qub)\n\n    for l in range(n_layers):\n        qubit_indices = list(range(n_qubits))\n        random.shuffle(qubit_indices)  # Shuffle the qubit indices to create random pairs\n\n        for i in range(0, n_qubits, 2):\n            control_qubit = qubit_indices[i]\n            target_qubit = qubit_indices[(i + 1) % n_qubits]\n\n            # Apply CRZ gate with conditional RY gate\n            random_num = random.uniform(0, 1)\n            qml.CRZ(weights[control_qubit], wires=[control_qubit, target_qubit])\n            qml.RY(random_num * weights[control_qubit], wires=control_qubit)\n            qml.CNOT(wires=[control_qubit, target_qubit])\n            qml.CZ(wires=[control_qubit, (control_qubit + 2) % n_qubits])  # Additional CZ gate for entanglement\n\n    return qml.expval(Tensor(*[qml.PauliZ(i) for i in range(n_qubits)]))\n\n\n# Define the Quanvolutional Neural Network\nclass QuanvolutionalNeuralNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(n_qubits, num_classes)\n        self.q_params = nn.Parameter(torch.Tensor(n_qubits, n_qubits))\n        self.lr1 = nn.LeakyReLU(0.1)\n        nn.init.xavier_uniform_(self.q_params)\n        self.pqc = qml.QNode(circuit, dev_train, interface = 'torch')\n        Q_Plot(circuit,n_qubits,n_qubits)\n\n    \n    def extract_patches(self, x):\n        patches = []\n        bs, c, w, h = x.size()\n        for i in range(w - patch_size + 1):\n            for j in range(h - patch_size + 1):\n                patch = x[:, :, i:i+patch_size, j:j+patch_size]\n                patches.append(patch)\n        patches = torch.stack(patches, dim=1).view(bs, -1, c * patch_size * patch_size)\n        return patches\n\n    def forward(self, x):\n        assert len(x.shape) == 4  # (bs, c, w, h)\n        bs = x.shape[0]  # batch_size = x.size(0)\n        c = x.shape[1]  # RGB\n        x = x.view(bs, c, img_size_single, img_size_single)\n        q_in = self.extract_patches(x)\n        q_in = q_in.to(device)\n        # print (q_in.shape)\n        # q_out = torch.Tensor(0, n_qubits)\n        q_out = torch.Tensor(0, n_qubits)\n\n        q_out = q_out.to(device)\n        for elem in q_in:\n            # print (elem.shape)\n            # print (self.q_params.shape)\n            q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))        \n        x = self.lr1(q_out.view(-1, n_qubits))\n        x = self.fc1(x)\n\n        return x\n\n\n\n# Set the data directory and transformations\ndata_dir = 'datasets/hymenoptera/'\n# import splitfolders as sf\n# sf.ratio('datasets/mri/train', 'output', ratio=(0.65, 0.05, 0.3), seed=42)\ndata_transforms = {\n    'train': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n    'val': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n}\n\n\n# Load the image datasets\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\nnum_classes=len(class_names)\n\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n\n\n# Initialize the Quanvolutional Neural Network\nqnn = QuanvolutionalNeuralNetwork(num_classes=num_classes)\nqnn = qnn.to(device)\n# print (\"Total trainable params:\",Q_count_parameters(qnn))\n\n# Set the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(qnn.parameters(), lr=0.001)\n\n# Train the model\nfor epoch in tqdm(range(num_epochs)):\n    # print(\"Epoch {}/{}, Qubits:{}\".format(epoch, num_epochs, n_qubits))\n    cpu_percent = psutil.cpu_percent()\n    mem_usage= psutil.virtual_memory().total / (1024 ** 3)\n    used_ram_gb = psutil.virtual_memory().used / (1024 ** 3)    \n    # print(f\"Epoch{epoch+1}/{num_epochs}, Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C}, IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]},CPU:{cpu_percent},RAM(GB):{used_ram_gb}'/'{mem_usage}\")\n    print(f\"Epoch:[{epoch+1}/{num_epochs}], Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C},IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]} CPU:{cpu_percent}, RAM(GB):{used_ram_gb}/{mem_usage}\")\n    \n    qnn.train()\n    running_loss = 0\n    running_corrects = 0\n    total_samples = 0\n\n    for batch_idx, (data, target) in tqdm(enumerate(dataloaders['train'])):\n        data = data.to(device)\n        target = target.view(-1).to(device)\n        batch_size = data.size(0)  # Get the actual batch size\n\n        optimizer.zero_grad()\n        output = qnn(data)\n\n        # Adjust the output tensor size if necessary\n        if output.size(0) > batch_size:\n            output = output[:batch_size]\n\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(output, 1)\n        running_corrects += torch.sum(predicted == target.data)\n        total_samples += batch_size\n\n    batch_loss = running_loss / len(dataloaders['train'])\n    batch_acc = running_corrects / total_samples\n    print(f'[{epoch + 1}] Training Loss: {batch_loss:.3f}, Training Accuracy: {batch_acc:.3f}')\n\n    running_loss = 0.0\n    running_corrects = 0\n    total_samples = 0\n\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for val_data, val_target in dataloaders['val']:\n            val_data = val_data.to(device)\n            val_target = val_target.to(device)\n            batch_size = val_data.size(0)  # Get the actual batch size\n\n            val_output = qnn(val_data)\n            _, val_predicted = torch.max(val_output.data, 1)\n\n            # Adjust the output and target tensors if necessary\n            if val_predicted.size(0) > batch_size:\n                val_predicted = val_predicted.narrow(0, 0, batch_size)\n                val_target = val_target.narrow(0, 0, batch_size)\n\n            val_total += batch_size\n            val_correct += (val_predicted == val_target).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"[{epoch + 1}] Validation Accuracy: {val_accuracy:.2f}%\")\n\n```\n\n### How important would you say this feature is?\n\n2: Somewhat important. Needed this quarter.\n\n### Additional information\n\nHere is the log:\n```\n[1] Training Loss: 0.705, Training Accuracy: 0.507\n  2%|\u258f         | 1[/60](https://file+.vscode-resource.vscode-cdn.net/60) [01:06<1:05:21, 66.46s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[1] Validation Accuracy: 45.00%\nEpoch:[2[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:29.8, RAM(GB):15.030136108398438[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:55,  6.93s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[2] Training Loss: 0.688, Training Accuracy: 0.507\n  3%|\u258e         | 2[/60](https://file+.vscode-resource.vscode-cdn.net/60) [02:11<1:03:33, 65.75s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[2] Validation Accuracy: 45.00%\nEpoch:[3[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:32.5, RAM(GB):14.813690185546875[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:55,  6.91s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[3] Training Loss: 0.686, Training Accuracy: 0.507\n  5%|\u258c         | 3[/60](https://file+.vscode-resource.vscode-cdn.net/60) [03:17<1:02:31, 65.81s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[3] Validation Accuracy: 45.00%\nEpoch:[4[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:29.7, RAM(GB):15.015106201171875[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:58,  7.37s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[4] Training Loss: 0.686, Training Accuracy: 0.507\n  7%|\u258b         | 4[/60](https://file+.vscode-resource.vscode-cdn.net/60) [04:27<1:03:00, 67.51s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[4] Validation Accuracy: 45.00%\n```\n\n\n\n\n\n\n", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1686345742478", "content": "Thanks for posting the issue there @Solomon!", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/9"}, "9": {"author": "Solomon", "date": "1686212418191", "content": "Hello,\nI am writing some code for processing images in a QNN, where each image is being fed through a full quantum NN.\nIn almost every PennyLane QC example, the quantum circuit returns a tuple of expectation values like so:\nexp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]    \nreturn tuple(exp_vals)                                                         \n\nwhich are then processed in a NN like so: q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0)\nHowever, after debugging for hours trying to understand the source of the following error in my code:\nAttributeError: 'tuple' object has no attribute 'float' \nI came to the realisation that either one of these modifications are required:\nOption 1:\nreturn qml.expval(Tensor(*[qml.PauliZ(i) for i in range(n_qubits)]))  e.g. adding a `Tensor\u2019 type.\nOption 2:\nInside the NN instead of q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0) use q_out_elem = self.pqc(elem, self.q_params)[0].float().unsqueeze(0)\nThese are very subtle differences but I am trying to understand why I need them in my code while every other example does not. Also, what is the fastest way to get the float value of the expectation value?\nThanks.\n[Python version]: 3.9.12 (main, Dec  2 2022, 15:48:07) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]\n[Deep Learning framework, Pytorch (Facebook) version]: 1.12.1\n[Quantum Machine Learning framework (Pennylane) version]: 0.30.0\n", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/10"}, "10": {"author": "Solomon", "date": "1686242994316", "content": "Here you go:\n\ngithub.com/PennyLaneAI/pennylane\n\n\n\n\n\n\n\n\nIssues with training of a QNN for the binary classification of images   9\n\n\n\n        opened \n\n\n\nJun 8, 2023\n\n\n\n\n\n          BoltzmannEntropy\n        \n\n\n\n\n          enhancement \n\n\n\n\n\n### Feature details\n\nDear team,\nThis is a follow up from here:\nhttps://discuss\u2026.pennylane.ai/t/expectation-values-and-tensors-in-qnn/3024/7 \n\nI was unable to find an example that specifically addresses my requirements of processing RGB images, encoding, and training the parameters of a PQC using a quantum circuit alone. Most existing examples tend to involve a classical feature extractor combined with training of only a quantum FC layer, which is not what I am looking for.\n\nI have written a self-contained example for binary classification using the bees/ants dataset, but facing an issue **where the validation and training accuracy do not change.** I believe this issue is not related to overfitting, but rather a problem in my code that I am unable to identify. \n\nI would appreciate it if you could take a look at my code and provide guidance on this.\n\n### Implementation\n\nHere is the code to reproduce the problem:\n```\n# Install necessary packages\n# !pip install torch==1.12.1 torchvision==0.13.1 pennylane==0.30.0 efficientnet_pytorch\n\n# Import required libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport os\nfrom tqdm import tqdm\nimport sys \nfrom pennylane.operation import Tensor\nimport psutil\nimport matplotlib.pyplot as plt\n\n# from qblocks.qgates import *\n\nprint(\"[Python version]:\", sys.version)\nprint(\"[Deep Learning framework, Pytorch (Facebook) version]:\", torch.__version__)\nprint(\"[Quantum Machine Learning framework (Pennylane) version]:\", qml.__version__)\n\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Set parameters\npatch_size = 2\nimg_size_single = 128\nimg_size_flat = img_size_single ** 2\nRGB_C = 3  # Number of channels in the image (RGB)\nbatch_size = 32\nnum_epochs = 60\nn_qubits = patch_size ** 2 * RGB_C\n\nn_layers = 6\nnum_classes = None\n\n# Note if you forget to wrap your circuit with dev: AttributeError: 'tuple' object has no attribute 'float' \ndev_train = qml.device('default.qubit', wires=n_qubits)   \n\n# Define the quantum circuit\n# @qml.qnode(dev_train, interface=\"torch\")\n\ndef Q_Plot(cirq_0, q_b,q_d):\n    # print(\"Plot Q:\", q_b)\n    fig, ax = qml.draw_mpl(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d))        \n    # print (qml.draw(cirq_0,expansion_strategy='device')(torch.zeros(q_b), torch.zeros(q_d)))\n    # plt.figure(figsize=(5,3))\n    # from pylab import rcParams\n    # rcParams['figure.figsize'] = 3, 6\n    # fig.set_size_inches(12,6)\n    plt.show()\n    fig.show()\n\n\ndef Q_count_parameters(qnn):\n    print(dict(qnn.named_parameters()))\n    for name, param in qnn.named_parameters():\n        param.requires_grad=True\n        # print (name, param.data)\n    return sum(p.numel() for p in qnn.parameters() if p.requires_grad)\n\nimport random\n\ndef circuit(inputs, weights):\n    # print('inputs / weights {}/{}'.format(inputs.shape, weights.shape))\n    for qub in range(n_qubits):\n        qml.Hadamard(wires=qub)\n        qml.RY(inputs[qub], wires=qub)\n\n    for l in range(n_layers):\n        qubit_indices = list(range(n_qubits))\n        random.shuffle(qubit_indices)  # Shuffle the qubit indices to create random pairs\n\n        for i in range(0, n_qubits, 2):\n            control_qubit = qubit_indices[i]\n            target_qubit = qubit_indices[(i + 1) % n_qubits]\n\n            # Apply CRZ gate with conditional RY gate\n            random_num = random.uniform(0, 1)\n            qml.CRZ(weights[control_qubit], wires=[control_qubit, target_qubit])\n            qml.RY(random_num * weights[control_qubit], wires=control_qubit)\n            qml.CNOT(wires=[control_qubit, target_qubit])\n            qml.CZ(wires=[control_qubit, (control_qubit + 2) % n_qubits])  # Additional CZ gate for entanglement\n\n    return qml.expval(Tensor(*[qml.PauliZ(i) for i in range(n_qubits)]))\n\n\n# Define the Quanvolutional Neural Network\nclass QuanvolutionalNeuralNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(n_qubits, num_classes)\n        self.q_params = nn.Parameter(torch.Tensor(n_qubits, n_qubits))\n        self.lr1 = nn.LeakyReLU(0.1)\n        nn.init.xavier_uniform_(self.q_params)\n        self.pqc = qml.QNode(circuit, dev_train, interface = 'torch')\n        Q_Plot(circuit,n_qubits,n_qubits)\n\n    \n    def extract_patches(self, x):\n        patches = []\n        bs, c, w, h = x.size()\n        for i in range(w - patch_size + 1):\n            for j in range(h - patch_size + 1):\n                patch = x[:, :, i:i+patch_size, j:j+patch_size]\n                patches.append(patch)\n        patches = torch.stack(patches, dim=1).view(bs, -1, c * patch_size * patch_size)\n        return patches\n\n    def forward(self, x):\n        assert len(x.shape) == 4  # (bs, c, w, h)\n        bs = x.shape[0]  # batch_size = x.size(0)\n        c = x.shape[1]  # RGB\n        x = x.view(bs, c, img_size_single, img_size_single)\n        q_in = self.extract_patches(x)\n        q_in = q_in.to(device)\n        # print (q_in.shape)\n        # q_out = torch.Tensor(0, n_qubits)\n        q_out = torch.Tensor(0, n_qubits)\n\n        q_out = q_out.to(device)\n        for elem in q_in:\n            # print (elem.shape)\n            # print (self.q_params.shape)\n            q_out_elem = self.pqc(elem, self.q_params).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))        \n        x = self.lr1(q_out.view(-1, n_qubits))\n        x = self.fc1(x)\n\n        return x\n\n\n\n# Set the data directory and transformations\ndata_dir = 'datasets/hymenoptera/'\n# import splitfolders as sf\n# sf.ratio('datasets/mri/train', 'output', ratio=(0.65, 0.05, 0.3), seed=42)\ndata_transforms = {\n    'train': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n    'val': transforms.Compose([\n        # transforms.Resize(256),\n        transforms.CenterCrop(img_size_single),\n        transforms.Grayscale() if RGB_C == 1 else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n    ]),\n}\n\n\n# Load the image datasets\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\nnum_classes=len(class_names)\n\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n\n\n# Initialize the Quanvolutional Neural Network\nqnn = QuanvolutionalNeuralNetwork(num_classes=num_classes)\nqnn = qnn.to(device)\n# print (\"Total trainable params:\",Q_count_parameters(qnn))\n\n# Set the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(qnn.parameters(), lr=0.001)\n\n# Train the model\nfor epoch in tqdm(range(num_epochs)):\n    # print(\"Epoch {}/{}, Qubits:{}\".format(epoch, num_epochs, n_qubits))\n    cpu_percent = psutil.cpu_percent()\n    mem_usage= psutil.virtual_memory().total / (1024 ** 3)\n    used_ram_gb = psutil.virtual_memory().used / (1024 ** 3)    \n    # print(f\"Epoch{epoch+1}/{num_epochs}, Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C}, IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]},CPU:{cpu_percent},RAM(GB):{used_ram_gb}'/'{mem_usage}\")\n    print(f\"Epoch:[{epoch+1}/{num_epochs}], Dataset:{data_dir,dataset_sizes}, Qubits:{n_qubits}, RGB:{RGB_C},IMG:{img_size_single} Layers:{n_layers},QNN Params:{[sum(p.numel() for p in qnn.parameters())]} CPU:{cpu_percent}, RAM(GB):{used_ram_gb}/{mem_usage}\")\n    \n    qnn.train()\n    running_loss = 0\n    running_corrects = 0\n    total_samples = 0\n\n    for batch_idx, (data, target) in tqdm(enumerate(dataloaders['train'])):\n        data = data.to(device)\n        target = target.view(-1).to(device)\n        batch_size = data.size(0)  # Get the actual batch size\n\n        optimizer.zero_grad()\n        output = qnn(data)\n\n        # Adjust the output tensor size if necessary\n        if output.size(0) > batch_size:\n            output = output[:batch_size]\n\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(output, 1)\n        running_corrects += torch.sum(predicted == target.data)\n        total_samples += batch_size\n\n    batch_loss = running_loss / len(dataloaders['train'])\n    batch_acc = running_corrects / total_samples\n    print(f'[{epoch + 1}] Training Loss: {batch_loss:.3f}, Training Accuracy: {batch_acc:.3f}')\n\n    running_loss = 0.0\n    running_corrects = 0\n    total_samples = 0\n\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for val_data, val_target in dataloaders['val']:\n            val_data = val_data.to(device)\n            val_target = val_target.to(device)\n            batch_size = val_data.size(0)  # Get the actual batch size\n\n            val_output = qnn(val_data)\n            _, val_predicted = torch.max(val_output.data, 1)\n\n            # Adjust the output and target tensors if necessary\n            if val_predicted.size(0) > batch_size:\n                val_predicted = val_predicted.narrow(0, 0, batch_size)\n                val_target = val_target.narrow(0, 0, batch_size)\n\n            val_total += batch_size\n            val_correct += (val_predicted == val_target).sum().item()\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"[{epoch + 1}] Validation Accuracy: {val_accuracy:.2f}%\")\n\n```\n\n### How important would you say this feature is?\n\n2: Somewhat important. Needed this quarter.\n\n### Additional information\n\nHere is the log:\n```\n[1] Training Loss: 0.705, Training Accuracy: 0.507\n  2%|\u258f         | 1[/60](https://file+.vscode-resource.vscode-cdn.net/60) [01:06<1:05:21, 66.46s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[1] Validation Accuracy: 45.00%\nEpoch:[2[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:29.8, RAM(GB):15.030136108398438[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:55,  6.93s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[2] Training Loss: 0.688, Training Accuracy: 0.507\n  3%|\u258e         | 2[/60](https://file+.vscode-resource.vscode-cdn.net/60) [02:11<1:03:33, 65.75s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[2] Validation Accuracy: 45.00%\nEpoch:[3[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:32.5, RAM(GB):14.813690185546875[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:55,  6.91s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[3] Training Loss: 0.686, Training Accuracy: 0.507\n  5%|\u258c         | 3[/60](https://file+.vscode-resource.vscode-cdn.net/60) [03:17<1:02:31, 65.81s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[3] Validation Accuracy: 45.00%\nEpoch:[4[/60](https://file+.vscode-resource.vscode-cdn.net/60)], Dataset:('datasets[/hymenoptera/](https://file+.vscode-resource.vscode-cdn.net/hymenoptera/)', {'train': 225, 'val': 140}), Qubits:12, RGB:3,IMG:128 Layers:6,QNN Params:[170] CPU:29.7, RAM(GB):15.015106201171875[/32.0](https://file+.vscode-resource.vscode-cdn.net/32.0)\n8it [00:58,  7.37s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[4] Training Loss: 0.686, Training Accuracy: 0.507\n  7%|\u258b         | 4[/60](https://file+.vscode-resource.vscode-cdn.net/60) [04:27<1:03:00, 67.51s[/it](https://file+.vscode-resource.vscode-cdn.net/it)]\n[4] Validation Accuracy: 45.00%\n```\n\n\n\n\n\n\n", "link": "https://discuss.pennylane.ai//t/expectation-values-and-tensors-in-qnn/3024/11"}}