{"0": {"author": "mamadpierre", "date": "1594162682937", "content": "There is an example presented by PennyLane for qml.qnn.TorchLayer which I copy below. If one modifies only the interface line from @qml.qnode(dev) to @qml.qnode(dev, interface='torch') the convergence behavior drastically changes:\nfor the case @qml.qnode(dev):\nAverage loss over epoch 10: 0.1589\nAverage loss over epoch 20: 0.1331\nAverage loss over epoch 30: 0.1321\n\nand for the case  @qml.qnode(dev, interface='torch'):\nAverage loss over epoch 100: 0.1709\nAverage loss over epoch 200: 0.1593\nAverage loss over epoch 300: 0.1538\nAverage loss over epoch 400: 0.1505\nAverage loss over epoch 500: 0.1482\nAverage loss over epoch 600: 0.1462\nAverage loss over epoch 700: 0.1448\nAverage loss over epoch 800: 0.1434\nAverage loss over epoch 900: 0.1426\nAverage loss over epoch 1000: 0.1415\n\nI am wondering what is the reason behind this and further how do we know in general which interface is the most suitable for the problem?\n\ncode:\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport sklearn.datasets\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface='torch') # the default was @qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\nweight_shapes = {\"weights\": (3, n_qubits, 3)}\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\nclayer1 = torch.nn.Linear(2, 2)\nclayer2 = torch.nn.Linear(2, 2)\nsoftmax = torch.nn.Softmax(dim=1)\nmodel = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax)\n\nsamples = 100\nx, y = sklearn.datasets.make_moons(samples)\ny_hot = np.zeros((samples, 2))\ny_hot[np.arange(samples), y] = 1\n\nX = torch.tensor(x).float()\nY = torch.tensor(y_hot).float()\n\nopt = torch.optim.SGD(model.parameters(), lr=0.5)\nloss = torch.nn.L1Loss()\n\n\n\nepochs = 1000\nbatch_size = 5\nbatches = samples // batch_size\n\ndata_loader = torch.utils.data.DataLoader(list(zip(X, Y)), batch_size=batch_size,\n                                          shuffle=True, drop_last=True)\n\nfor epoch in range(epochs):\n\n    running_loss = 0\n\n    for x, y in data_loader:\n        opt.zero_grad()\n        loss_evaluated = loss(model(x), y)\n        loss_evaluated.backward()\n        opt.step()\n        running_loss += loss_evaluated\n\n    avg_loss = running_loss / batches\n    if (epoch+1) % 100 ==0:\n        print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\n\n\n Solved by josh in post #2 \n\n\n                Hi @mamadpierre! \nCurrently, when using the qml.qnn module, it is best to always create a bare QNode with no interface, and use this to initialize the KerasLayer. That is, simply @qml.qnode(dev). \nThe behaviour you found (where training differs if interface= is passed to the decorator) is a known bu\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/1"}, "1": {"author": "josh", "date": "1594203044632", "content": "Hi @mamadpierre!\nCurrently, when using the qml.qnn module, it is best to always create a bare QNode with no interface, and use this to initialize the KerasLayer. That is, simply @qml.qnode(dev).\nThe behaviour you found (where training differs if interface= is passed to the decorator) is a known bug \u2014 this has been fixed in master 2, and will be making it into the next PennyLane release!Solution1", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/2"}, "2": {"author": "mamadpierre", "date": "1594206200508", "content": "Hi @josh\nThanks for your prompt response. I understood.  Now that you answered my question thoroughly and I already shared the above code, I ask you another one.\nHow to use the above code with device = \"cuda\"?\nPlease note that:\n\nI add device = \"cuda\" and modify below two lines:\n\nx, y = x.to(device), y.to(device)\nmodel = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax).to(device)\nBut I receive an error.\n\nIf I omit the qlayer from nn.Sequential, the classical code works fine with cuda.\n\n\nError:\n    line 68, in <module>\n    loss_evaluated.backward()\n    line 198, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n    line 100, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Expected object of device type cuda but got device type cpu for argument 2 'mat2' in call to _th_mm", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/3"}, "3": {"author": "josh", "date": "1594209379790", "content": "Thanks for reporting this @mamadpierre! I will look into this and get back to you, it looks like the current iteration of TorchLayer does not support placing models on the GPU.\nFor now, I recommend using the CPU with the TorchLayer.1", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/4"}, "4": {"author": "mamadpierre", "date": "1594212608960", "content": "Thanks for the time and consideration @josh. My guess is the weights and inputs structures are by default not suitable for Cuda which maybe can be modified?\nThe error has been reported almost at the same time here 1 as well.", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/5"}, "5": {"author": "Tom_Bromley", "date": "1594218652620", "content": "Hi @mamadpierre!\nThanks for checking this out! We had a quick look and managed to get the following to work without error:\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport sklearn.datasets\n\u200b\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\u200b\n@qml.qnode(dev) # the default was @qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\u200b\nweight_shapes = {\"weights\": (3, n_qubits, 3)}\n\u200b\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\u200b\nclayer1 = torch.nn.Linear(2, 2)\nclayer2 = torch.nn.Linear(2, 2)\nsoftmax = torch.nn.Softmax(dim=1)\n\u200b\n\u200b\ndevice = \"cuda\"\nmodel = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax).to(device)\n# model = torch.nn.Sequential(clayer1, clayer2, softmax).to(device)\n\u200b\nsamples = 100\nx, y = sklearn.datasets.make_moons(samples)\ny_hot = np.zeros((samples, 2))\ny_hot[np.arange(samples), y] = 1\n\u200b\nX = torch.tensor(x).float()\nY = torch.tensor(y_hot).float()\nX, Y = X.to(device), Y.to(device)\n\u200b\nmodel(X[:10])\n\nThat being said, TorchLayer is fairly new so it\u2019s possible that it doesn\u2019t work so well on GPU. For now, it might be best to stick with the CPU version as @josh mentioned.", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/6"}, "6": {"author": "mamadpierre", "date": "1594225594281", "content": "Hi @Tom_Bromley Thanks for the time and quick reply.  I copy pasted the provided code and I receive the following error:\nTraceback (most recent call last):\n  File \"torchQuantumLayer.py\", line 70, in <module>\n    loss_evaluated = loss(model(x), y)\n  File \"anaconda3/envs/MnistPytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"anaconda3/envs/MnistPytorch/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n    input = module(input)\n  File \"anaconda3/envs/MnistPytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"anaconda3/envs/MnistPytorch/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 87, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"anaconda3/envs/MnistPytorch/lib/python3.7/site-packages/torch/nn/functional.py\", line 1610, in linear\n    ret = torch.addmm(bias, input, weight.t())\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm\n\n\nVersions:\nPython 3.7.7 (default, May  7 2020, 21:25:33) \n[GCC 7.3.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport pennylane\nimport torch\nprint(torch.__version__)\n1.5.1\nprint(pennylane.__version__)\n0.10.0", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/7"}, "7": {"author": "Tom_Bromley", "date": "1594293244233", "content": "Hi @mamadpierre,\nThanks for also trying that code! Weird that it behaves differently for both of us. I ended up using an earlier version of Torch as the GPU drivers on my device needed updating, perhaps it was that  For now we\u2019ll just have to recommend running on CPU, but this has definitely put GPU support on our radar, thanks!1", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/8"}, "8": {"author": "mamadpierre", "date": "1594162682937", "content": "There is an example presented by PennyLane for qml.qnn.TorchLayer which I copy below. If one modifies only the interface line from @qml.qnode(dev) to @qml.qnode(dev, interface='torch') the convergence behavior drastically changes:\nfor the case @qml.qnode(dev):\nAverage loss over epoch 10: 0.1589\nAverage loss over epoch 20: 0.1331\nAverage loss over epoch 30: 0.1321\n\nand for the case  @qml.qnode(dev, interface='torch'):\nAverage loss over epoch 100: 0.1709\nAverage loss over epoch 200: 0.1593\nAverage loss over epoch 300: 0.1538\nAverage loss over epoch 400: 0.1505\nAverage loss over epoch 500: 0.1482\nAverage loss over epoch 600: 0.1462\nAverage loss over epoch 700: 0.1448\nAverage loss over epoch 800: 0.1434\nAverage loss over epoch 900: 0.1426\nAverage loss over epoch 1000: 0.1415\n\nI am wondering what is the reason behind this and further how do we know in general which interface is the most suitable for the problem?\n\ncode:\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport sklearn.datasets\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface='torch') # the default was @qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\nweight_shapes = {\"weights\": (3, n_qubits, 3)}\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n\nclayer1 = torch.nn.Linear(2, 2)\nclayer2 = torch.nn.Linear(2, 2)\nsoftmax = torch.nn.Softmax(dim=1)\nmodel = torch.nn.Sequential(clayer1, qlayer, clayer2, softmax)\n\nsamples = 100\nx, y = sklearn.datasets.make_moons(samples)\ny_hot = np.zeros((samples, 2))\ny_hot[np.arange(samples), y] = 1\n\nX = torch.tensor(x).float()\nY = torch.tensor(y_hot).float()\n\nopt = torch.optim.SGD(model.parameters(), lr=0.5)\nloss = torch.nn.L1Loss()\n\n\n\nepochs = 1000\nbatch_size = 5\nbatches = samples // batch_size\n\ndata_loader = torch.utils.data.DataLoader(list(zip(X, Y)), batch_size=batch_size,\n                                          shuffle=True, drop_last=True)\n\nfor epoch in range(epochs):\n\n    running_loss = 0\n\n    for x, y in data_loader:\n        opt.zero_grad()\n        loss_evaluated = loss(model(x), y)\n        loss_evaluated.backward()\n        opt.step()\n        running_loss += loss_evaluated\n\n    avg_loss = running_loss / batches\n    if (epoch+1) % 100 ==0:\n        print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n\n\n\n Solved by josh in post #2 \n\n\n                Hi @mamadpierre! \nCurrently, when using the qml.qnn module, it is best to always create a bare QNode with no interface, and use this to initialize the KerasLayer. That is, simply @qml.qnode(dev). \nThe behaviour you found (where training differs if interface= is passed to the decorator) is a known bu\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/different-interfaces-different-performances/454/9"}}