{"0": {"author": "Ziqi_Wang", "date": "1600477120581", "content": "Hi!\nI am running the Multiclass margin classifier project from Pennylane. I knew that Pennylane-qiskit plugin in Windows 10 environment doesn\u2019t work, so I tried to use Windows subsystem Linux. I was able to pip install all the necessary packages like pennylane-qiskit and the code was able to start running. However, it took several hours and thousands of items on IBMQ backend to finish only two iterations, and gave me errors as follows:\nWARNING:urllib3.connectionpool:Retrying (PostForcelistRetry(total=4, connect=3, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /api/Network/ibm-q/Groups/open/Projects/main/Jobs/5f5f1824ab585b001a542449/v/1\nIter:     1 | Cost: 0.3086328 | Acc train: 0.3482143 | Acc test: 0.3947368 \nWARNING:urllib3.connectionpool:Retrying (PostForcelistRetry(total=4, connect=3, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /api/Network/ibm-q/Groups/open/Projects/main/Jobs/5f5f26d8ab585b001a54257f/v/1\nIter:     2 | Cost: 0.1968828 | Acc train: 0.3303571 | Acc test: 0.368421\n\nI already asked in Qiskit Forum, and they said it doesn\u2019t seem to be Qiskit\u2019s error and I should ask in Pennylane Forum. I also tried to run on a real Linux machine and it has the same issue. Maybe its because the algorithm is too much for the remote quantum computer? Any help would be appreciated! Thanks!", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/1"}, "1": {"author": "josh", "date": "1600664206390", "content": "Hi @Ziqi_Wang!\nThat is odd\u2014do you have a minimal code example that produces that same error that you could share here?", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/2"}, "2": {"author": "Ziqi_Wang", "date": "1600706211421", "content": "Thanks for your response!\nThe code is literally the same as the tutorial called \" Multiclass margin classifier\" except that I tried to use the quantum computer from IBM Quantum Experience instead of the default pennylane simulator. The link to the tutorial is as follows:https://pennylane.ai/qml/demos/tutorial_multiclass_classification.html 1\nAnd the source code is:\nimport pennylane as qml\nimport torch\nfrom pennylane import numpy as np\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom qiskit import QuantumRegister\n\nnum_classes = 3\nmargin = 0.15\nfeature_size = 4\nbatch_size = 10\nlr_adam = 0.01\ntrain_split = 0.75\nnum_qubits = 2\nnum_layers = 6\ntotal_iterations = 100\n#HERE is the only difference that I used ibmq backend\ndev = qml.device('qiskit.ibmq', wires=num_qubits, backend=\"ibmq_qasm_simulator\", ibmqx_token='XXX')\n\ndef layer(W):\n    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n    qml.CNOT(wires=[0, 1])\n\ndef circuit(weights, feat=None):\n    qml.templates.embeddings.AmplitudeEmbedding(feat, [0, 1], pad=0.0, normalize=True)\n    for W in weights:\n        layer(W)\n    return qml.expval(qml.PauliZ(0))\n\nqnode1 = qml.QNode(circuit, dev).to_torch()\nqnode2 = qml.QNode(circuit, dev).to_torch()\nqnode3 = qml.QNode(circuit, dev).to_torch()\n\n\ndef variational_classifier(q_circuit, params, feat):\n    weights = params[0]\n    bias = params[1]\n    return q_circuit(weights, feat=feat) + bias\n\ndef multiclass_svm_loss(q_circuits, all_params, feature_vecs, true_labels):\n    loss = 0\n    num_samples = len(true_labels)\n    for i, feature_vec in enumerate(feature_vecs):\n\n        s_true = variational_classifier(\n            q_circuits[int(true_labels[i])],\n            (all_params[0][int(true_labels[i])], all_params[1][int(true_labels[i])]),\n            feature_vec,\n        )\n        s_true = s_true.float()\n        li = 0\n\n        for j in range(num_classes):\n            if j != int(true_labels[i]):\n                s_j = variational_classifier(\n                    q_circuits[j], (all_params[0][j], all_params[1][j]), feature_vec\n                )\n                s_j = s_j.float()\n                li += torch.max(torch.zeros(1).float(), s_j - s_true + margin)\n        loss += li\n\n    return loss / num_samples\n\ndef classify(q_circuits, all_params, feature_vecs, labels):\n    predicted_labels = []\n    for i, feature_vec in enumerate(feature_vecs):\n        scores = [0, 0, 0]\n        for c in range(num_classes):\n            score = variational_classifier(\n                q_circuits[c], (all_params[0][c], all_params[1][c]), feature_vec\n            )\n            scores[c] = float(score)\n        pred_class = np.argmax(scores)\n        predicted_labels.append(pred_class)\n    return predicted_labels\n\ndef accuracy(labels, hard_predictions):\n    loss = 0\n    for l, p in zip(labels, hard_predictions):\n        if torch.abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / labels.shape[0]\n    return loss\n\n\ndef load_and_process_data():\n    data = np.loadtxt(\"iris.csv\", delimiter=\",\")\n    X = torch.tensor(data[:, 0:feature_size])\n    print(\"First X sample (original)  :\", X[0])\n\n    normalization = torch.sqrt(torch.sum(X ** 2, dim=1))\n    X_norm = X / normalization.reshape(len(X), 1)\n    print(\"First X sample (normalized):\", X_norm[0])\n\n    Y = torch.tensor(data[:, -1])\n    return X, Y\n\ndef split_data(feature_vecs, Y):\n    np.random.seed(0)\n    num_data = len(Y)\n    num_train = int(train_split * num_data)\n    index = np.random.permutation(range(num_data))\n    feat_vecs_train = feature_vecs[index[:num_train]]\n    Y_train = Y[index[:num_train]]\n    feat_vecs_test = feature_vecs[index[num_train:]]\n    Y_test = Y[index[num_train:]]\n    return feat_vecs_train, feat_vecs_test, Y_train, Y_test\n\ndef training(features, Y):\n    num_data = Y.shape[0]\n    feat_vecs_train, feat_vecs_test, Y_train, Y_test = split_data(features, Y)\n    num_train = Y_train.shape[0]\n    q_circuits = [qnode1, qnode2, qnode3]\n    all_weights = [\n        Variable(0.1 * torch.randn(num_layers, num_qubits, 3), requires_grad=True)\n        for i in range(num_classes)\n    ]\n    all_bias = [Variable(0.1 * torch.ones(1), requires_grad=True) for i in range(num_classes)]\n    optimizer = optim.Adam(all_weights + all_bias, lr=lr_adam)\n    params = (all_weights, all_bias)\n    print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n\n    costs, train_acc, test_acc = [], [], []\n\n    for it in range(total_iterations):\n        batch_index = np.random.randint(0, num_train, (batch_size,))\n        feat_vecs_train_batch = feat_vecs_train[batch_index]\n        Y_train_batch = Y_train[batch_index]\n\n        optimizer.zero_grad()\n        curr_cost = multiclass_svm_loss(q_circuits, params, feat_vecs_train_batch, Y_train_batch)\n        curr_cost.backward()\n        optimizer.step()\n        predictions_train = classify(q_circuits, params, feat_vecs_train, Y_train)\n        predictions_test = classify(q_circuits, params, feat_vecs_test, Y_test)\n        acc_train = accuracy(Y_train, predictions_train)\n        acc_test = accuracy(Y_test, predictions_test)\n        print(\n            \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc test: {:0.7f} \"\n            \"\".format(it + 1, curr_cost.item(), acc_train, acc_test)\n        )\n\n        costs.append(curr_cost.item())\n        train_acc.append(acc_train)\n        test_acc.append(acc_test)\n\n    return costs, train_acc, test_acc\n\nfeatures, Y = load_and_process_data()\ncosts, train_acc, test_acc = training(features, Y)\nimport matplotlib.pyplot as plt\nfig, ax1 = plt.subplots()\niters = np.arange(0, total_iterations, 1)\ncolors = [\"tab:red\", \"tab:blue\"]\nax1.set_xlabel(\"Iteration\", fontsize=17)\nax1.set_ylabel(\"Cost\", fontsize=17, color=colors[0])\nax1.plot(iters, costs, color=colors[0], linewidth=4)\nax1.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[0])\nax2 = ax1.twinx()\nax2.set_ylabel(\"Test Acc.\", fontsize=17, color=colors[1])\nax2.plot(iters, test_acc, color=colors[1], linewidth=4)\nax2.tick_params(axis=\"x\", labelsize=14)\nax2.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[1])\nplt.grid(False)\nplt.tight_layout()\nplt.show()", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/3"}, "3": {"author": "Tom_Bromley", "date": "1600869360369", "content": "Hi @Ziqi_Wang!\nBefore answering your question - I just wanted to point out that you shared your IBM token in the above code. You should probably edit your post to remove the token and also go onto the IBM Quantum Experience website and generate a new token. It\u2019s an easy mistake to make, but best not to share the token publicly because others could use it.\nIt\u2019s great that you managed to get the code working using WSL! Unfortunately I\u2019d say your experience with wait times using remote backends is expected. When running on a local simulator, you are able to evaluate the circuit very quickly, but when using a remote backend you need to send the job, wait for it to go through the queue, wait for it to simulate, and wait for it to come back to you. This can take a single evaluation from fractions of a second to ~1 second. Moreover, the delay will become increasingly noticeable for deep circuits, since calculation of the gradient requires 2 * N_param circuit evalutions.\nSome ways you could mitigate this are:\n\nBook time on a reservable device, saving you from waiting in the queue\nUse a shallower circuit (potentially you could use more qubits, i.e., greater width, to compensate)\n\nYou could also check out this post for more of a discussion:\n\n\n\nTips on how to improve performance on IBM hardware PennyLane Qiskit\n\n\n    Hello, \nI am trying to get some examples (specifically, variations of the variational quantum classifier) working on IBM hardware. I am managing to send jobs to the simulated hardware, and \u201creal\u201d stuff too. However, the issues I am finding is that training time is extremely long. \nI am assuming therefore, at the moment, it is only feasible to train very small models (~1000 training points, very small network architectures). Is this a correct assumption? \nDoes anyone have any suggestions about ho\u2026\n  \n\nThanks!", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/4"}, "4": {"author": "Ziqi_Wang", "date": "1600874662674", "content": "Thank you so much for your response!\nI didn\u2019t even think about the token when I added the sample code, but I will definitely remember not to include my token next time.\nIt does make sense as the remote backend is usually occupied by others and there are 111 parameters which make this demo\u2019s calculation rather complicated.\nI will look into the advice, thanks again", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/5"}, "5": {"author": "josh", "date": "1600874853753", "content": "@Ziqi_Wang, something to note is that the parameter-shift rule for analytic quantum gradients performs 2p quantum evaluations per optimization step, where p is the number of parameters.\nSo for 111 parameters, that means 222 jobs are being submitted per optimization step!", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/6"}, "6": {"author": "Ziqi_Wang", "date": "1600477120581", "content": "Hi!\nI am running the Multiclass margin classifier project from Pennylane. I knew that Pennylane-qiskit plugin in Windows 10 environment doesn\u2019t work, so I tried to use Windows subsystem Linux. I was able to pip install all the necessary packages like pennylane-qiskit and the code was able to start running. However, it took several hours and thousands of items on IBMQ backend to finish only two iterations, and gave me errors as follows:\nWARNING:urllib3.connectionpool:Retrying (PostForcelistRetry(total=4, connect=3, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /api/Network/ibm-q/Groups/open/Projects/main/Jobs/5f5f1824ab585b001a542449/v/1\nIter:     1 | Cost: 0.3086328 | Acc train: 0.3482143 | Acc test: 0.3947368 \nWARNING:urllib3.connectionpool:Retrying (PostForcelistRetry(total=4, connect=3, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /api/Network/ibm-q/Groups/open/Projects/main/Jobs/5f5f26d8ab585b001a54257f/v/1\nIter:     2 | Cost: 0.1968828 | Acc train: 0.3303571 | Acc test: 0.368421\n\nI already asked in Qiskit Forum, and they said it doesn\u2019t seem to be Qiskit\u2019s error and I should ask in Pennylane Forum. I also tried to run on a real Linux machine and it has the same issue. Maybe its because the algorithm is too much for the remote quantum computer? Any help would be appreciated! Thanks!", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/7"}, "7": {"author": "josh", "date": "1600874853753", "content": "@Ziqi_Wang, something to note is that the parameter-shift rule for analytic quantum gradients performs 2p2p quantum evaluations per optimization step, where pp is the number of parameters.\nSo for 111 parameters, that means 222 jobs are being submitted per optimization step!", "link": "https://discuss.pennylane.ai//t/pennylane-qiskit-connection-error/571/8"}}