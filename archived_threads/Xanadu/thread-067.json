{"0": {"author": "Satanik_Mitra", "date": "1617884987536", "content": "I am trying implement a binary classifier with strawberryfield following the link https://strawberryfields.ai/photonics/demos/run_state_learner.html 6\nMy query is how can I feed my dataset features into this model. I am having a dataset containing 2D features. please suggest.\nThanks", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/1"}, "1": {"author": "Tom_Bromley", "date": "1617892582591", "content": "Hi @Satanik_Mitra!\nRight, this demo focuses on training for state preparation and is more of an optimization task than an ML task, which has input data and a prediction.\nHowever, it\u2019s certainly possible to do binary classification in Strawberry Fields. One way to encode d-dimensional input data into d modes is to have a single gate act on each mode with a parameter set by one dimension of the input data. For example, you could encode 4-dimensional data (x0, x1, x2, x3) into 4 modes by placing a Dgate 2 on each mode. The Dgate on the first mode would encode x0, and so forth.\nThe best tutorial to follow is this one 3. That tutorial also has a section specific to batching 1. You can also check out an implementation of binary classification for fraud detection in our repo here 1, however note that the code there uses an older version of Strawberry Fields.\nFinally, the above is focused on using Strawberry Fields. You can also check out the PennyLane-Strawberry Fields plugin which allows you to harness PennyLane for photonic systems. A relevant tutorial to check out would be function fitting 2.\nThanks!", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/2"}, "2": {"author": "Satanik_Mitra", "date": "1617963116320", "content": "Thank you so much @Tom_Bromley for your kind help. I will go through the links you suggested and implement it. In case any further issues I will post it here. \nRegards\nSatanik Mitra1", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/3"}, "3": {"author": "Satanik_Mitra", "date": "1618318849954", "content": "Hi @Tom_Bromley I have encountered the \" CircuitError: The Program is locked, no more Commands can be appended to it.\" I am trying to implement the below code. My intention was to execute the Engine for 2 times in a loop with same X1 and X2 values. If you can suggest something where I am going wrong. I have tried with engine reset() as well.\nfor i in range(2):\n \n   with prog.context as q:\n     Squeezed(x1) | q[0]\n     Squeezed(x2) | q[1]\n     BSgate(0.4, 0.10) | (q[0], q[1])\n     Rgate(0.4) | q[0]\n     Rgate(0.4) | q[1]\n     MeasureHomodyne(0.0) | q[0]\n    results = eng.run(prog,args=mapping)\n    \neng.print_applied()\n    if eng.run_progs:\n      eng.reset()", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/4"}, "4": {"author": "Tom_Bromley", "date": "1618338568457", "content": "Hi @Satanik_Mitra!\nA program becomes locked after it is run, as you can see in the note here 1. The issue in your code is the second run of the for loop. There, the prog.context context manager is trying to add to the program, which is locked due to the first eng.run().\n\nMy intention was to execute the Engine for 2 times in a loop with same X1 and X2 values.\n\nI\u2019m curious about your use case. Is the idea of running twice with the same values to extract samples from the homodyne measurement? If so, sampling can be done in the following way:\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\n\nshots = 100\nprog = sf.Program(2)\nx1 = 0.4\nx2 = 0.5\n\nwith prog.context as q:\n    Squeezed(x1) | q[0]\n    Squeezed(x2) | q[1]\n    BSgate(0.4, 0.10) | (q[0], q[1])\n    Rgate(0.4) | q[0]\n    Rgate(0.4) | q[1]\n    MeasureHomodyne(0.0) | q[0]\n\neng = sf.Engine(\"fock\", backend_options={\"cutoff_dim\": 4})\nsamples = []\n\nfor i in range(shots):\n    result = eng.run(prog)\n    s = result.samples\n    samples.append(s.flatten()[0])\n\nAlternatively, if you are looking for batching with the TF backend, check out our tutorial here.1", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/5"}, "5": {"author": "Satanik_Mitra", "date": "1618394423423", "content": "Thanks a lot @Tom_Bromley for the reply. You are correctly mention that there\u2019s no point in executing the code with same X1 X2 values twice, actually  I was exploring the possibilities of executing a circuit multiple time, and as a trial run I choose X1 X2 to same in both run. However, I want feed newer X1 X2 every time by which I can run through all the data points in the training set. Now, I will try to implement it with your modified code. Thanks a lot for you suggestions and modified code. I will come back to you in case any further issue. You are a great help\u2026  ", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/6"}, "6": {"author": "Tom_Bromley", "date": "1618403606804", "content": "Thanks for the info @Satanik_Mitra! In that case, I\u2019d definitely recommend checking out the batching section of the demo. Good luck and let us know if you have any other difficulties!1", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/7"}, "7": {"author": "Satanik_Mitra", "date": "1618498493287", "content": "steps = 10\n\nfor i in range(len(train_x)):\n\n  _x1 = tf.Variable(train_x[i][0])\n\n  _x2 = tf.Variable(train_x[i][1])\n\n  _y = train_y[i]\n\n  for step in range(steps):\n\n    with tf.GradientTape() as tape:\n\n        # execute the engine\n\n        results = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2},**{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\n        prob1 = results.state.fock_prob([2,0])\n\n        prob2 = results.state.fock_prob([0,2])\n\n        loss1 = -tf.reduce_sum(prob1)\n\n        loss2 = -tf.reduce_sum(prob2)\n\n    if loss1<loss2:   \n\n      gradients = tape.gradient(loss1, [_x1, _x2])\n\n    else:\n\n      gradients = tape.gradient(loss2, [_x1, _x2])\n\n    opt.apply_gradients(zip(gradients, [_x1, _x2]))\n\nHi @Tom_Bromley, As I mentioned earlier in my dataset I have two feature values for every data points. I am passing each of the data points through the circuit 10 times by putting steps =10 and calculating the loss with  loss1 and loss2. Likewise I am feeding my entire training set through the circuits. This way I am getting training accuracy as well. Next, I  ran the validation set as well with same approach however there I am not considering the loss part and predict the labels directly. In this classification approach I try to incorporate the classification approach followed in  \" Quantum machine learning in feature Hilbert spaces\"\nAnyway, I am worried with my approach as I am not feeding the entire training set in one go (that\u2019s I am looping and feeding). Is it a right approach? if you can kindly suggest whether my approach of training and validation is correct or not. and if not where I went wrong. I have tried with the batching approach but there I am getting prediction for only the first batch size. The training is not iterated over the entire dataset. Hence, I followed the above approach.", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/8"}, "8": {"author": "Ant_Hayes", "date": "1618507138476", "content": "Hi @Satanik_Mitra,\nThanks for sharing your updated approach! Feeding your data through the circuit 10 times sounds good. You can vary this number to see if it gives better predictions, certainly worth checking (as long as it doesn\u2019t take too long to train!). Having a separate validation data set to check the trained model is also a good idea \nIt may be useful to measure the validation loss too as this will give an idea of how well the trained model is generalising to unseen feature data. If you have enough data it can be useful to split a third \u201ctest\u201d data set to check the direct predictions.\nIt\u2019s difficult to comment on why your batching approach only predicted for the first batch without seeing your code (feel free to share!), however the code you have provided is an example of batching itself  i.e it breaks up a large data set and feeds smaller subsets of the data to the model bit-by-bit. Nice Work!\nLet us know if you have any other questions!", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/9"}, "9": {"author": "Satanik_Mitra", "date": "1618553502449", "content": "Thank you @Ant_Hayes for your reply. Yeah you are correct that my approach is an extension of batching approach, or you can say with batch_size=1  . However, please have a look at the chunk below -\n_x1 = tf.Variable(train_x[:,0])\n_x2 = tf.Variable(train_x[:,1])\n_y = train_y[:]\n\nfor step in range(steps):\n    with tf.GradientTape() as tape:\n        results = eng.run(circuit, args={**{\"x1\": _x1[:6], \"x2\": _x2[:6]},**{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\nThe batching approach I tried to execute in the following way. Taking batch_size=6", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/10"}, "10": {"author": "Ant_Hayes", "date": "1618570561074", "content": "Hi @Satanik_Mitra,\nThanks for sharing your batching code! It may be the case that you need to include the batch size in the backend options when defining the engine:\nbatch_size = 6\neng = sf.Engine(\"tf\", backend_options={ \"batch_size\": batch_size})\n\nfor step in range(steps):\n    with tf.GradientTape() as tape:\n        results = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2}, **{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\nThis way you won\u2019t need to index the the features when running the engine which may be why the model was only training on the first batch.\nAlternatively, expanding on your method:\nbatches = np.arange(0, len(_x1), 6).tolist() # makes list of multiples of 6\nfor step in range(steps):\n    for i in range(len(batches) - 1):\n        slice = batches[i:i+2] # this will be used to slice the feature data to create a batch\n        with tf.GradientTape() as tape:\n            results = eng.run(circuit, args={**{\"x1\": _x1[slice[0]:slice[1]], \"x2\": _x2[slice[0]:slice[1]]},**{f\"theta{i}\": _thetas[i] for i in range(8)}})\n\n\nThis would iterate over the whole data set in batches of size 6. Note that this assumes len(_x1)  % 6 = 0, otherwise some data points would be missed. Also note that these examples haven\u2019t been tested (since we don\u2019t have your data so please take this as guidance rather than an exact solution   )\nHope this helps!1", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/11"}, "11": {"author": "Satanik_Mitra", "date": "1618575429246", "content": "Thanks a lot @Ant_Hayes . I will made the modification in my code accordingly and let you know if any issue arise ", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/12"}, "12": {"author": "Ant_Hayes", "date": "1618595561365", "content": "No problem @Satanik_Mitra , let us know if you have any more questions!", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/13"}, "13": {"author": "Satanik_Mitra", "date": "1618646100775", "content": "Hi @Ant_Hayes please check the circuit and theta values I am using for classification of my 2D dataset. However, the final accuracy of training set and testing set is overfitted. Can you please suggest anything?\nThetas -\nthetas = circuit.params(*[f\"theta{i}\" for i in range(4)])                    \n_thetas = tf.Variable(0.4 * tf.ones((4, batch_size)))\n\nCircuit\nwith circuit.context as q:\n\n    Squeezed(sq,x1) | q[0]\n\n    Squeezed(sq,x2) | q[1]\n\n    BSgate(thetas[0], thetas[1]) | (q[0], q[1])\n\n    Pgate(thetas[2]) | q[0]\n\n    Pgate(thetas[2]) | q[1]\n\n    Vgate(thetas[3]) | q[0]\n\n    Vgate(thetas[3]) | q[1]\n\nTraining execution\nwith tf.GradientTape() as tape:\n  results = eng.run(circuit, args={**{\"x1\": _x1, \"x2\": _x2},**{f\"theta{i}\": _thetas[i] for i in range(4)}})\n\nTesting execution (I am checking it without theta values)\nfor j in range(len(test_x)):\n\n    x1_val = tf.Variable(test_x[j][0])\n\n    x2_val = tf.Variable(test_x[j][1])\n\n    _y = test_y[j]\n\n    validation = eng.run(circuit, args={**{\"x1\": x1_val, \"x2\": x2_val}})\n\n        \n\n    prob1 = validation.state.fock_prob([2,0])\n\n    prob2 = validation.state.fock_prob([0,2])\n\n    m1 = tf.reduce_mean(prob1)\n\n    m2 = tf.reduce_mean(prob2)\n\n    p0 = m1/(m1+m2)\n\n    p1 = m2/(m1+m2)\n\n    if p0 > p1:\n\n      label = 0\n\n    else:\n\n      label = 1\n\n    val_label.append(label)\n\n    val_true.append(_y)\n\nprint(accuracy(labels, predictions))\n\nprint(accuracy(val_true, val_label))\n\nAccuracy training 0.58\nAccuracy Testing  0.70\nPlease let me know where I went wrong or how can I handle the overfitting?", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/14"}, "14": {"author": "Ant_Hayes", "date": "1618831542293", "content": "Hi @Satanik_Mitra, your implementatoin using strawberryfields looks good! When it comes to handling overfitting there is no set method but there are a couple of general solutions:\n\n\nUse more data points (if possible).\n\n\nReduce the complexity of the model. The aim is to reduce the number of trainable parameters.\n\n\nEarly stopping. Once the model\u2019s training accuracy plateaus, stop the training.\n\n\nThe key is to prevent the model from \u201cmemorising \u201d the training data so it can generalise to unseen data.\nYour implementation looks sound, so at this point it is a matter of adding small variations to your approach in order to increase the accuracies. There\u2019s no guarantee that the model will be able to hit 99.99% accuracy but it worth trying! 70% fidelity of a state is a pretty good starting point!", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/15"}, "15": {"author": "Satanik_Mitra", "date": "1618908314685", "content": "Thank you so much @Ant_Hayes for the motivating comment. I will try to fine tune the model as per your suggestions. ", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/16"}, "16": {"author": "Ant_Hayes", "date": "1618912253473", "content": "Hey @Satanik_Mitra, no problem! Your doing some great work here  feel free to share your findings!", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/17"}, "17": {"author": "Satanik_Mitra", "date": "1619078307496", "content": "@Ant_Hayes I have tried with increased datapoints received marginally higher train accuracy than validation. However, I was trying with state.fock_prob([0,2]) and state.fock_prob([2,0])\nis there any documentation available which discussed in details about the state probabilities? In my work I keep the cut off dimension as 3 which will capture 3 states |0>, |1> and |2> having two modes do I need to try out all the combination of states? It will be great if someway I can be enlighten a little bit about the state probabilities.", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/18"}, "18": {"author": "Ant_Hayes", "date": "1619087870632", "content": "Hi @Satanik_Mitra, glad to hear your seeing improvements (even if they\u2019re marginal).\nYes here is the documentation on the fock_prob() 1 which outlines that the returned probability is the overlap of a multimode fock state with the state of interest. So in your example  prob1 is the overlap of your trained state and a multimode fock state with the second excited state in the 1st mode and the vacuum sate in the second mode. Similarly for prob2. As the overlap (the probability) approaches 1 the trained state approaches the target multimode fock state.\nThe cutoff_dim is simply a truncation of the fock space dimension which would ideally be infinite for CV sates. Increasing this can increase the accuracy of computations so would be worth trying! But note that this will come at the cost of memory usage!\nLet us know if you have any more questions ", "link": "https://discuss.pennylane.ai//t/feeding-dataset-into-strawberry-field/959/19"}}