{"0": {"author": "QuantumMan", "date": "1697519710825", "content": "Hello! If applicable, put your complete code example down below. Make sure that your code:\n\nis 100% self-contained \u2014 someone can copy-paste exactly what is here and run it to\nreproduce the behaviour you are observing\nincludes comments\n\nI am trying to run the scripts describe in blog - Distributing quantum simulations using lightning.gpu with NVIDIA cuQuantum | PennyLane Blog 2 on NERSC machines, but facing problem running the script. Followed the script to install pennylane-lightning-gpu from source code.\nfrom mpi4py import MPI\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom timeit import default_timer as timer\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Set number of runs for timing averaging\nnum_runs = 3\n\n# Choose number of qubits (wires) and circuit layers\nn_wires = 32\nn_layers = 2\n\n# Instantiate CPU (lightning.qubit) or GPU (lightning.gpu) device\n# mpi=True to switch on distributed simulation\n# batch_obs=True to reduce the device memory demand for adjoint backpropagation\ndev = qml.device('lightning.gpu', wires=n_wires, mpi=True, batch_obs=True)\n\n# Create QNode of device and circuit\n@qml.qnode(dev, diff_method=\"adjoint\")\ndef circuit_adj(weights):\n    qml.StronglyEntanglingLayers(weights, wires=list(range(n_wires)))\n    return qml.math.hstack([qml.expval(qml.PauliZ(i)) for i in range(n_wires)])\n\n# Set trainable parameters for calculating circuit Jacobian at the rank=0 process\nif rank == 0:\n    params = np.random.random(qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_wires))\nelse:\n    params = None\n\n# Broadcast the trainable parameters across MPI processes from rank=0 process\nparams = comm.bcast(params, root=0)\n\n# Run, calculate the quantum circuit Jacobian and average the timing results\ntiming = []\nfor t in range(num_runs):\n    start = timer()\n    jac = qml.jacobian(circuit_adj)(params)\n    end = timer()\n    timing.append(end - start)\n\n# MPI barrier to ensure all calculations are done\ncomm.Barrier()\n\nif rank == 0:\n    print(\"num_gpus: \", size, \" wires: \", n_wires, \" layers \", n_layers, \" time: \", qml.numpy.mean(timing)) \n\nIf you want help with diagnosing an error, please put the full error message below:\nmpirun -np 4 python test.py\n*** The MPI_Comm_rank() function was called before MPI_INIT was invoked.\n*** This is disallowed by the MPI standard.\n*** Your MPI job will now abort.\n[nid008340:1176249] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n*** The MPI_Comm_rank() function was called before MPI_INIT was invoked.\n*** This is disallowed by the MPI standard.\n*** Your MPI job will now abort.\n[nid008340:1176248] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n*** The MPI_Comm_rank() function was called before MPI_INIT was invoked.\n*** This is disallowed by the MPI standard.\n*** Your MPI job will now abort.\n[nid008340:1176246] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n*** The MPI_Comm_rank() function was called before MPI_INIT was invoked.\n*** This is disallowed by the MPI standard.\n*** Your MPI job will now abort.\n[nid008340:1176247] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n--------------------------------------------------------------------------\nprterun detected that one or more processes exited with non-zero status, thus causing\nthe job to be terminated. The first process to do so was:\n\n  Process name: [prterun-nid008340-1176242@1,0]\n  Exit code:    14\n--------------------------------------------------------------------------\n\n\nAnd, finally, make sure to include the versions of your packages. Specifically, show us the output of qml.about().\n>>> qml.about()\nName: PennyLane\nVersion: 0.32.0\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/PennyLaneAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /global/u1/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml, typing-extensions\nRequired-by: PennyLane-Lightning, PennyLane-Lightning-GPU\n\nPlatform info:           Linux-5.14.21-150400.24.81_12.0.86-cray_shasta_c-x86_64-with-glibc2.31\nPython version:          3.10.12\nNumpy version:           1.23.5\nScipy version:           1.11.3\nInstalled devices:\n- default.gaussian (PennyLane-0.32.0)\n- default.mixed (PennyLane-0.32.0)\n- default.qubit (PennyLane-0.32.0)\n- default.qubit.autograd (PennyLane-0.32.0)\n- default.qubit.jax (PennyLane-0.32.0)\n- default.qubit.tf (PennyLane-0.32.0)\n- default.qubit.torch (PennyLane-0.32.0)\n- default.qutrit (PennyLane-0.32.0)\n- null.qubit (PennyLane-0.32.0)\n- lightning.qubit (PennyLane-Lightning-0.32.0)\n- lightning.gpu (PennyLane-Lightning-GPU-0.33.0.dev0)\n>>> \n>>> \n\n\nCUDA details\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Tue_May__3_18:49:52_PDT_2022\nCuda compilation tools, release 11.7, V11.7.64\nBuild cuda_11.7.r11.7/compiler.31294372_0\n", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1697588667947", "content": "Hi @QuantumMan,\nThank you for your question.\nUnfortunately each HPC system needs its own MPI implementation. I see that your error mentions MPI_INIT so I would think this is either an initialization error or an installation error.\n@mlxd might have more insights on this.1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/2"}, "2": {"author": "QuantumMan", "date": "1697592303397", "content": "Thanks @CatalinaAlbornoz - As per this blog NERSC HPC Perlmutter machine has been used, and i am trying the same machine. So probably i am missing something. @mlxd  any help will be appreciated. thanks in advance.1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/3"}, "3": {"author": "QuantumMan", "date": "1697592323553", "content": "\n\n\npennylane.ai\n\n\n\nDistributing quantum simulations using lightning.gpu with NVIDIA cuQuantum |... 3\nThe new MPI support for lightning.gpu now allows users to execute large-scale workloads on multi-node/multi-GPU systems, leading to better performance for systems with more than 30 qubits.\n\n\n\n\n\n", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/4"}, "4": {"author": "CatalinaAlbornoz", "date": "1697639401532", "content": "Hi @QuantumMan thank you for the clarification. We\u2019re looking into this. We may need several days to come back to you but we will be back.", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/5"}, "5": {"author": "QuantumMan", "date": "1697679142608", "content": "I resolved this. You are right, this is mostly dependent on the HPC software. One of the things i made sure is to maintain the same version of pennylane pip installed version and lightning-gpu plugin installation. I am not sure if some documentation/error messages make it clear. thanks for your help.", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/6"}, "6": {"author": "CatalinaAlbornoz", "date": "1697771570104", "content": "I\u2019m glad you managed to solve the issue @QuantumMan !", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/7"}, "7": {"author": "QuantumMan", "date": "1697948882757", "content": "Hi @CatalinaAlbornoz  - when i am trying to run the same script on the same HPC machine with 4 nodes. i see the script fails beyond 24 quibits(20,22 qubits work fine) with [/global/u1/p/prmantha/pennylane-lightning-gpu/pennylane_lightning_gpu/src/util/DataBuffer.hpp][Line:48][Method:DataBuffer]: Error in PennyLane Lightning: out of memory error.\nI am tyring to run the script on 16 A100 GPUs with multi-node/multi-gpu support. below is the command\nmpirun -np 16 python testmpi.py 24(qubits passed as argument)\nAny thoughts why this could happen? also how do i verify that multi-gpus are being utlized? anyway to capture the gpu metrics\u2026", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/8"}, "8": {"author": "QuantumMan", "date": "1697949579913", "content": "Here is the full error message\n  File \"/global/homes/p/prmantha/pilot-streaming/examples/scripts/task-parallelism/testmpi.py\", line 22, in <module>\n    dev = qml.device('lightning.gpu', wires=n_wires, mpi=True, batch_obs=True)\n  File \"/global/homes/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages/pennylane/__init__.py\", line 345, in device\n    dev = plugin_device_class(*args, **options)\n  File \"/global/homes/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages/pennylane_lightning_gpu/lightning_gpu.py\", line 287, in __init__\n    self._gpu_state = _gpu_dtype(c_dtype, mpi)(\npennylane_lightning_gpu.lightning_gpu_qubit_ops.PLException: [/global/u1/p/prmantha/pennylane-lightning-gpu/pennylane_lightning_gpu/src/util/DataBuffer.hpp][Line:48][Method:DataBuffer]: Error in PennyLane Lightning: out of memory```\n\nFull script \n\n```from mpi4py import MPI\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom timeit import default_timer as timer\nimport datetime\nimport sys\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Set number of runs for timing averaging\nnum_runs = 3\n\n# Choose number of qubits (wires) and circuit layers\nn_wires = int(sys.argv[1])\nn_layers = 2\n\n# Instantiate CPU (lightning.qubit) or GPU (lightning.gpu) device\n# mpi=True to switch on distributed simulation\n# batch_obs=True to reduce the device memory demand for adjoint backpropagation\ndev = qml.device('lightning.gpu', wires=n_wires, mpi=True, batch_obs=True)\n\n# Create QNode of device and circuit\n@qml.qnode(dev, diff_method=\"adjoint\")\ndef circuit_adj(weights):\n    qml.StronglyEntanglingLayers(weights, wires=list(range(n_wires)))\n    return qml.math.hstack([qml.expval(qml.PauliZ(i)) for i in range(n_wires)])\n\n# Set trainable parameters for calculating circuit Jacobian at the rank=0 process\nif rank == 0:\n    params = np.random.random(qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_wires))\nelse:\n    params = None\n\n# Broadcast the trainable parameters across MPI processes from rank=0 process\nparams = comm.bcast(params, root=0)\n\n# Run, calculate the quantum circuit Jacobian and average the timing results\ntiming = []\nfor t in range(num_runs):\n    start = timer()\n    jac = qml.jacobian(circuit_adj)(params)\n    end = timer()\n    timing.append(end - start)\n\n# MPI barrier to ensure all calculations are done\ncomm.Barrier()\n\nif rank == 0:\n    run_timestamp=datetime.datetime.now()\n    RESULT_FILE= \"mpi-script-\" + run_timestamp.strftime(\"%Y%m%d-%H%M%S\") +  \".csv\"\n    result_str=\"num_gpus: {}, wires: {}, layers: {}, time: {}s\".format(size, n_wires, n_layers, qml.numpy.mean(timing))\n    print(result_str)\n    with open(RESULT_FILE, \"w\") as f:\n        f.write(result_str)\n        f.flush()```\n\n\n\nThe script fails when initializing device with lightning.gpu, why is it failing with error without loading anything into memory?", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/9"}, "9": {"author": "CatalinaAlbornoz", "date": "1698099005866", "content": "Hi @QuantumMan,\nPennyLane lightning starts by initializing a state with the full size of the system you specified. Even if you don\u2019t perform any computation on the 24 qubits, this error is telling you that your computer doesn\u2019t have enough memory to store the state generated by so many qubits. Normally if you have 80GB RAM you should be able to reach about 30qubits but this can change depending on your specific settings.1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/10"}, "10": {"author": "CatalinaAlbornoz", "date": "1698106535963", "content": "Hi @QuantumMan, I mentioned 80GB as an example. In your case with 40GB GPUs you should be able to reach 28 or 29 qubits. Depending on the number of observables that you have you can split the computation into the different GPUs but here the problem happens at initialization so I don\u2019t think there\u2019s much that works here. For you to be able to run your problem you need to be able to run the statevector in a single GPU at least once.1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/11"}, "11": {"author": "mlxd", "date": "1698188388442", "content": "Hi @QuantumMan\nI had some time to test this out today, and was able to run your example code across single GPUs, and multiple GPUs, depending on the workload. I wasn\u2019t able to hit the memory wall you reported on 24 qubits, so I figured we can try to make a similar build, and start from there.\nFirst, to make sure you have a working build with the CUDA aware MPI-env, I\u2019d suggest ensuring these steps are followed:\n\n# Ensure all necessary modules are loaded up-front\n\nmodule load PrgEnv-gnu cray-mpich cudatoolkit craype-accel-nvidia80 evp-patch gcc/11.2.0\n\n# Required due to a potentially missing lib for the CUDA-aware MPICH library\n\nexport LD_LIBRARY_PATH=${CRAY_LD_LIBRARY_PATH}:/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib/:$LD_LIBRARY_PATH\n\n# Create a python virtualenv\n\npython -m venv pyenv && source ./pyenv/bin/activate\n\n# install the following dependencies\n\npython -m pip install cmake ninja custatevec-cu11 wheel pennylane~=0.32.0 pennylane-lightning~=0.32.0\n\n# build mpi4py against the system's CUDA-aware MPICH\n\nMPICC=\"cc -shared\" python -m pip install --force --no-cache-dir --no-binary=mpi4py mpi4py\n\n# clone and checkout lightning.gpu at version 0.32.0\n\ngit clone https://github.com/PennyLaneAI/pennylane-lightning-gpu\n\ncd pennylane-lightning-gpu && git checkout v0.32.0\n\n# build the extension module with MPI support, and package it all into a wheel\n\npython setup.py build_ext --define=\"PLLGPU_ENABLE_MPI=ON;-DCMAKE_CXX_COMPILERS=$(which mpicxx);-DCMAKE_C_COMPILER=$(which mpicc)\"\n\npython setup.py bdist_wheel\n\n# Assuming the above steps are reproduced, your wheel can be installed as needed\n\npython -m pip install ./dist/*.whl\n\n# Grab an allocation for however many GPUs you want; I'd start with a single interactive node/4 GPUs\n\nsalloc ...\n\n# and launch over multiple processes\n\nsrun -n 4 python script.py\n\n\nWith the above, I was able to run your adjoint-diff script up to about 28-29 qubits. The reason for this limit is how the script is defined. This script is designed to evaluate the Jacobian of the circuit, relative to one observable per qubit in the circuit. When calculating the Jacobian of the quantum circuit using the adjoint diff method several copies of the state-vector are required, with the number of copies scaling with the number of observables you require. We aim to linearise the required observables with the batch_obs keyword, but that only allows us to save memory up to a point, as multiple copies are still needed for the method to work. If you take a single node (4 GPUs) with an interactive allocation, you can launch tmux, and split the session, with your srun command on the left, and watch -n1 nvidia-smi on the right to see what is happening to memory usage. You should get images similar to the following:\nsrun_4_adjoint_sel_281913\u00d7671 240 KB\nNote, the above example should also work without MPI (for a single node with 4+ GPUs), as the batching support allows the adjoint to use std::thread to concurrently evaluate the Jacobian terms. I\u2019d expect 28 qubits to be faster without MPI. However, if your problem grows (29 qubits+, with 29+ observables) you\u2019ll hit the memory wall quickly.\nIf you want to simulate a larger number of qubits, and don\u2019t care about gradients, you can simply evaluate the circuit in the forward execution, disabling the gradient pipeline by setting diff_method=None, and removing the qml.jacobian call. In this situation, you will have a single state-vector, which is reused to evaluate the observables, and should allow up to 33 qubits (16 bytes per complex coefficient x 2^33 coefficients), coming in around 128GB of GPU memory required, which will be split to around 32GB per card (plus some overheads), as in the following image:\nsrun_4_sel_fwd_33847\u00d7593 73.6 KB\nI\u2019d suggest setting up your env again with the above steps, and trying to repeat running scripts with the following steps:\n\n\nSet diff_method=None, and remove qml.jacobian to see how the forward pass scales. You should be able to hit the same as I did (33 qubits)\n\n\nTry adding more nodes/GPUs and increase the count further (assume every qubit you add will require multiplying your required GPU count by 2)\n\n\nLower your qubit count by 2-8 (depends on your problem/number of observables/etc), and try to evaluate the Jacobians again. For a single node, you should be able to hit 29 qubits with batch_obs=True, at the expense of a slightly longer runtime. If your problem works without this, it\u2019ll be much faster (I hit 28 qubits without this).\n\n\nHope this helps to understand what\u2019s happening under the hood \u2014 in essence, gradients require a lot more memory than a general forward execution due to the additional needs of multiple state-vector copies per observable and overheads needed in the evaluations. Let us know if we can help any more here.2", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/12"}, "12": {"author": "QuantumMan", "date": "1699811503572", "content": "Thanks @mlxd for the detailed instructions. It did work finally following your instructions. I could run 33 qubits on one node with 4GPUs/160GB memory without jacobian call and diff_method=None. But when i try to scale beyond 33qubits to 40 qubits on 4 nodes, i see OOM errors. Is it possible to linearly scale the nunber of qubits, or am i missing something. thanks.", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/13"}, "13": {"author": "mlxd", "date": "1699880639509", "content": "Hi @QuantumMan\nUnfortunately linear scaling is not possible. For statevector simulators, the memory requirements grow exponentially with the qubit count.\nFor 40 qubits, you\u2019ll need at a minimum 16 (bytes per complex double) x 2^40 (coefficients for 40 qubits), requiring on the order of hundreds of nodes to represent the state. You can think of it as a doubling in the required GPU count for every qubit increase.1 Reply", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/14"}, "14": {"author": "QuantumMan", "date": "1699940652054", "content": "\n\n\n mlxd:\n\n2^40\n\n\nThats right, so with 4 nodes = 4(nodes) * 4(gpus) * 40GB(mem/gpu) = 640GB memory available , based on the above math for 35 qubits we need 16 * (2^35) = 549 GB , but when i try to scale beyond 33 Qubits i.e 34, 35 i see OOM errors.\nsrun -n 16 python penny-no-jacob-mpi.py\nsrun: Job 18174449 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for StepId=18174449.1\nslurmstepd: error: Detected 1 oom_kill event in StepId=18174449.1. Some of the step tasks have been OOM Killed.\nsrun: error: nid004013: task 8: Out Of Memory\nsrun: Terminating StepId=18174449.1\nslurmstepd: error: Detected 1 oom_kill event in StepId=18174449.1. Some of the step tasks have been OOM Killed.\nslurmstepd: error: Detected 1 oom_kill event in StepId=18174449.1. Some of the step tasks have been OOM Killed.\nslurmstepd: error: Detected 1 oom_kill event in StepId=18174449.1. Some of the step tasks have been OOM Killed.\nIs it because of some memory overhead this is failing?1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/15"}, "15": {"author": "isaacdevlugt", "date": "1700060620680", "content": "@QuantumMan am joining in a little late here, but just because your circuit has N qubits doesn\u2019t mean that the memory used will be exactly 2^N scaled by the data type (e.g., 16 bytes per coeff). 2^N scaled by the data type is really the lower bound on your memory usage.\nIt would help to see what else you\u2019re trying to calculate in penny-no-jacob-mpi.py ", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/16"}, "16": {"author": "QuantumMan", "date": "1700188154291", "content": "Hi,\nHere is the script\nimport pennylane as qml\nimport numpy as np\nfrom timeit import default_timer as timer\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Set number of runs for timing averaging\nnum_runs = 3\n\n# Choose number of qubits (wires) and circuit layers\nn_wires = 35\nn_layers = 2\n\n# Instantiate CPU (lightning.qubit) or GPU (lightning.gpu) device.\n# mpi=True to switch on distributed simulation\ndev = qml.device('lightning.gpu', wires=n_wires, mpi=True)\n\n# Set target wires for probability calculation\nprob_wires = range(n_wires)\n\n# Create QNode of device and circuit\n@qml.qnode(dev)\ndef circuit(weights):\n    qml.StronglyEntanglingLayers(weights, wires=list(range(n_wires)))\n    return qml.probs(wires=prob_wires)\n\n# Set trainable parameters for calculating circuit Jacobian at the rank=0 process\nif rank == 0:\n    params = np.random.random(qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_wires))\nelse:\n    params = None\n\n# Broadcast the trainable parameters across MPI processes from rank=0 process \nparams = comm.bcast(params, root=0)\n\n# Run, calculate the quantum circuit Jacobian and average the timing results\ntiming = []\nfor t in range(num_runs):\n    start = timer()\n    local_probs = circuit(params)\n    end = timer()\n    timing.append(end - start)\n\n# MPI barrier to ensure all calculations are done\ncomm.Barrier()\n\nif rank == 0:\n    print(\"num_gpus: \", size, \" wires: \", n_wires, \" layers \", n_layers, \" time: \", qml.numpy.mean(timing))```2 Replies", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/17"}, "17": {"author": "isaacdevlugt", "date": "1700249131170", "content": "Your code will definitely use more memory than what\u2019s required to store a complex state of size 2^N . If you want a good idea for how your code is going to scale up as you increase the number of qubits (i.e., what your RAM requirements are as a function of the number of qubits) then you can use one of the many profilers available for Python programs. Here\u2019s a decent resource!\n\n\n\nGeeksforGeeks \u2013 23 Feb 21\n\n\n\nMonitoring memory usage of a running Python program - GeeksforGeeks\nA Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.\n\n\n\n\n\nLet me know if this helps ", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/18"}, "18": {"author": "isaacdevlugt", "date": "1700060620680", "content": "@QuantumMan am joining in a little late here, but just because your circuit has N qubits doesn\u2019t mean that the memory used will be exactly 2^N2N scaled by the data type (e.g., 16 bytes per coeff). 2^N2N scaled by the data type is really the lower bound on your memory usage.\nIt would help to see what else you\u2019re trying to calculate in penny-no-jacob-mpi.py ", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/19"}, "19": {"author": "isaacdevlugt", "date": "1700249131170", "content": "Your code will definitely use more memory than what\u2019s required to store a complex state of size 2^N2N . If you want a good idea for how your code is going to scale up as you increase the number of qubits (i.e., what your RAM requirements are as a function of the number of qubits) then you can use one of the many profilers available for Python programs. Here\u2019s a decent resource!\n\n\n\nGeeksforGeeks \u2013 23 Feb 21\n\n\n\nMonitoring memory usage of a running Python program - GeeksforGeeks\nA Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.\n\n\n\n\n\nLet me know if this helps ", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/20"}, "20": {"author": "mlxd", "date": "1700500617172", "content": "Hi @QuantumMan\nJust some comments on the script: I believe the issue here is when returning the local probability vectors back to the hosts, since the OOM error seems to occur at the host level (if I am mistaken, any additional error logs you can provide here may help). In this instance, you may be using too much memory on the host too \u2014 for problems like this, the general best case option is to increase the number of resources (doubling the MPI ranks, nodes used), and examine if the failure still occurs.\nAlso, including additional overheads in the CUDA runtime library, custatevec library, MPI dispatch buffer, and caching mechanisms means (as @isaacdevlugt  mentioned) we can very easily hit the memory ceiling with such a tight tolerance. Targeting to have about 50% memory utilisation for workloads like this is often optimal to allow everything to fit including any incurred overheads. Though, with HPC systems, this usually becomes a tuning problem to get the largest problem size to fit onto the available resources.\nIf increasing the node count doesn\u2019t help here (and note that custatevec requires a power of 2 number of MPI ranks), then feel free to let us know and we can see if something else is going on.", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/21"}, "21": {"author": "QuantumMan", "date": "1701141493515", "content": "Hi @mlxd  - it used to work with the instructions you shared above - and it somehow it stopped working. I am sure i messsed up soemthing in. my setup\u2026 i started clean, and i still see the same problem - anythoughts on what could be the issue?\n/global/homes/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages/pennylane_lightning_gpu/lightning_gpu.py:112: UserWarning: [/global/u1/p/prmantha/pennylane-lightning-gpu/pennylane_lightning_gpu/src/util/cuda_helpers.hpp][Line:603][Method:getGPUCount]: Error in PennyLane Lightning: no CUDA-capable device is detected\n  warn(str(e), UserWarning)\n/global/homes/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages/pennylane_lightning_gpu/lightning_gpu.py:112: UserWarning: [/global/u1/p/prmantha/pennylane-lightning-gpu/pennylane_lightning_gpu/src/util/cuda_helpers.hpp][Line:603][Method:getGPUCount]: Error in PennyLane Lightning: no CUDA-capable device is detected\n  warn(str(e), UserWarning)\n/global/homes/p/prmantha/.local/perlmutter/python-3.10/lib/python3.10/site-packages/pennylane_lightning_gpu/lightning_gpu.py:994: RuntimeWarning: \n            !!!#####################################################################################\n            !!!\n            !!! WARNING: INSUFFICIENT SUPPORT DETECTED FOR GPU DEVICE WITH `lightning.gpu`\n            !!!          DEFAULTING TO CPU DEVICE `lightning.qubit`\n            !!!\n            !!!#####################################################################################\n            \n  warn(\n\nDetails\n\nPlatform info:           Linux-5.14.21-150400.24.81_12.0.86-cray_shasta_c-x86_64-with-glibc2.31\nPython version:          3.10.12\nNumpy version:           1.23.5\nScipy version:           1.11.3\nInstalled devices:\n- default.gaussian (PennyLane-0.32.0)\n- default.mixed (PennyLane-0.32.0)\n- default.qubit (PennyLane-0.32.0)\n- default.qubit.autograd (PennyLane-0.32.0)\n- default.qubit.jax (PennyLane-0.32.0)\n- default.qubit.tf (PennyLane-0.32.0)\n- default.qubit.torch (PennyLane-0.32.0)\n- default.qutrit (PennyLane-0.32.0)\n- null.qubit (PennyLane-0.32.0)\n- lightning.qubit (PennyLane-Lightning-0.32.0)\n- lightning.gpu (PennyLane-Lightning-GPU-0.32.0)\n\n\nHere is the nvcc version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Tue_May__3_18:49:52_PDT_2022\nCuda compilation tools, release 11.7, V11.7.64\nBuild cuda_11.7.r11.7/compiler.31294372_0\n", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/22"}, "22": {"author": "QuantumMan", "date": "1701220033423", "content": "Figured out the problem\u2026 its the salloc args i didn\u2019t provide properly on the hpc machine. Thanks for all your support.", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/23"}, "23": {"author": "CatalinaAlbornoz", "date": "1701231533526", "content": "I\u2019m glad to see that you figured out the issue @QuantumMan !\nWe have a very small survey for PennyLane v0.33 1, and it would be awesome if you\u2019d give us some feedback and tell us about your needs. Thank you! 1", "link": "https://discuss.pennylane.ai//t/lightning-gpu-failing-on-multi-node-multi-gpus/3557/24"}}