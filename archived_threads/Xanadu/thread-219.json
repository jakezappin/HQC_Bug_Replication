{"0": {"author": "NikSchet", "date": "1612541432895", "content": "Dear all so i have this code\nBlocks=0\n\n@qml.qnode(dev)\ndef circuit(weights, x=None):\n    for i in range(blocks):\n        AngleEmbedding(x, wires = range(n_qubits))\n        StronglyEntanglingLayers(weights[i], wires = range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n\n# variational quantum classifier\ndef variational_classifier(theta, x=None):\n    weights = theta[0]\n    bias = theta[1]\n    return circuit(weights, x=x) + bias\n\n# draw random quantum node weights\ntheta_weights = strong_ent_layers_uniform(layers, n_qubits, seed=42)\ntheta_bias = 0.0\ntheta_init = (theta_weights, theta_bias) # initial weights\n\n\n\nI would like to find a way to increase number of blocks but i have problem defining weights. Is there an easy way to do this?\n\n\nIt would make more sense each block to have different weights\n\n\nAlso is this similar to data-reuploading technique?\n\n\np.s. does the qml.enable_tape() make any difference in this sketch?\nNot sure if it helps but i use this code to update weights:\nfor it, batch_index in enumerate(chain(*(n_epochs * [X_batches]))):\n    # Update the weights by one optimizer step\n    batch_cost = \\\n        lambda theta: cost(theta, X_train[batch_index], e_train[batch_index])\n    theta = pennylane_opt.step(batch_cost, theta)\n\nMany thanks!!\n\n\n Solved by sjahangiri in post #10 \n\n\n                Hi @NikSchet. \nRegarding the errors you get in the code you copied above, there are few small issues, such as extra ) in qml.Rot(x, wires = range(n_qubits))) and the way that the qml.Rot parameters are defined. A working version of the code with some corrections is copied in the following. Please le\u2026\n              \n", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/1"}, "1": {"author": "antalszava", "date": "1612474523142", "content": "Hi @NikSchet,\nThanks for the question! \n1 & 2.\n\nI would like to find a way to increase number of blocks but i have problem defining weights. Is there an easy way to do this?\n\n\nIt would make more sense each block to have different weights\n\nSure! One way would be to create a list of weights where the seed is dependent on the block number:\ntheta_weights = [strong_ent_layers_uniform(layers, n_qubits, seed=i+1) for i in range(blocks)]\n\nThis will produce different weights for each block. Another approach is to simply leave out seeding and just depend on randomness.\n3.\n\nAlso is this similar to data-reuploading technique?\n\nThe structure is somewhat similar, however, in the multi-qubit case, the circuit with entanglement proposed in P\u00e9rez-Salinas et al. (2019) 1 involves applying CZ gates instead of CNOT gates for creating entanglement, and omits the entanglers for the last layer. You could check out passing the imprimitive=qml.CZ keyword argument to StronglyEntanglingLayers to make this change.\nAlso, it\u2019s worth noting that the cost function studied in the paper (that is also showcased in the Data-reuploading classifier demonstration 1) was related to the fidelity of the final state of the circuit.\n+1\n\np.s. does the qml.enable_tape() make any difference in this sketch?\n\nqml.enable_tape() is the remnant of switching to the new core of PennyLane that is now the default since our newest release that was published this week. If you\u2019ve updated to that version, then calling this function doesn\u2019t affect the use of PennyLane. Otherwise, you may experience better performance when using the new core.\nNote on StronglyEntanglingLayers\nWhen using StronglyEntanglingLayers, the number of layers is specified inherently when passing the weights (denoted as L in the docs 1): the shape of the weights will determine how many layers of StronglyEntanglingLayer to apply (so there will be L many layers applied per block).\nHope this helps!2 Replies1", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/2"}, "2": {"author": "NikSchet", "date": "1612567410308", "content": "Thank you very much for the detailed answer! I will try to implement the changes. especially the fidelity cost function.", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/3"}, "3": {"author": "NikSchet", "date": "1612605522114", "content": "\n\n\n antalszava:\n\nimprimitive=qml.CZ\n\n\nSo i used the `imprimitive=qml.CZ with worst results than using qml.CNOT, actually it didnt work and the network failed to capture an easy pattern . I guess it is a commutator problem with RX in the angle embedding? Is there any other explanation? Thanks in advance", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/4"}, "4": {"author": "antalszava", "date": "1612798133637", "content": "Hi @NikSchet,\nYes, that will most likely be due to the fact that general single-qubit rotations were considered in the original proposal.\nThere could be two possibilities:\n\nTrying with rotation='Y' when using AngleEmbedding.\nIf the results are still poor, turning to layers with more general single-qubit rotations (qml.Rot). For this, the previously linked demonstration could give a good starting point.\n\nHope this helps!1", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/5"}, "5": {"author": "NikSchet", "date": "1612807429754", "content": "@antalszava Thank you very much for the reply indeed if i try with Ry rotations and CZ everything works great. I will try it the general gml.rot as data embedding layer!\nI also have some more questions regarding the  variational classifier code.\n\n\nHow about probability measurement instead of expectation value?\n\n\nDoes it make sense to add Hadamard gates before the angle embedding layer? So far my simulations showed no significance difference\n\n\nThanks in advance", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/6"}, "6": {"author": "antalszava", "date": "1612821920248", "content": "Hi @NikSchet,\nGlad that it worked out!  Would be curious to see if you observe any differences with qml.Rot.\nGood questions! When switching to outputting probabilities, some care should be taken as the output dimension of the QNode will change. Maybe probabilities could be used to define a cost function that outputs a single scalar.\nWe haven\u2019t really delved into trying out these possibilities and their effectiveness will most probably also depend on your specific dataset. Would nonetheless be interesting to hear your findings on how such changes could influence training! 2", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/7"}, "7": {"author": "NikSchet", "date": "1613125181876", "content": "@antalszava\nThanks! i already tried everything and it seems that qml.ROT offers more rich prediction grid, and needs slightly more training, but i will come back with more conclusive evidence. (i will try the probabilities but i need to crack some math formulas first for the cost function)\nMeantime i try to automate and made more compact the new code and i am again slightly confused with how to set weights on this cause i get many errors:\n@qml.qnode(dev)\ndef circuit(weights, x):\nqml.Hadamard(wires = range(n_qubits))\nfor i in range(blocks):\n\n    qml.Rot(x, wires = range(n_qubits)))\n    StronglyEntanglingLayers(weights[i], wires = range(n_qubits),imprimitive=entangler)\nreturn qml.expval(qml.PauliZ(0))\n\n# draw random quantum node weights\ntheta_weights = [strong_ent_layers_uniform(layers, n_qubits, seed=randomseed+i+1) for i in range(blocks)]\ntheta_bias = 0.0\ntheta_init = (theta_weights, theta_bias) # initial weights\n\nTo be honest setting weights for each network is the hardest thing in Pennylane for me    please help me on this one too. Thanks in advance!", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/8"}, "8": {"author": "NikSchet", "date": "1613126199548", "content": "Moreover I am trying 3 different classical data scaling (normalization) : (-\u03c0,\u03c0), (0,2\u03c0), (0,\u03c0)\nIt looks like the (0,2\u03c0) is worst choice even though it is similar to (-\u03c0,\u03c0)  and for sure better than (0,\u03c0). Is there any valid explanation?", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/9"}, "9": {"author": "sjahangiri", "date": "1613169719515", "content": "Hi @NikSchet.\nRegarding the errors you get in the code you copied above, there are few small issues, such as extra ) in qml.Rot(x, wires = range(n_qubits))) and the way that the qml.Rot parameters are defined. A working version of the code with some corrections is copied in the following. Please let us know if you have any questions. Could you please also send us a minimal working example of your code that gives different results for the classical data scaling you tried? Thanks.\nimport pennylane as qml\nfrom pennylane.templates.layers import StronglyEntanglingLayers\nfrom pennylane.init import strong_ent_layers_uniform\n\nblocks = 3 \nn_qubits = 3\nlayers = 2\nentangler = qml.CNOT \nrandomseed = 42\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev) \ndef circuit(weights, x, y, z): \n    qml.Hadamard(wires=0) \n    for i in range(blocks): \n        qml.Rot(x, y, z, wires=0)\n        StronglyEntanglingLayers(weights[i], wires=range(n_qubits), imprimitive=entangler) \n    return qml.expval(qml.PauliZ(0)) \n \n# draw random quantum node weights \ntheta_weights = [strong_ent_layers_uniform(layers, n_qubits, seed=randomseed+i+1) for i in range(blocks)] \n\ncircuit(theta_weights, 0.1, 0.2, 0.3)Solution1", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/10"}, "10": {"author": "NikSchet", "date": "1613212974340", "content": "Thank you very much for the  fast reply . Regarding scaling\n\nin simple datasets i see no major difference with scaling\nDifference is more noticeable in complex unbalanced, highly noise datasets. On second thought i believe that is not about scaling intervals (-\u03c0,\u03c0 - 0,2\u03c0 - 0,\u03c0 ) it is about how you approximate a function best.\n\nAnyway for simple datasets and for 3 different scalings  as mentioned above the respective codes can be found here (naming of the file denotes  the  scaling interval). Again no major differences are noticed\n(-\u03c0,\u03c0 ) [VCDR_ST2_-pi_pi.ipynb] (https://github.com/nsansen/Quantum-Machine-Learning/blob/main/VCDR_ST2_-pi_pi.ipynb 2)\n(0,2\u03c0 ) VCDR_ST2_0_2pi.ipynb\n(0,\u03c0 ) VCDR_ST2_0_pi.ipynb", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/11"}, "11": {"author": "NikSchet", "date": "1613222708741", "content": "@sjahangiri Again thank you very much . Does this code make sense to you? i am not sure if i applied weights correctly to the circuit (please ignore non essential code lines. The code seems to run but i dont really get good results, not sure if there is a problem with how i set the weights (same code runs perfectly with single RX rotations instead of qml.ROT). Also does the placement of the hadamard gates make sense? or should i put them in the loop? \n@qml.qnode(dev)\ndef circuit(weights, x=None):\n    qml.Hadamard(wires=0)\n    qml.Hadamard(wires=1)\n    for i in range(blocks):\n        qml.Rot(theta[2], theta[3], theta[4], wires=0)\n        qml.Rot(theta[5], theta[6], theta[7], wires=1)\n        StronglyEntanglingLayers(weights[i], wires = range(n_qubits),imprimitive=entangler)\n    return qml.expval(qml.PauliZ(0))\n\n# variational quantum classifier\ndef variational_classifier(theta, x=None):\n    weights = theta[0]\n    bias = theta[1]\n    return circuit(weights, x=x) + bias\n\n# draw random quantum node weights\ntheta_weights = [strong_ent_layers_uniform(layers, n_qubits, seed=randomseed+i+1) for i in range(blocks)]\ntheta_bias = 0.0\ntheta_init = (theta_weights, theta_bias,0.1,0.2,0.3,0.1,0.2,0.3) # initial weights\n\ndef cost(theta, X, expectations):\n    e_predicted = \\\n        np.array([variational_classifier(theta, x=x) for x in X])\n    loss = np.mean((e_predicted - expectations)**2)    \n    return loss\n\n\ndef accuracy(labels, predictions):\n\n    loss = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / len(labels)\n\n    return loss\n\n\n# convert classes to expectations: -1 to -1, 1 to +1\ne_train = np.empty_like(y_train)\ne_train[y_train == -1] = -1\ne_train[y_train == 1] = +1\n# calculate numbe of batches\nbatches = len(X_train) // batch_size\n\n# train the variational classifier\ntheta = theta_init\n\n\n# split training data into batches\nX_batches = np.array_split(np.arange(len(X_train)), batches)\n\n\n\nfor it, batch_index in enumerate(chain(*(n_epochs * [X_batches]))):\n    # Update the weights by one optimizer step\n    batch_cost = \\\n        lambda theta: cost(theta, X_train[batch_index], e_train[batch_index])\n    theta = opt.step(batch_cost, theta)\n", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/12"}, "12": {"author": "sjahangiri", "date": "1613488815686", "content": "Hi @NikSchet,\nThe  placement of the Hadamard gates looks fine. I could not run the code because the train and the test data are not provided. However, I noticed that the x parameter passed to the circuit is not used anywhere. Also, you can use different parameters for the qml.ROT gates and consider taking them out from theta and pass them separately to the circuit. Hope it helps.", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/13"}, "13": {"author": "NikSchet", "date": "1613500519774", "content": "I am still not able to make it work, and when it works it fails. one such code with steady angles in qml.ROT is this one. and i am not able to modify it.\nThis should be a standalone working code with moons dataset. Thanks in advance for your time and help!!\nimport pennylane as qml\nimport pandas as pd\nfrom pennylane import numpy as np\nfrom pennylane.templates.layers import StronglyEntanglingLayers\nfrom pennylane.init import strong_ent_layers_uniform\nfrom pennylane.optimize import GradientDescentOptimizer\nfrom sklearn.datasets import make_moons , make_circles\nfrom sklearn.preprocessing import StandardScaler , minmax_scale\nfrom itertools import chain\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\nblocks=1\nlayers=2\nbatch_size = 16\nn_epochs = 20\ntest_size=0.2 #(train/test split)\nlearning_rate = 0.6\nentangler = qml.CNOT\nopt = GradientDescentOptimizer(stepsize=learning_rate)\nn_qubits =2 \ndev = qml.device(\"default.qubit\", wires=n_qubits)\nrandomseed = 1\n\nX, y = make_moons(n_samples=400, noise=0)\nX = minmax_scale(X, feature_range=(-np.pi, np.pi))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\nfrom sklearn.metrics import roc_auc_score\n# quantum circuit\n#\n#\n# draw random quantum node weights\ntheta_weights = [strong_ent_layers_uniform(layers, n_qubits, seed=randomseed+i+1) for i in range(blocks)]\ntheta_bias = 0.0\nchi1=0.1\npsi1=0.2\nzet1=0.1\nchi2=0.1\npsi2=0.2\nzet2=0.1\ntheta_init = (theta_weights, theta_bias,chi1,psi1,zet1,chi2,psi2,zet2) # initial weights\n\n@qml.qnode(dev)\ndef circuit(weights):\n    qml.Hadamard(wires=0)\n    qml.Hadamard(wires=1)\n    for i in range(blocks):\n        qml.Rot(chi1, psi1, zet1, wires=0)\n        qml.Rot(chi2, psi2, zet2, wires=1)\n        StronglyEntanglingLayers(weights[i], wires = range(n_qubits),imprimitive=entangler)\n    return qml.expval(qml.PauliZ(0))\n\n# variational quantum classifier\ndef variational_classifier(theta):\n    weights = theta[0]\n    bias = theta[1]\n    return circuit(weights) + bias + chi1 + psi1 + zet1 + chi1 + psi2 + zet2\n\n\n\n# train the variational classifier\ntheta = theta_init\n\ndef cost(theta, X, expectations):\n    e_predicted = \\\n        np.array([variational_classifier(theta) for x in X])\n    loss = np.mean((e_predicted - expectations)**2)    \n    return loss\n\ndef accuracy(labels, predictions):\n\n    loss = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / len(labels)\n\n    return loss\n\n\n# calculate numbe of batches\nbatches = len(X_train) // batch_size\n# split training data into batches\nX_batches = np.array_split(np.arange(len(X_train)), batches)\nlossplot = []\naucrocplot = []\naccuracytrainplot = []\naccuracytestplot = []\n\nfor it, batch_index in enumerate(chain(*(n_epochs * [X_batches]))):\n    # Update the weights by one optimizer step\n    batch_cost = \\\n        lambda theta: cost(theta, X_train[batch_index],y_train[batch_index])\n    theta = opt.step(batch_cost, theta)\n   \n    \n    if it % 10 == 0:\n        expectations = np.array([variational_classifier(theta) for x in X_train])\n        prob_class_one = (expectations + 1) / 2.0\n        prob_class_one = pd.DataFrame.from_dict(prob_class_one)\n        prob_class_one = prob_class_one.iloc[:, :]\n        prob_class_one = prob_class_one[0].apply(lambda x: -1 if x <= 0.5 else 1)\n        prob_class_onet = prob_class_one.to_numpy()\n           \n        expectations = np.array([variational_classifier(theta) for x in X_test])\n        prob_class_one = (expectations + 1) / 2.0\n        prob_class_one = pd.DataFrame.from_dict(prob_class_one)\n        prob_class_one = prob_class_one.iloc[:, :]\n        prob_class_one = prob_class_one[0].apply(lambda x: -1 if x <= 0.5 else 1)\n        prob_class_one = prob_class_one.to_numpy()\n        #\n        #\n        #--------- GRID PLOT START\n        #\n        #\n        plt.figure()\n        cm = plt.cm.RdBu\n        fig= plt.figure(figsize=(5,5))\n        xx, yy = np.meshgrid(np.linspace(-np.pi, np.pi, 15), np.linspace(-np.pi, np.pi, 15))\n        X_grid = [np.array([x,y]) for x, y in zip(xx.flatten(), yy.flatten())]\n        predictions_grid = np.array([variational_classifier(theta) for x in X_grid])\n        zminus = (predictions_grid + 1.0) / 2.0\n        zminus = predictions_grid\n        Z=np.reshape(zminus, xx.shape)\n# plot decision regions\n        cnt = plt.contourf(xx, yy,Z, levels=np.arange(-1, 1., 0.1), cmap=cm, alpha=0.8, extend=\"both\")\n        plt.contour(xx, yy,Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\n        plt.show()\n        #\n        #\n        #--------- GRID PLOT END\n        #\n        #\n    #print(\"Acc test\",metrics.accuracy_score(y_test, prob_class_one))\n    #print(metrics.confusion_matrix(y_test, prob_class_one))\n        lossplot.append(cost(theta, X_train[batch_index], y_train[batch_index]))\n        aucrocplot.append(roc_auc_score(y_test, prob_class_one))\n        accuracytrainplot.append(metrics.accuracy_score(y_train, prob_class_onet))\n        accuracytestplot.append(metrics.accuracy_score(y_test, prob_class_one))\n#    print(\"It\",it+1,\"out of\",len(X_batches) *n_epochs)\n        print(\"It\",it+1,\"out of\",len(X_batches) *n_epochs, \"loss: \",cost(theta, X_train[batch_index], y_train[batch_index]),\n        \" : Acc train: \",round(metrics.accuracy_score(y_train, prob_class_onet),2),\n        \" : Acc test : \",round(metrics.accuracy_score(y_test, prob_class_one),3),\n        \" : Auc : \",round(roc_auc_score(y_test, prob_class_one),3)\n        )\n        if metrics.accuracy_score(y_train, prob_class_onet) >= 0.97:\n                break\n\nplt.plot(lossplot) #lets plot the second line\nplt.ylabel('Loss')\nplt.show()\n\nplt.subplot(2,1,1)\nplt.plot(accuracytrainplot,'r',label=\"train\")\nplt.plot(accuracytestplot,'b',label=\"test\")\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(aucrocplot) #lets plot the second line\nplt.ylabel('auc roc score')\nplt.show()", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/14"}, "14": {"author": "sjahangiri", "date": "1613518787651", "content": "Hi @NikSchet,\nThanks for providing the code. It seems to me that the features X are not used in the circuit at all. Is there a reason for that? In other words, the e_predicted values that are computed in the function loss do not use any information about the sample features. For more information, you can see two examples in this tutorial 1. I suggest to start from one of the examples provided in the tutorial and then try to modify the circuit part only, by using your desired gates as you have above. Please let me know if you need any help with building the new circuit. Thanks.", "link": "https://discuss.pennylane.ai//t/variational-classifier-problem-with-weights/809/15"}}