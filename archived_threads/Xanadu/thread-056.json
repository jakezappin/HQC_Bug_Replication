{"0": {"author": "Hemant_Gahankari", "date": "1603980396831", "content": "Hi ,\nI am trying to use amplitude embedding to encode 4 features as follows ,\nfrom sklearn.datasets import load_iris\n\nfrom sklearn.utils import shuffle\n\n# import some data to play with\n\niris = datasets.load_iris()\n\nX = iris.data[:, :]  # we only take the first two features.\n\nY = iris.target\n\ntrainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrainy = tf.one_hot(trainy, depth=3)\n\ntesty = tf.one_hot(testy, depth=3)\n\nn_qubits = 2\n\nlayers = 1\n\ndata_dimension = 3\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\n\ndef qnode(inputs, weights):\n\n    qml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True)\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweight_shapes = {\"weights\": (layers,n_qubits,3)}\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(n_qubits,activation='relu',input_dim=4))\n\nmodel.add(qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits))\n\nmodel.add(tf.keras.layers.Dense(data_dimension, activation='softmax'))\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=opt,metrics=[\"accuracy\"])\n\nhistory = model.fit(trainX, trainy, validation_data=(trainX, trainy), epochs=30, batch_size=5)\n\nI get error while model.fit(), ValueError: \u2018features\u2019 must be of shape (4,); got (2,). Use the \u2018pad\u2019 argument for automated padding.\nI have four features , number of wires are 2 , so why padding is required.\nIf I put pad=0.\nI get , AttributeError: \u2018float\u2019 object has no attribute \u2018val\u2019\nAny suggestion ?", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/1"}, "1": {"author": "antalszava", "date": "1604012022082", "content": "Hi @Hemant_Gahankari,\nThanks so much for your question! \nWhen defining a quantum function (a function that is passed to a QNode), non-differentiable parameters (such as inputs in this case) require a default value to be defined. This is how differentiable parameters of a quantum function are being tracked when creating a QNode.\nIn this specific case, the definition of the qnode function could be changed to def qnode(weights, inputs=None). This way weights is marked as differentiable and inputs as non-differentiable.\nFurther to this, there is an upcoming QNode using the new QuantumTape class 2. This is at the moment in experimental phase and compatibility with templates (such as AmplitudeEmbedding) is underway! \nHope this helps!1", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/2"}, "2": {"author": "Hemant_Gahankari", "date": "1604067724791", "content": "Hi ,\nI made the changes , the code looks like below ,\nfrom sklearn.datasets import load_iris\n\nfrom sklearn.utils import shuffle\n\n# import some data to play with\n\niris = datasets.load_iris()\n\nX = iris.data[:, :]  # we only take the first two features.\n\nY = iris.target\n\ntrainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrainy = tf.one_hot(trainy, depth=3)\n\ntesty = tf.one_hot(testy, depth=3)\n\nn_qubits = 2\n\nlayers = 1\n\ndata_dimension = 3\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\n\ndef qnode(weights, inputs=None):\n\n    qml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True,pad=0.)\n\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweight_shapes = {\"weights\": (layers,n_qubits,3)}\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(n_qubits,activation='relu',input_dim=4))\n\nmodel.add(qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits))\n\nmodel.add(tf.keras.layers.Dense(data_dimension, activation='softmax'))\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n\nfrom matplotlib import pyplot\n\n# plot loss during training\n\npyplot.subplot(211)\n\npyplot.title('Loss')\n\npyplot.plot(history.history['loss'], label='train')\n\npyplot.plot(history.history['val_loss'], label='test')\n\npyplot.legend()\n\npyplot.show()\n\n# plot accuracy during training\n\npyplot.subplot(212)\n\npyplot.title('Accuracy')\n\npyplot.plot(history.history['accuracy'], label='train')\n\npyplot.plot(history.history['val_accuracy'], label='test')\n\npyplot.legend()\n\nI am not able to understand ,\n\n\nIf I do not give pad=0. , I get an error  , ValueError: \u2018features\u2019 must be of shape (4,); got (2,).  My question is if I have 4 features, 2 qubits should be good to embed. Why is pad required.\n\n\nIf I give pad=0. , model.fit() begins but I get tensorflow:Gradients do not exist for variables [\u2018dense_10/kernel:0\u2019, \u2018dense_10/bias:0\u2019] when minimizing the loss. And I do not get good results for loss and accuracy. (This works well with angle embedding.)\n\n", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/3"}, "3": {"author": "Hemant_Gahankari", "date": "1604068238371", "content": "Hi ,\nI fixed the issue of padding , it was due to dense layer having 2 units, instead of 4. It worked with following code ,\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(4,activation='relu',input_dim=4))\n\nmodel.add(qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits))\n\nmodel.add(tf.keras.layers.Dense(data_dimension, activation='softmax'))\n", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/4"}, "4": {"author": "Hemant_Gahankari", "date": "1604068417344", "content": "Thank you folks , PennyLane rocks  and very happy with the swift responses from you all, I finally have two end to end classification demos working with angle and amplitude embedding.2", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/5"}, "5": {"author": "Hemant_Gahankari", "date": "1604068813100", "content": "I would be very happy to share the complete code if you folks want to consider putting it up on your demos page.\nI think these will give very good start to many like me in getting up to speed easily with end to end classification example with TF-Keras with minimal data processing etc.", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/6"}, "6": {"author": "antalszava", "date": "1604069896776", "content": "Hi @Hemant_Gahankari,\nThat\u2019s really great to hear, happy that we could help! \nFor sure! You could submit code which later appears as part of the demos page 3 by following this link on How to submit a demo 3.1", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/7"}, "7": {"author": "bengeof", "date": "1616830413780", "content": "Hi I am trying to add a qnode to keras actor-critic model. It works with angle embedding but with amplitude embedding facing some difficulty. Hoping to get some help from the community. Thanks\nCode below\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pennylane as qml\nimport keras_metrics\n\nConfiguration parameters for the whole setup\nseed = 42\ngamma = 0.99  # Discount factor for past rewards\nmax_steps_per_episode = 10000\nenv = gym.make(\u201cCartPole-v0\u201d)  # Create the environment\nenv.seed(seed)\neps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\nn_qubits = 2\ndev = qml.device(\u201cdefault.qubit\u201d, wires=n_qubits)\n@qml.qnode(dev)\ndef qnode(weights, inputs=None):\nqml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits), normalize=True)\nqml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\nreturn [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\nn_layers = 1\nweight_shapes = {\u201cweights\u201d: (n_layers,n_qubits,3)}\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nnum_inputs = 4\nnum_actions = 2\nnum_hidden = 4\ninputs = layers.Input(shape=(num_inputs,))\ninput1 = tf.keras.layers.Dense(4,activation=\u2018relu\u2019,input_dim=4, trainable=False)(inputs)\ncommon = layers.Dense(num_hidden, activation=\u201crelu\u201d)(qlayer(input1))\naction = layers.Dense(num_actions, activation=\u201csoftmax\u201d)(common)\ncritic = layers.Dense(1)(common)\nmodel = keras.Model(inputs=inputs, outputs=[action, critic])\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nhuber_loss = keras.losses.Huber()\naction_probs_history = \ncritic_value_history = \nrewards_history = \nrunning_reward = 0\nepisode_count = 0\nwhile True:  # Run until solved\nstate = env.reset()\nepisode_reward = 0\nwith tf.GradientTape() as tape:\nfor timestep in range(1, max_steps_per_episode):\n# env.render(); Adding this line would show the attempts\n# of the agent in a pop up window.\n        state = tf.convert_to_tensor(state)\n        state = tf.expand_dims(state, 0)\n\n        # Predict action probabilities and estimated future rewards\n        # from environment state\n        action_probs, critic_value = model(state)\n        critic_value_history.append(critic_value[0, 0])\n\n        # Sample action from action probability distribution\n        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n        action_probs_history.append(tf.math.log(action_probs[0, action]))\n\n        # Apply the sampled action in our environment\n        state, reward, done, _ = env.step(action)\n        rewards_history.append(reward)\n        episode_reward += reward\n\n        if done:\n            break\n\n    # Update running reward to check condition for solving\n    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n    # Calculate expected value from rewards\n    # - At each timestep what was the total reward received after that timestep\n    # - Rewards in the past are discounted by multiplying them with gamma\n    # - These are the labels for our critic\n    returns = []\n    discounted_sum = 0\n    for r in rewards_history[::-1]:\n        discounted_sum = r + gamma * discounted_sum\n        returns.insert(0, discounted_sum)\n\n    # Normalize\n    returns = np.array(returns)\n    returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n    returns = returns.tolist()\n\n    # Calculating loss values to update our network\n    history = zip(action_probs_history, critic_value_history, returns)\n    actor_losses = []\n    critic_losses = []\n    for log_prob, value, ret in history:\n        # At this point in history, the critic estimated that we would get a\n        # total reward = `value` in the future. We took an action with log probability\n        # of `log_prob` and ended up recieving a total reward = `ret`.\n        # The actor must be updated so that it predicts an action that leads to\n        # high rewards (compared to critic's estimate) with high probability.\n        diff = ret - value\n        actor_losses.append(-log_prob * diff)  # actor loss\n\n        # The critic must be updated so that it predicts a better estimate of\n        # the future rewards.\n        critic_losses.append(\n            huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n        )\n\n    # Backpropagation\n    loss_value = sum(actor_losses) + sum(critic_losses)\n    grads = tape.gradient(loss_value, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # Clear the loss and reward history\n    action_probs_history.clear()\n    critic_value_history.clear()\n    rewards_history.clear()\n\n# Log details\nepisode_count += 1\nif episode_count % 10 == 0:\n    template = \"running reward: {:.2f} at episode {}\"\n    print(template.format(running_reward, episode_count))\n\nif running_reward > 195:  # Condition to consider the task solved\n    print(\"Solved at episode {}!\".format(episode_count))\n    break\n\nError : Tensor conversion requested dtype complex128 for Tensor with dtype float32: <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.        , 0.42925665, 0.8368186 , 0.33981368], dtype=float32)>\nThe classical version : Actor Critic Method 1\nThanks in Advance ", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/8"}, "8": {"author": "bengeof", "date": "1616830825160", "content": "Actually I get the same error trying to reproduce your code\nValueError: Tensor conversion requested dtype complex128 for Tensor with dtype float32: <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.        , 0.24570765, 0.        , 0.96934396], dtype=float32)>", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/9"}, "9": {"author": "nathan", "date": "1617036022579", "content": "Hi @bengeof, welcome to the forum! \nTwo small requests that will better let us help you:\n\n\nYour issue seems to be unrelated to the original subject/content of this post. Would you be able to create a standalone post for the issue you are experiencing (type error when using KerasLayers). This will also allow other users to see/find the question and any potential answers more easily\n\n\nI tried to look into your code, and was able to eventually reproduce the error, but there is a lot going on there (and a lot of formatting errors). This means I had to make some guesses about missing values and indentation, which I\u2019d like to verify before digging into debugging. Would you be able to i) strip down your example to a minimal (non-)working version, and ii) please put everything in a single code-block (some of it is now in text, some in code)?\n\n\nThanks ", "link": "https://discuss.pennylane.ai//t/using-amplitideembedding-template/646/10"}}