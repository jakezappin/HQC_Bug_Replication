{"0": {"author": "Muhammad_Kashif", "date": "1624277050933", "content": "Hello,\nHow can I use the pennylane optimizers in Hybrid neural network? the model works fine with tensorflow optimizers like Adam, SGD etc., but when I try and run the same model with pennylane optimizers like QNGOptimizer etc, the following error pops up.\nValueError: Could not interpret optimizer identifier: <pennylane.optimize.qng.QNGOptimizer object at 0x7f619aa4c810>\nthe same error pops up for other optimizers like AdamOptimizer etc as well. Can some one please direct me how can I replace the optimizer with pennylane optimizer in this  1 tutorial on hybrid NN.\nThank you\u2026", "link": "https://discuss.pennylane.ai//t/pennylane-optimizer-with-hybrid-qnn/1135/1"}, "1": {"author": "nathan", "date": "1624301935795", "content": "Hi @Muhammad_Kashif, thanks for your question \nAll of the optimizers that are included inside PennyLane are meant to be used with the PennyLane-provided version of numpy, i.e., the version of numpy you get when you do from pennylane import numpy as np. So if you want to port a tutorial to work with PennyLane\u2019s built-in optimizers, you\u2019ll have to rework that tutorial\u2019s code from TensorFlow or PyTorch (though a great many tutorials are already written in numpy).\nNote that some built-in optimizers, like AdamOptimizer should work out of the box as long as your hybrid model is coded in PennyLane-provided NumPy. Others, like QNGOptimizer are more \u201cquantum\u201d than \u201chybrid\u201d, i.e., they are designed to work with quantum circuits, but may not necessarily extend to full hybrid models.", "link": "https://discuss.pennylane.ai//t/pennylane-optimizer-with-hybrid-qnn/1135/2"}, "2": {"author": "Muhammad_Kashif", "date": "1626080227695", "content": "Hi,\nSorry for asking in abit older thread, but my question was related to this topic.\nCan we use pennylane optimizers like AdamOptimizer, GradientDescentOptimizer etc with keras model.compile, because when I try to use these optimizers with model.compile, I get the following error:\nCould not interpret optimizer identifier: <pennylane.optimize.gradient_descent.GradientDescentOptimizer object at 0x000002CE1E571EE0>\n\nPS: I am using numpy from pennylane i,.e., from pennylane import numpy as np\nSecondly, are these optimizers (available in pennylane) particularly  AdamOptimizer and GradientDescentOptimizer are different to that tensorflow optimizers (Adm and SGD).\nThanks for the help.", "link": "https://discuss.pennylane.ai//t/pennylane-optimizer-with-hybrid-qnn/1135/3"}, "3": {"author": "Tom_Bromley", "date": "1626093323997", "content": "Hey @Muhammad_Kashif!\n\nCan we use pennylane optimizers like AdamOptimizer , GradientDescentOptimizer etc with keras model.compile\n\nThe built-in PennyLane optimizers are designed for the NumPy/Autograd interface and are not compatible with TensorFlow. However, gradient descent, Adam, and many common optimizers are already available in TensorFlow and can be used with a Keras model.\nCheck out the model.compile method for more details on how to specify the TensorFlow-based optimizer. This should be as simple as setting optimizer='adam' if you are happy with the default settings of the TF Adam optimizer.\n\nSecondly, are these optimizers (available in pennylane) particularly AdamOptimizer and GradientDescentOptimizer are different to that  tensorflow optimizers (Adm and SGD) .\n\nThey are implemented differently (i.e., TensorFlow vs Autograd compatibility), but use the same underlying algorithm.", "link": "https://discuss.pennylane.ai//t/pennylane-optimizer-with-hybrid-qnn/1135/4"}, "4": {"author": "Muhammad_Kashif", "date": "1624277050933", "content": "Hello,\nHow can I use the pennylane optimizers in Hybrid neural network? the model works fine with tensorflow optimizers like Adam, SGD etc., but when I try and run the same model with pennylane optimizers like QNGOptimizer etc, the following error pops up.\nValueError: Could not interpret optimizer identifier: <pennylane.optimize.qng.QNGOptimizer object at 0x7f619aa4c810>\nthe same error pops up for other optimizers like AdamOptimizer etc as well. Can some one please direct me how can I replace the optimizer with pennylane optimizer in this  1 tutorial on hybrid NN.\nThank you\u2026", "link": "https://discuss.pennylane.ai//t/pennylane-optimizer-with-hybrid-qnn/1135/5"}}