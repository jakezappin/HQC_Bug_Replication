{"0": {"author": "Daniel63656", "date": "1622540699683", "content": "Hi I am training a hybrid network with a QNode as PyTorch-Layer. Transfer learning is no problem but I can\u2019t train the whole network. It seems that the pennylane/torch interface tries to differentiate the inputs which results in this error. I inserted a print in the pennyLane/torch.py module to confirm this.\nMy StackTrace:\nValueError                                Traceback (most recent call last)\n<ipython-input-5-846e188427fd> in <module>\n     67 optimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n     68 \n---> 69 qHist = train(device, hybrid_model, optimizer, criterion, 1, dataloader_train, dataloader_test)\n     70 \n     71 plotTrainingResults([cHist, qHist], [\"classical\", \"quantum\"])\n\n~\\Hahn_schickard\\jupyter\\pyTorch_utils.py in train(device, model, optimizer, criterion, epochs, training, testing)\n     35             correct += (predicted == labels).float().sum()\n     36             loss = criterion(outputs, labels)\n---> 37             loss.backward()\n     38             optimizer.step()\n     39             batch_percentage = i*20//number_batches\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    243                 create_graph=create_graph,\n    244                 inputs=inputs)\n--> 245         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    246 \n    247     def register_hook(self, hook):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    145     Variable._execution_engine.run_backward(\n    146         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--> 147         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    148 \n    149 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\n     87     def apply(self, *args):\n     88         # _forward_cls is defined by derived class\n---> 89         return self._forward_cls.backward(self, *args)  # type: ignore\n     90 \n     91 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in backward(ctx, dy)\n    176         if dy.is_cuda:\n    177             cuda_device = dy.get_device()\n--> 178         vjp = dy.view(1, -1) @ ctx.jacobian.apply(ctx, *ctx.saved_tensors).to(dy)\n    179         vjp = torch.unbind(vjp.view(-1))\n    180         return (None,) + tuple(vjp)\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in forward(ctx_, parent_ctx, *input_)\n    124                 ctx_.dy = parent_ctx.dy\n    125                 ctx_.save_for_backward(*input_)\n--> 126                 jacobian = _evaluate_grad_matrix(\"jacobian\")\n    127                 return jacobian\n    128 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in _evaluate_grad_matrix(grad_matrix_fn)\n    109             print(ctx.args)\n    110             grad_matrix = getattr(tape, grad_matrix_fn)(\n--> 111                 device, params=ctx.args, **tape.jacobian_options\n    112             )\n    113             tape.set_parameters(ctx.all_params, trainable_only=False)\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\qubit_param_shift.py in jacobian(self, device, params, **options)\n    122         self._append_evA_tape = True\n    123         self._evA_result = None\n--> 124         return super().jacobian(device, params, **options)\n    125 \n    126     def parameter_shift(self, idx, params, **options):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\jacobian_tape.py in jacobian(self, device, params, **options)\n    514 \n    515         # perform gradient method validation\n--> 516         diff_methods = self._grad_method_validation(method)\n    517 \n    518         if not self._has_trainable_params(params, diff_methods):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\jacobian_tape.py in _grad_method_validation(self, method)\n    198 \n    199         if nondiff_params:\n--> 200             raise ValueError(f\"Cannot differentiate with respect to parameter(s) {nondiff_params}\")\n    201 \n    202         numeric_params = {idx for idx, g in diff_methods.items() if g == \"F\"}\n\nValueError: Cannot differentiate with respect to parameter(s) {0}\n\nI use  a qml.qnnTorchLayer with parameters being inputs, weights; exactly as described in\nhttps://pennylane.readthedocs.io/en/stable/code/api/pennylane.qnn.TorchLayer.html 1\nHere is my code:\ndef rotation_layer(params, qubits):\n    # parametrized ry, rz rotations\n    n = len(qubits)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i], wires=q)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i+n], wires=q)\n\ndef entanglement_layer(qubits):\n    n = len(qubits)\n    for i in range(0, n):\n        qml.CNOT(wires=[qubits[i], qubits[(i+1)%n]])\n\n@qml.qnode(qml.device('default.qubit', wires=4))\ndef QNode4(inputs, weights):\n    inputs = F.normalize(inputs,dim=-1,p=2)  #L2-normalization\n    N = len(inputs)\n    n = int(m.log(N,2))\n    # data encoding\n    qml.QubitStateVector(inputs, wires=range(n))\n    \n    #variational circuit\n    measureWires = range(n)\n    for w in weights:\n        rotation_layer(w, measureWires)\n        entanglement_layer(measureWires)\n\n    #measure\n    return qml.probs(wires = measureWires)\n\n# model class\nclass QNet(nn.Module):\n\n    def __init__(self):\n        super(QNet, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 16, 128)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 16)\n        n_qubits = 4\n        n_layers = 2\n        self.fc4 = qml.qnn.TorchLayer(QNode4, {\"weights\": (n_layers, n_qubits**2)})\n        self.fc5 = nn.Linear(16, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        # sqrt because output are amplitudes. This way input=output if there is only data encoding\n        x = torch.sqrt(self.fc4(x)).to(device)\n        x = self.fc5(x)\n        return x\n    \n\n#training\nhybrid_model = QNet().to(device)\nepochs = 16\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n\nqHist = train(device, hybrid_model, optimizer, criterion, 1, dataloader_train, dataloader_test)\n\nplotTrainingResults([cHist, qHist], [\"classical\", \"quantum\"])\n\nI can\u2019t find a way to make pennylane stop trying to differentiate the inputs and as far as I understand the qml.qnn.TorchLayer class, this shouldn\u2019t be happening in the first pace.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/1"}, "1": {"author": "josh", "date": "1622542244856", "content": "Hi @Daniel63656!\nI\u2019m trying to run your code now, but I\u2019m running into an issue where device and train are not defined.\nWould you be able to share a minimal non-working example that is executable as-is? That will help me better debug the issue ", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/2"}, "2": {"author": "Daniel63656", "date": "1622543661209", "content": "Thanks for your quick reply.\nHere is a working example (sorry that I have to post it like that, as a new user I can not upload files )\nimport copy\nimport math as m\nimport numpy as np\nimport pennylane as qml\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(torch.cuda.is_available())\n\n## Classical Network\n\nWe first build and train a classical network that is build in a way that a linear layer (16x16) can be swapped for a QNode later on\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 16, 128)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 16)\n        self.fc4 = nn.Linear(16, 16)\n        self.fc5 = nn.Linear(16, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = self.fc5(x)\n        return x\n    \n    # backward function (where gradients are computed) is automatically defined by pyTorch\n    \nmodel = Net()\nprint(model)\nprint(\"\")\nprint(\"parameters:\")\nparams = list(model.parameters())\nfor i in range(len(params)):\n    print(params[i].size())\n\n# training method\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 300\n\n# Training function returning training history\ndef train(device, model, optimizer, criterion, epochs, training, testing=None):\n    print('starting...')\n    since = time.time()\n    history = History()\n    \n    for epoch in range(epochs):\n        correct = 0\n        number_batches = len(training)\n\n        if (testing != None):\n            valAcc, valCost = evaluate(device, model, criterion, testing)\n            history.testAccuracy.append(valAcc)\n            history.testCost.append(valCost)\n        \n        \n        for i, data in enumerate(training):  #loop through batches\n            inputs, labels = data\n            batch_size = len(labels)\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            # forward + backward + optimize\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).float().sum()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            batch_percentage = i*20//number_batches\n            print(\"epoch %d/%d [%-20s]\" % (epoch+1,epochs,'='*batch_percentage), end ='\\r')\n            \n        # save statistics\n        accuracy = correct / (number_batches*batch_size)\n        history.trainingAccuracy.append(accuracy)\n        history.trainingCost.append(loss.item())\n        print(\"epoch {}/{} completed              Accuracy: {:.2f}\".format(epoch+1, epochs, accuracy), end ='\\r')\n\n    # get final trained results\n    finalAcc, finalCost = evaluate(device, model, criterion, training)\n    history.trainingAccuracy.append(finalAcc)\n    history.trainingCost.append(finalCost)\n    \n    if (testing != None):\n         valAcc, valCost = evaluate(device, model, criterion, testing)\n         history.testAccuracy.append(valAcc)\n         history.testCost.append(valCost)\n    \n    print(\"epoch {}/{} completed              Accuracy: {:.2f}\".format(epochs, epochs, finalAcc))\n    print(\"Training completed in {:.2f}s\".format(time.time() - since))\n    return history\n\ndef evaluate(device, model, criterion, dataloader):\n    correct = 0\n    for i, data in enumerate(dataloader):\n        inputs, labels = data\n        batch_size = len(labels)\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).float().sum()\n        loss = criterion(outputs, labels)\n    acc = correct / (len(dataloader)*batch_size)\n    return acc, loss.item()\n    \n    \n# wrapper class for training history\nclass History:\n    def __init__(self):\n        self.trainingAccuracy = list()\n        self.trainingCost = list()\n        self.testAccuracy = list()\n        self.testCost = list()\n        \ndef saveHistory(PATH, name, history):\n    save = np.asarray([history.trainingAccuracy, history.trainingCost, history.testAccuracy, history.testCost])\n    np.save(PATH + name + \".npy\", save)\n    \ndef loadHistory(PATH, name):\n    load = np.load(PATH + name + \".npy\", allow_pickle=True)\n    history = History()\n    history.trainingAccuracy = load[0]\n    history.trainingCost     = load[1]\n    history.testAccuracy     = load[2]\n    history.testCost         = load[3]\n    return history\n    \ndef plotTrainingResults(histories, labels):\n    fig, axs = plt.subplots(2,2, figsize=(12, 5))\n    plt.subplots_adjust(hspace = 0.3)\n    \n    axs[0,0].grid()\n    axs[0,1].grid()\n    axs[1,0].grid()\n    axs[1,1].grid()\n    axs[0,0].set_ylim([0, 1.1])\n    axs[0,1].set_ylim([0, 1.1])\n    axs[0,0].set_title('Training Accuracy')\n    axs[0,1].set_title('Test Accuracy')\n    axs[1,0].set_title('Training Cost')\n    axs[1,1].set_title('Test Cost')\n    lines= []\n    for hist in histories:\n        lines.append(axs[0,0].plot(hist.trainingAccuracy))\n        axs[0,1].plot(hist.testAccuracy)\n        axs[1,0].plot(hist.trainingCost)\n        axs[1,1].plot(hist.testCost)\n    \n    for ax in axs.flat:\n        ax.label_outer()\n    \n    fig.legend(labels, loc='upper left', prop={'size': 11})\n    plt.show()\n\n## MNIST Dataset\n\nbatch_size = 8\n# use None on these to load whole dataset\ntraining_size_limit = 1000\ntest_size_limit = 400\n\n\ntrf = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5), (0.5))])\n\ntraining_data = datasets.MNIST(root='./MNIST', train=True,  download=True, transform=trf)\ntest_data     = datasets.MNIST(root='./MNIST', train=False, download=True, transform=trf)\n\nif (len(training_data) == 60000 and len(test_data) == 10000):\n    print(\"data loading successful!\")\n\nif (training_size_limit != None):\n    training_data = torch.utils.data.Subset(training_data, range(training_size_limit))\nif (test_size_limit != None):\n    test_data     = torch.utils.data.Subset(test_data, range(test_size_limit))\n\n    \n# Initialize dataloaders\ndataloader_train = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\ndataloader_test  = torch.utils.data.DataLoader(test_data,     batch_size=batch_size, shuffle=True)\n\n## Training\n\nmodel = Net().to(device)\nepochs = 16\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n\ncHist = train(device, model, optimizer, criterion, epochs, dataloader_train, dataloader_test)\n\nplotTrainingResults([cHist], [\"classical\"])\n\n## Transfer Learning\n\nclass Pretrained(torch.nn.Module):\n    def __init__(self, pretrained_model, qLayer):\n        super().__init__()\n        self.__dict__ = copy.deepcopy(pretrained_model.__dict__)\n        # freeze old parameters of model\n        params = list(self.parameters())\n        for i in range(len(params)-4):\n            params[i].requires_grad = False\n        # swap layer\n        self.fc4 = qLayer\n        \n    def forward(self,x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        # sqrt because output are amplitudes. This way input=output if there is only data encoding\n        x = torch.sqrt(self.fc4(x)).to(device)\n        x = self.fc5(x)\n        return x\n\n#----------Ansatz 4----------#\n@qml.qnode(qml.device('default.qubit', wires=4), interface=\"torch\")\ndef QNode4(inputs, weights):\n    inputs = F.normalize(inputs,dim=-1,p=2)  #L2-normalization\n    N = len(inputs)\n    n = int(m.log(N,2))\n    # data encoding\n    qml.QubitStateVector(inputs, wires=range(n))\n    \n    #variational circuit\n    measureWires = range(n)\n    for w in weights:\n        rotation_layer(w, measureWires)\n        entanglement_layer(measureWires)\n\n    #measure\n    return qml.probs(wires = measureWires)\n\n\n#create new model\nn_qubits = 4\nn_layers = 2\nqLayer = qml.qnn.TorchLayer(QNode4, {\"weights\": (n_layers, n_qubits*2)})\npretrained = Pretrained(model, qLayer).to(device)\n\n#train\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(pretrained.parameters(), lr=0.01, momentum=0.8)\na4Hist = train(device, pretrained, optimizer, criterion, 5, dataloader_train)\n\n## Train the whole model\n\ndef rotation_layer(params, qubits):\n    # parametrized ry, rz rotations\n    n = len(qubits)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i], wires=q)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i+n], wires=q)\n\ndef entanglement_layer(qubits):\n    n = len(qubits)\n    for i in range(0, n):\n        qml.CNOT(wires=[qubits[i], qubits[(i+1)%n]])\n\n@qml.qnode(qml.device('default.qubit', wires=4))\ndef QNode4(inputs, weights):\n    inputs = F.normalize(inputs,dim=-1,p=2)  #L2-normalization\n    N = len(inputs)\n    n = int(m.log(N,2))\n    # data encoding\n    qml.QubitStateVector(inputs, wires=range(n))\n    \n    #variational circuit\n    measureWires = range(n)\n    for w in weights:\n        rotation_layer(w, measureWires)\n        entanglement_layer(measureWires)\n\n    #measure\n    return qml.probs(wires = measureWires)\n\n# model class\nclass QNet(nn.Module):\n\n    def __init__(self):\n        super(QNet, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 16, 128)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 16)\n        n_qubits = 4\n        n_layers = 2\n        self.fc4 = qml.qnn.TorchLayer(QNode4, {\"weights\": (n_layers, n_qubits**2)})\n        self.fc5 = nn.Linear(16, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        # sqrt because output are amplitudes. This way input=output if there is only data encoding\n        x = torch.sqrt(self.fc4(x)).to(device)\n        x = self.fc5(x)\n        return x\n    \n\n#training\nhybrid_model = QNet().to(device)\nepochs = 16\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n\nqHist = train(device, hybrid_model, optimizer, criterion, 1, dataloader_train, dataloader_test)\n\nplotTrainingResults([cHist, qHist], [\"classical\", \"quantum\"])\n\n\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/3"}, "3": {"author": "josh", "date": "1622558702538", "content": "No worries @Daniel63656! Thanks for posting the full code.\nI think I\u2019ve zeroed in on the issue; in QNode4, the input tensor is being encoded in the circuit using qml.QubitStateVector, which does not support differentiation via the parameter-shift rule, unfortunately:\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef circuit(inputs, weights):\n    qml.QubitStateVector(inputs, wires=[0, 1])\n    qml.templates.StronglyEntanglingLayers(weights, wires=[0, 1])\n    return qml.probs(wires=[0, 1])\n\ninputs = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=True)\ninputs = inputs / torch.linalg.norm(inputs)\nweights = torch.ones([3, 2, 3], requires_grad=True)\n\n>>> res = torch.sum(torch.sin(circuit(inputs, weights)))\n>>> res.backward()\n  File \"/home/josh/xanadu/pennylane/pennylane/tape/jacobian_tape.py\", line 205, in _grad_method_validation\n    raise ValueError(f\"Cannot differentiate with respect to parameter(s) {nondiff_params}\")\nValueError: Cannot differentiate with respect to parameter(s) {0}\n\nOne \u2018workaround\u2019 is to instead use a state preparation ansatz or template. That is, a state preparation that uses a set of unitary gates to prepare the state from the ground state. Because it decomposes down into gates that do support the parameter-shift rule, the overall state preparation is now differentiable.\nOne example in PennyLane is the qml.MottonenStatePreparation 1 template:\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef circuit(inputs, weights):\n    qml.templates.MottonenStatePreparation(inputs, wires=[0, 1])\n    qml.templates.StronglyEntanglingLayers(weights, wires=[0, 1])\n    return qml.probs(wires=[0, 1])\n\ninputs = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=True)\ninputs = inputs / torch.linalg.norm(inputs)\nweights = torch.ones([3, 2, 3], requires_grad=True)\n\nHowever, the Mottonen state preparation template is currently not fully tested for differentiability at the moment, which we are working on. So the above code will still fail, but with a different error \nHowever, it is just a issue with the dtype; adding the following to line 206 of pennylane/templates/state_preparations/mottonen.py,\nindex 860eeb80..a7a83544 100644\n--- a/pennylane/templates/state_preparations/mottonen.py\n+++ b/pennylane/templates/state_preparations/mottonen.py\n@@ -203,6 +203,9 @@ def _get_alpha_y(a, n, k):\n     with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n         division = numerator / denominator\n\n+    division = qml.math.cast(division, np.float64)\n+    denominator = qml.math.cast(denominator, np.float64)\n+\n     division = qml.math.where(denominator != 0.0, division, 0.0)\n\n     return 2 * qml.math.arcsin(qml.math.sqrt(division))\n\nfixes the bug \n>>> res = torch.sum(torch.sin(circuit(inputs, weights)))\n>>> res.backward()\n>>> print(weights.grad)\ntensor([[[ 3.8434e-03, -2.5692e-03, -2.9169e-03],\n         [ 2.2742e-03, -6.6792e-03, -5.7919e-03]],\n\n        [[-5.7919e-03, -1.3341e-02, -1.2886e-02],\n         [ 2.1471e-02,  1.0306e-02,  1.6321e-02]],\n\n        [[ 1.6321e-02,  1.3402e-02,  3.9293e-17],\n         [-8.7039e-03,  1.7823e-02, -3.4684e-18]]])\n\nI will make a PR to solve this bug; in the meantime, feel free to apply the fix I posted above directly to the mottonen.py file.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/4"}, "4": {"author": "Daniel63656", "date": "1622561108277", "content": "Thanks josh,\nI have my own implementation of a data amplitude encoding circuit and i noticed that using it doesn\u2019t lead to the error (forgot to mention that, sorry). However mine is way slower.\nBut I am not sure if I understand the problem correctly. The inputs parameter should be a non-trainable parameter and therefore not differentiated at all.\nAre you saying that the qml.QubitStateVector method prevents the parameter-shift differentiation applied to the parameters (simply because it is part of the circuit?)\nAlso, are you familiar with qiskits amplitude embedding implementation? The circuit can be build out of U3 and CNOT gates only and should therefore be differentiable. I assumed pennyLane uses a similiar implementation.\nhttps://qiskit.org/documentation/stubs/qiskit.extensions.Initialize.html", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/5"}, "5": {"author": "josh", "date": "1622564461835", "content": "\nAlso, are you familiar with qiskits amplitude embedding implementation?\n\nI am familiar with the function, but not the decomposition they are using. However I assume it must be similar to ours, yes. The one we use is described here: https://arxiv.org/pdf/quant-ph/0407010.pdf 2\n\nBut I am not sure if I understand the problem correctly. The inputs parameter should be a non-trainable parameter and therefore not differentiated at all.\n\nIt might be good to verify this in the code if possible. Perhaps something in your model is causing the QNode inputs to become trainable? This can happen if the inputs to the QNode are the output of a function that is itself differentiable \nIf the input is not differentiable, then it should have the setting requires_grad=False. PennyLane ignores Torch tensors that have requires_grad=False, so the following works correctly:\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef circuit(inputs, weights):\n    qml.QubitStateVector(inputs, wires=[0, 1])\n    qml.templates.StronglyEntanglingLayers(weights, wires=[0, 1])\n    return qml.probs(wires=[0, 1])\n\n# set the inputs as non-differentiable\ninputs = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=False)\ninputs = inputs / torch.linalg.norm(inputs)\n\nweights = torch.ones([3, 2, 3], requires_grad=True)\n\nres = torch.sum(torch.sin(circuit(inputs, weights)))\nres.backward()\nprint(weights.grad)\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/6"}, "6": {"author": "Daniel63656", "date": "1622566787288", "content": "I know this example but because I am using the qml.qnn.TorchLayer in a network class, I can\u2019t implement it like this (at least I don\u2019t know how).\nBut from the qml.qnnTorchLayer docu:\n\nThe signature of the QNode  must  contain an  inputs  named argument for input data, with all other arguments to be treated as internal weights.\n\nSo I assumed this to be working properly. Also like I said I inserted the line\nprint(ctx.args)\n\nin line 109 of pennylane\\interfaces\\torch.py\nto see what the interface tries to differentiate. Using QubitStateVector results in\n\n[array([0.        , 0.11698876, 0.12460404, 0.        , 0.29290894,\n0.5918856 , 0.467779  , 0.        , 0.        , 0.        ,\n0.        , 0.5444552 , 0.13933586, 0.        , 0.        ,\n0.        ], dtype=float32), 3.524852752685547, 0.36004096269607544, 1.1748652458190918, 4.160643577575684, 0.32508784532546997, 1.1772842407226562, 4.3311238288879395, 0.13922111690044403, 5.3807806968688965, 4.490009307861328, 3.0298593044281006, 5.180414199829102, 1.7754203081130981, 4.418515205383301, 4.486685276031494, 1.1825096607208252]\n\nwhere one can clearly see the inputs (array) in the list of stuff to be differentiated. When I use my own encoding, this array is missing and there are only the weights of the QNode in this list (than the network trains).\nIn another forum someone suggested making inputs a keyword argument. Didn\u2019t help.\nI\u2019m really out of things to try by now. There seems to be no documented case of how to use a QNode in an existing model, with certainty that pennyLane won\u2019t try to differentiate the inputs.\nBy the way I used this call to decompose qiskit.initialize into U3 gates. Maybe it helps.\n#make some input vector x\nqc = QuantumCircuit(n)\n# this function takes care of amplitude encoding in a highly efficient way\nqc.initialize(x, [k for k in range(n)])\nqc.measure_all()\n\n# present the circuit with U-Gates\nqc = transpile(qc, basis_gates=['u1', 'u2', 'u3', 'cx'])\nqc.draw(output='mpl')\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/7"}, "7": {"author": "Tom_Bromley", "date": "1622583877075", "content": "Hi @Daniel63656!\nI\u2019m joining the discussion a bit late so was wondering if we could rewind a bit.\n\nBut I am not sure if I understand the problem correctly. The inputs parameter should be a non-trainable parameter and therefore not differentiated at all.\n\nWhy do you expect that to be the case? You have a quantum network defined as:\ndef forward(self, x):\n    # Max pooling over a (2, 2) window\n    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n    # If the size is a square, you can specify with a single number\n    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n    x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = F.relu(self.fc3(x))\n    # sqrt because output are amplitudes. This way input=output if there is only data encoding\n    x = torch.sqrt(self.fc4(x)).to(device)\n    x = self.fc5(x)\n    return x\n\nGiven the line x = torch.sqrt(self.fc4(x)).to(device) where self.fc4 is a TorchLayer, don\u2019t we expect to differentiate the TorchLayer with respect to its inputs so that we can access the derivative of QNet with respect to weights of, e.g., fc1 or fc2? In that case, we need the amplitude embedding of inputs to be differentiable, so working out an alternative to QubitStateVector like MottonenStatePreparation makes sense.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/8"}, "8": {"author": "Daniel63656", "date": "1622615273367", "content": "Hi Tom,\nthis would explain why I can train without problems when I freeze all previous layers. Otherwise autograd calculates del Cost / del inputs? I thought in backpropagation this does not result in the actual differentiation (rather inputs-labels for cross entropy for example).\nWhen I use my own data embedding this seemed not to be a problem.\nI don\u2019t use the inputs directly in the circuit, I rather construct angles out of them in the function findAngles(x) . In QNode1 I use the embedding layer also for the variational circuit (so weights are also used to calculate angles). When I insert a\nprint(model.fc4.weights.grad)\n\nafter optimizer.step() in the training, all the gradients are None. This seemed logical to me, because the weights don\u2019t end up directly in the circuit and I could see why autograd can\u2019t follow along.\nIn QNode2 I just removed the weights to angles method and put the weights straight in the variation layer. I have gradients now (but mostly zero). But now knowing, that inputs also need to be differentiable, the above problem extends to them as well. So using QNode2 will also hamper training, because autograd still can\u2019t make del Cost / del inputs? So I guess in this case the gradient is just silently None instead of throwing an error?\nHere the code\ndef rotation_layer(params, qubits):\n    # parametrized ry, rz rotations\n    n = len(qubits)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i], wires=q)\n    for i,q in enumerate(qubits):\n        qml.RY(params[i+n], wires=q)\n\ndef entanglement_layer(qubits):\n    n = len(qubits)\n    for i in range(0, n):\n        qml.CNOT(wires=[qubits[i], qubits[(i+1)%n]])\n        \n# getting rotation angles from params\ndef findAngles(x):       # cost: O(n)\n    if (len(x) > 1):\n        # auxilary vector v with 2^(n-1) = N/2 dimensions\n        v = [m.sqrt(abs(x[2*k])**2 + abs(x[2*k+1])**2) for k in range(len(x)//2)]\n        inner_angles = findAngles(v)\n        # output vector angles with N/2 dimesnions\n        angles = []\n        for k in range(len(v)):\n            if (v[k] != 0):\n                if (x[2*k] > 0):\n                    angles.append(2*m.asin(x[2*k+1]/v[k]))\n                else:\n                    angles.append(2*m.pi-2*m.asin(x[2*k+1]/v[k]))\n            else:\n                angles.append(0)\n        if (inner_angles != None):   #this appends the lists if inner_angles isn't empty\n            angles = inner_angles + angles\n        return angles\n    \ndef dataEmbedding(angles, N):\n    n = int(m.log(N,2))\n    \n    qml.RY(angles[0], wires=0)   #apply first Ry which isn't controlled\n    rep = 2\n    idx = 1\n    for num_con in range(1,n):\n        for i in range(rep):\n            # calculate control pattern \n            conditionState = idx-(2**num_con-1)\n            binary_index = '{:0{}b}'.format(conditionState, num_con)\n            # calculate matrix of RY-Gate\n            phi = angles[idx]/2\n            U = np.array([[m.cos(phi),  m.sin(phi)], [-m.sin(phi),  m.cos(phi)]])\n            # apply. Pattern in binary form can be given as parameter directly\n            qml.ControlledQubitUnitary(U, control_wires=range(num_con), \n                                       wires=num_con, control_values=binary_index)\n            idx += 1\n        rep*=2\n\n# create the QNode\n@qml.qnode(qml.device('default.qubit', wires=4), interface=\"torch\")\ndef QNode1(inputs, weights):\n    inputs = F.normalize(inputs,dim=-1,p=2)  #L2-normalization\n    N = len(inputs)\n    n = int(m.log(N,2))\n    # data encoding\n    angles = findAngles(inputs)\n    dataEmbedding(angles, N)\n    \n    #variational circuit\n    measureWires = range(n)\n    for w in weights:\n        angles = findAngles(w)\n        dataEmbedding(angles, N)\n\n    #measure\n    return qml.probs(wires = measureWires)\n\n# create the QNode\n@qml.qnode(qml.device('default.qubit', wires=4), interface=\"torch\")\ndef QNode2(inputs, weights):\n    inputs = F.normalize(inputs,dim=-1,p=2)  #L2-normalization\n    N = len(inputs)\n    n = int(m.log(N,2))\n    # data encoding\n    dataEmbedding(inputs, N)\n    \n    #variational circuit\n    measureWires = range(n)\n    for w in weights:\n        dataEmbedding(w, N)\n\n    #measure\n    return qml.probs(wires = measureWires)\n\n# model class\nclass QNet(nn.Module):\n\n    def __init__(self):\n        super(QNet, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 16, 128)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 16)\n        n_qubits = 4\n        n_layers = 2\n        self.fc4 = qml.qnn.TorchLayer(QNode2, {\"weights\": (n_layers, n_qubits**2)})\n        self.fc5 = nn.Linear(16, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        # sqrt because output are amplitudes. This way input=output if there is only data encoding\n        x = torch.sqrt(self.fc4(x)).to(device)\n        x = self.fc5(x)\n        return x\n    \n\n#training\nhybrid_model = QNet().to(device)\nepochs = 16\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n\nqHist = tran(device, hybrid_model, optimizer, criterion, 1, dataloader_train, dataloader_test)\n\nplotTrainingResults([cHist, qHist], [\"classical\", \"quantum\"])\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/9"}, "9": {"author": "Tom_Bromley", "date": "1622640225483", "content": "Hi @Daniel63656,\nYes, in general you should aim for your QNode to be differentiable with respect to inputs. You can do this by making sure that every operation you apply to inputs (and the trainable parameters) is differentiable. In QNode1, it looks like findAngles is not differentiable because of use of m.sqrt (assuming m is Python\u2019s math module) - you should swap out these cases with torch-compatible functionality (see here for example). It looks like the dataEmbedding may also be breaking differentiability with use of the math module, so this could be giving you None or zero gradients.\nI\u2019d suggest breaking things up a little and checking that you can find the gradient with respect to findAngles and dataEmbedding before constructing the full model.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/10"}, "10": {"author": "Daniel63656", "date": "1622642136884", "content": "I still don\u2019t get why we would need to differentiate the inputs.\n\nI thought in backpropagation this does not result in the actual differentiation (rather inputs-labels for cross entropy for example)\n\nIs this a special thing of autograd?\n\nI\u2019d suggest breaking things up a little and checking that you can find the gradient with respect to  findAngles  and  dataEmbedding  before constructing the full model.\n\nHow would I do that? So far I was only able to print weights.grad. model.fc4.inputs is \u201cnot defined\u201d", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/11"}, "11": {"author": "Tom_Bromley", "date": "1622722997514", "content": "Hey @Daniel63656,\n\nI thought in backpropagation this does not result in the actual differentiation (rather inputs-labels for cross entropy for example)\n\nI\u2019m not quite sure what you mean here, do you have a reference for this intuition?\nDifferentiating the inputs of a layer in a sequential model is a common feature and not particular to quantum or PennyLane. Check out this 1 for a bit more detail.\n\nHow would I do that? So far I was only able to print weights.grad. model.fc4.inputs is \u201cnot defined\u201d\n\nThe following example shows how findAngles can be set up to be compatible with torch differentiation:\nimport math as m\nimport torch\n\ndef findAngles(x):       # cost: O(n)\n    if (len(x) > 1):\n        # auxilary vector v with 2^(n-1) = N/2 dimensions\n        v = [torch.sqrt(abs(x[2*k])**2 + torch.abs(x[2*k+1])**2) for k in range(len(x)//2)]\n        inner_angles = findAngles(v)\n        # output vector angles with N/2 dimesnions\n        angles = []\n        for k in range(len(v)):\n            if (v[k] != 0):\n                if (x[2*k] > 0):\n                    angles.append(2*torch.asin(x[2*k+1]/v[k]))\n                else:\n                    angles.append(2*m.pi-2*torch.asin(x[2*k+1]/v[k]))\n            else:\n                angles.append(0)\n        if (inner_angles != None):   #this appends the lists if inner_angles isn't empty\n            angles = inner_angles + angles\n        return angles\n    \nx = torch.ones(8, requires_grad=True)\nout = torch.stack(findAngles(x))\n\nloss = torch.sum(out)\nloss.backward()\n\nx.grad\n\nIf we had used, for example, m.sqrt(abs(x[2*k])**2 rather than torch.sqrt(abs(x[2*k])**2, the above would raise an error.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/12"}, "12": {"author": "Tom_Bromley", "date": "1622726291664", "content": "@Daniel63656, does the following example help motivate why we differentiate with respect to the inputs of a layer:\nConsider a cost function C(x, w1, w2) = f2(w2, f1(w1, x)), where f2(w2, x)  and f1(w1, x) are layers, w1 and w2 are trainable weights, and x is the model input.\nWe want to find dC / dw2 and dC / dw1. Finding the former is easy, while the latter can be done with the chain rule:\ndC / dw1 = (d f2 / d o1) * (d o1 / d w1), where o1 is the output of the first layer o1 = f1(w1, x). We hence need to evaluate d f2 / d o1, which is the derivative of the output of layer 2 with respect to its input.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/13"}, "13": {"author": "Daniel63656", "date": "1622787363725", "content": "When I use the modified findAngles(x) method to train the whole network with QNode3, I get following error:\nValueError                                Traceback (most recent call last)\n<ipython-input-30-8e86a1aed95a> in <module>\n    109 optimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n    110 \n--> 111 qHist = train(device, hybrid_model, optimizer, criterion, epochs, dataloader_train, dataloader_test)\n    112 \n    113 plotTrainingResults([cHist, qHist], [\"classical\", \"quantum\"])\n\n~\\Hahn_schickard\\jupyter\\pyTorch_utils.py in train(device, model, optimizer, criterion, epochs, training, testing)\n     31             optimizer.zero_grad()\n     32             # forward + backward + optimize\n---> 33             outputs = model(inputs)\n     34             _, predicted = torch.max(outputs.data, 1)\n     35             correct += (predicted == labels).float().sum()\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n<ipython-input-30-8e86a1aed95a> in forward(self, x)\n     98         x = F.relu(self.fc3(x))\n     99         # sqrt because output are amplitudes. This way input=output if there is only data encoding\n--> 100         x = torch.sqrt(self.fc4(x)).to(device)\n    101         x = self.fc5(x)\n    102         return x\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    272             reconstructor = []\n    273             for x in torch.unbind(inputs):\n--> 274                 reconstructor.append(self.forward(x))\n    275             return torch.stack(reconstructor)\n    276 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    276 \n    277         # If the input is 1-dimensional, calculate the forward pass as usual\n--> 278         return self._evaluate_qnode(inputs)\n    279 \n    280     def _evaluate_qnode(self, x):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in _evaluate_qnode(self, x)\n    291             **{arg: weight.to(x) for arg, weight in self.qnode_weights.items()},\n    292         }\n--> 293         return self.qnode(**kwargs).type(x.dtype)\n    294 \n    295     def __str__(self):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnode.py in __call__(self, *args, **kwargs)\n    553 \n    554         # execute the tape\n--> 555         res = self.qtape.execute(device=self.device)\n    556 \n    557         if original_shots is not None:\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\tape.py in execute(self, device, params)\n   1262             params = self.get_parameters()\n   1263 \n-> 1264         return self._execute(params, device=device)\n   1265 \n   1266     def execute_device(self, params, device):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in _execute(self, params, **kwargs)\n    256     def _execute(self, params, **kwargs):\n    257         kwargs[\"tape\"] = self\n--> 258         res = _TorchInterface.apply(kwargs, *params)\n    259         return res\n    260 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in forward(ctx, input_kwargs, *input_)\n     71         # evaluate the tape\n     72         tape.set_parameters(ctx.all_params_unwrapped, trainable_only=False)\n---> 73         res = tape.execute_device(ctx.args, device)\n     74         tape.set_parameters(ctx.all_params, trainable_only=False)\n     75 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\tape.py in execute_device(self, params, device)\n   1293 \n   1294         if isinstance(device, qml.QubitDevice):\n-> 1295             res = device.execute(self)\n   1296         else:\n   1297             res = device.execute(self.operations, self.observables, {})\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\_qubit_device.py in execute(self, circuit, **kwargs)\n    182 \n    183         # apply all circuit operations\n--> 184         self.apply(circuit.operations, rotations=circuit.diagonalizing_gates, **kwargs)\n    185 \n    186         # generate computational basis samples\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py in apply(self, operations, rotations, **kwargs)\n    190                 self._apply_basis_state(operation.parameters[0], operation.wires)\n    191             else:\n--> 192                 self._state = self._apply_operation(self._state, operation)\n    193 \n    194         # store the pre-rotated state\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py in _apply_operation(self, state, operation)\n    215             return self._apply_ops[operation.base_name](state, axes, inverse=operation.inverse)\n    216 \n--> 217         matrix = self._get_unitary_matrix(operation)\n    218 \n    219         if isinstance(operation, DiagonalOperation):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py in _get_unitary_matrix(self, unitary)\n    405             return unitary.eigvals\n    406 \n--> 407         return unitary.matrix\n    408 \n    409     @classmethod\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\operation.py in matrix(self)\n    660     @property\n    661     def matrix(self):\n--> 662         op_matrix = self._matrix(*self.parameters)\n    663 \n    664         if self.inverse:\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\ops\\qubit.py in _matrix(cls, *params)\n   1879 \n   1880         if not np.allclose(U @ U.conj().T, np.identity(U.shape[0])):\n-> 1881             raise ValueError(\"Operator must be unitary.\")\n   1882 \n   1883         return U\n\nValueError: Operator must be unitary.\n\nWith the old findAngles(x) method this doesn\u2019t happen. But the math stayed exactly the same?!\n\nI understand that the chainrule requires you to calculate d f2/ d o1. I just thought that term can by replaced by a term specified by the cost function, simply containing the inputs.\nFor example given the quadratic cost C=(y-o)**2 / 2 and a layer with weights w, biases b, input a, w*a+b=z, o = output, labels y and sigma as activation function, than\nd C/d w = (o-y)* d sigma(z)*a\nd C/d b = (o-y)* d sigma(z)\n\nSo in order to do backpropagation, you can substitute the terms containing input derivatives with something cost function specific without actually differentiating with respect to inputs.\nI guess autograd doesn\u2019t do that, in order to be able to differentiate any network, regardeless of the cost functions, layer types,\u2026 (to be automatic differentiation)?", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/14"}, "14": {"author": "Tom_Bromley", "date": "1622810863116", "content": "@Daniel63656, your error looks like an issue with qml.QubitUnitary  or qml.ControlledQubitUnitary. Perhaps it is your use of qml.ControlledQubitUnitary in the following function:\ndef dataEmbedding(angles, N):\n    n = int(m.log(N,2))\n    \n    qml.RY(angles[0], wires=0)   #apply first Ry which isn't controlled\n    rep = 2\n    idx = 1\n    for num_con in range(1,n):\n        for i in range(rep):\n            # calculate control pattern \n            conditionState = idx-(2**num_con-1)\n            binary_index = '{:0{}b}'.format(conditionState, num_con)\n            # calculate matrix of RY-Gate\n            phi = angles[idx]/2\n            U = np.array([[m.cos(phi),  m.sin(phi)], [-m.sin(phi),  m.cos(phi)]])\n            # apply. Pattern in binary form can be given as parameter directly\n            qml.ControlledQubitUnitary(U, control_wires=range(num_con), \n                                       wires=num_con, control_values=binary_index)\n            idx += 1\n        rep*=2\n\nMaybe worth checking what U looks like.\nNote that the above function should also be torch-compatible. I don\u2019t think using qml.ControlledQubitUnitary is compatible with differentiation when using the torch interface, so it may be good to think of alternatives to using this gate, which may for example include MultiControlledX and a single qubit rotation, or PauliRot 1.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/15"}, "15": {"author": "Daniel63656", "date": "1623069141931", "content": "I noticed, that after the first loss.backward(), the input vector x becomes all naN after the first maxpool layer (first layer in the model). By the time it reaches the QNode U is now obviously not unitary.\n\nI don\u2019t think using  qml.ControlledQubitUnitary  is compatible with differentiation when using the torch interface\n\nDo you think this can cause that issue?\n\nMultiControlledX and a single qubit rotation, or PauliRot\n\nCould you point me in the right direction on how I could do that?", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/16"}, "16": {"author": "antalszava", "date": "1623117858731", "content": "Hi @Daniel63656,\n\nDo you think this can cause that issue?\n\nQuite probably, the cause is the unsupported differentiablity as Tom suggested previously.\nCould you perhaps have a look if by adding the following code, further details are revealed?\nfrom torch import autograd\nautograd.set_detect_anomaly(True)\n\nThis can be helpful when the backward pass values become nan using Torch.\n\nOn the note of using a different differentiable set of operations, it seems that after the first RY operation, the dataEmbedding function applies uniformly controlled RY operations by using the qml.ControlledQubitUnitary. Uniformly controlled RY operations are used in the previously mentioned MottonenStatePreparation circuit too. Applying a uniformly controlled RY operation can be substituted with CNOTs and RY operations: see Figures 1-2 in original paper from Mottonen, et al. 1", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/17"}, "17": {"author": "Daniel63656", "date": "1623228370515", "content": "If I use MottonenStatePreparation I get another error\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-54-2d0d62781f31> in <module>\n     51 criterion = nn.CrossEntropyLoss()\n     52 optimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n---> 53 q2Hist = train(device, hybrid_model, optimizer, criterion, epochs, sparse_train)\n\n~\\Hahn_schickard\\jupyter\\pyTorch_utils.py in train(device, model, optimizer, criterion, epochs, training, testing)\n     31             optimizer.zero_grad()\n     32             # forward + backward + optimize\n---> 33             outputs = model(inputs)\n     34             _, predicted = torch.max(outputs.data, 1)\n     35             correct += (predicted == labels).float().sum()\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n<ipython-input-54-2d0d62781f31> in forward(self, x)\n     42         x = F.relu(self.fc2(x))\n     43         x = F.relu(self.fc3(x))\n---> 44         x = self.fc4(x)\n     45         x = self.fc5(x)\n     46         return x\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    272             reconstructor = []\n    273             for x in torch.unbind(inputs):\n--> 274                 reconstructor.append(self.forward(x))\n    275             return torch.stack(reconstructor)\n    276 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    276 \n    277         # If the input is 1-dimensional, calculate the forward pass as usual\n--> 278         return self._evaluate_qnode(inputs)\n    279 \n    280     def _evaluate_qnode(self, x):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnn\\torch.py in _evaluate_qnode(self, x)\n    291             **{arg: weight.to(x) for arg, weight in self.qnode_weights.items()},\n    292         }\n--> 293         return self.qnode(**kwargs).type(x.dtype)\n    294 \n    295     def __str__(self):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnode.py in __call__(self, *args, **kwargs)\n    550         if self.mutable or self.qtape is None:\n    551             # construct the tape\n--> 552             self.construct(args, kwargs)\n    553 \n    554         # execute the tape\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\qnode.py in construct(self, args, kwargs)\n    530             self.qtape = self.qtape.expand(\n    531                 depth=self.max_expansion,\n--> 532                 stop_at=lambda obj: not isinstance(obj, qml.tape.QuantumTape)\n    533                 and self.device.supports_operation(obj.name),\n    534             )\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\tape.py in expand(self, depth, stop_at, expand_measurements)\n    557         \"\"\"\n    558         new_tape = expand_tape(\n--> 559             self, depth=depth, stop_at=stop_at, expand_measurements=expand_measurements\n    560         )\n    561         new_tape._update()\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\tape.py in expand_tape(tape, depth, stop_at, expand_measurements)\n    193                 # Object is an operation; query it for its expansion\n    194                 try:\n--> 195                     obj = obj.expand()\n    196                 except NotImplementedError:\n    197                     # Object does not define an expansion; treat this as\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py in expand(self)\n    278             # Apply inverse y rotation cascade to prepare correct absolute values of amplitudes\n    279             for k in range(len(wires_reverse), 0, -1):\n--> 280                 alpha_y_k = _get_alpha_y(a, len(wires_reverse), k)\n    281                 control = wires_reverse[k:]\n    282                 target = wires_reverse[k - 1]\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py in _get_alpha_y(a, n, k)\n    204         division = numerator / denominator\n    205 \n--> 206     division = qml.math.where(denominator != 0.0, division, 0.0)\n    207 \n    208     return 2 * qml.math.arcsin(qml.math.sqrt(division))\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\math\\fn.py in where(condition, x, y)\n    969     tensor([ 0.6000,  0.2300,  0.7000, -4.0000, -5.0000], grad_fn=<SWhereBackward>)\n    970     \"\"\"\n--> 971     return _get_multi_tensorbox([x, y]).where(condition, x, y, wrap_output=False)\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\math\\tensorbox.py in _wrapper(*args, **kwargs)\n     38             return cls(func(*args, **kwargs))\n     39 \n---> 40         return func(*args, **kwargs)\n     41 \n     42     return _wrapper\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\math\\torch_box.py in where(condition, x, y)\n    196     @wrap_output\n    197     def where(condition, x, y):\n--> 198         return torch.where(TorchBox.astensor(condition), *TorchBox.unbox_list([x, y]))\n\nRuntimeError: expected scalar type float but found double\n\nI can\u2019t simply change the return type since a QNode must return a measurement", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/18"}, "18": {"author": "antalszava", "date": "1623281393607", "content": "Hi @Daniel63656,\nCreated a branch for the changes that Josh mentioned previously for aiding the issue that you see, could you try installing this version of PennyLane and checking if the error persists? That would be only until we\u2019ve fully incorporated it.\nThe version on the branch can be installed via\npip install git+https://github.com/PennyLaneAI/pennylane.git@mottonen_cast_fix\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/19"}, "19": {"author": "Daniel63656", "date": "1623315785071", "content": "installing this PennyLane version gives another error:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-5196e0249258> in <module>\n     50 criterion = nn.CrossEntropyLoss()\n     51 optimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n---> 52 q3Hist = train(device, hybrid_model, optimizer, criterion, epochs, sparse_train)\n\n~\\Hahn_schickard\\quantencomputing\\jupyter\\pyTorch_utils.py in train(device, model, optimizer, criterion, epochs, training, testing)\n     31             optimizer.zero_grad()\n     32             # forward + backward + optimize\n---> 33             outputs = model(inputs)\n     34             _, predicted = torch.max(outputs.data, 1)\n     35             correct += (predicted == labels).float().sum()\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n<ipython-input-9-5196e0249258> in forward(self, x)\n     41         x = F.relu(self.fc2(x))\n     42         x = F.relu(self.fc3(x))\n---> 43         x = self.fc4(x)\n     44         x = self.fc5(x)\n     45         return x\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\n    887             result = self._slow_forward(*input, **kwargs)\n    888         else:\n--> 889             result = self.forward(*input, **kwargs)\n    890         for hook in itertools.chain(\n    891                 _global_forward_hooks.values(),\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    272             reconstructor = []\n    273             for x in torch.unbind(inputs):\n--> 274                 reconstructor.append(self.forward(x))\n    275             return torch.stack(reconstructor)\n    276 \n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\qnn\\torch.py in forward(self, inputs)\n    276 \n    277         # If the input is 1-dimensional, calculate the forward pass as usual\n--> 278         return self._evaluate_qnode(inputs)\n    279 \n    280     def _evaluate_qnode(self, x):\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\qnn\\torch.py in _evaluate_qnode(self, x)\n    291             **{arg: weight.to(x) for arg, weight in self.qnode_weights.items()},\n    292         }\n--> 293         return self.qnode(**kwargs).type(x.dtype)\n    294 \n    295     def __str__(self):\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\qnode.py in __call__(self, *args, **kwargs)\n    550         if self.mutable or self.qtape is None:\n    551             # construct the tape\n--> 552             self.construct(args, kwargs)\n    553 \n    554         # execute the tape\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\qnode.py in construct(self, args, kwargs)\n    530             self.qtape = self.qtape.expand(\n    531                 depth=self.max_expansion,\n--> 532                 stop_at=lambda obj: not isinstance(obj, qml.tape.QuantumTape)\n    533                 and self.device.supports_operation(obj.name),\n    534             )\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\tape\\tape.py in expand(self, depth, stop_at, expand_measurements)\n    557         \"\"\"\n    558         new_tape = expand_tape(\n--> 559             self, depth=depth, stop_at=stop_at, expand_measurements=expand_measurements\n    560         )\n    561         new_tape._update()\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\tape\\tape.py in expand_tape(tape, depth, stop_at, expand_measurements)\n    193                 # Object is an operation; query it for its expansion\n    194                 try:\n--> 195                     obj = obj.expand()\n    196                 except NotImplementedError:\n    197                     # Object does not define an expansion; treat this as\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py in expand(self)\n    278             # Apply inverse y rotation cascade to prepare correct absolute values of amplitudes\n    279             for k in range(len(wires_reverse), 0, -1):\n--> 280                 alpha_y_k = _get_alpha_y(a, len(wires_reverse), k)\n    281                 control = wires_reverse[k:]\n    282                 target = wires_reverse[k - 1]\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py in _get_alpha_y(a, n, k)\n    204         division = numerator / denominator\n    205 \n--> 206     division = qml.math.where(denominator != 0.0, division, 0.0)\n    207 \n    208     return 2 * qml.math.arcsin(qml.math.sqrt(division))\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\math\\fn.py in where(condition, x, y)\n    969     tensor([ 0.6000,  0.2300,  0.7000, -4.0000, -5.0000], grad_fn=<SWhereBackward>)\n    970     \"\"\"\n--> 971     return _get_multi_tensorbox([x, y]).where(condition, x, y, wrap_output=False)\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\math\\tensorbox.py in _wrapper(*args, **kwargs)\n     38             return cls(func(*args, **kwargs))\n     39 \n---> 40         return func(*args, **kwargs)\n     41 \n     42     return _wrapper\n\n~\\anaconda3\\envs\\quantum\\lib\\site-packages\\pennylane\\math\\torch_box.py in where(condition, x, y)\n    196     @wrap_output\n    197     def where(condition, x, y):\n--> 198         return torch.where(TorchBox.astensor(condition), *TorchBox.unbox_list([x, y]))\n\nRuntimeError: expected scalar type float but found double\n\nUsing Pennylane with cuda has suddenly stopped working entirely for some reason, even after creating a fresh environment. I know that using cuda with pennylane can go wrong but so far the same code has always worked on the gpu (I think the qLayer was executed on the CPU only, now this causes errors).\nI also tried using DiagonalQubitUnitary for embedding\ndef diagonal_embedding(inputs, qubits):\n    for q in qubits:\n        qml.Hadamard(wires=q)\n    real = torch.cos(inputs)\n    imag = torch.sin(inputs)\n    inputs = (real + imag*1j).clone().detach().requires_grad_(True)\n    qml.DiagonalQubitUnitary(inputs, wires=qubits)\n\nAgain not differentiable \nValueError                                Traceback (most recent call last)\n<ipython-input-22-b4ba44eb2ba7> in <module>\n     56 criterion = nn.CrossEntropyLoss()\n     57 optimizer = optim.SGD(hybrid_model.parameters(), lr=0.01, momentum=0.8)\n---> 58 q3Hist = train(device, hybrid_model, optimizer, criterion, epochs, sparse_train, sparse_test)\n\n~\\Hahn_schickard\\jupyter\\pyTorch_utils.py in train(device, model, optimizer, criterion, epochs, training, testing)\n     35             correct += (predicted == labels).float().sum()\n     36             loss = criterion(outputs, labels)\n---> 37             loss.backward()\n     38             optimizer.step()\n     39             batch_percentage = i*20//number_batches\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    243                 create_graph=create_graph,\n    244                 inputs=inputs)\n--> 245         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    246 \n    247     def register_hook(self, hook):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    145     Variable._execution_engine.run_backward(\n    146         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--> 147         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    148 \n    149 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\n     87     def apply(self, *args):\n     88         # _forward_cls is defined by derived class\n---> 89         return self._forward_cls.backward(self, *args)  # type: ignore\n     90 \n     91 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in backward(ctx, dy)\n    175         if dy.is_cuda:\n    176             cuda_device = dy.get_device()\n--> 177         vjp = dy.view(1, -1) @ ctx.jacobian.apply(ctx, *ctx.saved_tensors).to(dy)\n    178         vjp = torch.unbind(vjp.view(-1))\n    179         return (None,) + tuple(vjp)\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in forward(ctx_, parent_ctx, *input_)\n    123                 ctx_.dy = parent_ctx.dy\n    124                 ctx_.save_for_backward(*input_)\n--> 125                 jacobian = _evaluate_grad_matrix(\"jacobian\")\n    126                 return jacobian\n    127 \n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\interfaces\\torch.py in _evaluate_grad_matrix(grad_matrix_fn)\n    108             tape.set_parameters(ctx.all_params_unwrapped, trainable_only=False)\n    109             grad_matrix = getattr(tape, grad_matrix_fn)(\n--> 110                 device, params=ctx.args, **tape.jacobian_options\n    111             )\n    112             tape.set_parameters(ctx.all_params, trainable_only=False)\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\qubit_param_shift.py in jacobian(self, device, params, **options)\n    122         self._append_evA_tape = True\n    123         self._evA_result = None\n--> 124         return super().jacobian(device, params, **options)\n    125 \n    126     def parameter_shift(self, idx, params, **options):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\jacobian_tape.py in jacobian(self, device, params, **options)\n    514 \n    515         # perform gradient method validation\n--> 516         diff_methods = self._grad_method_validation(method)\n    517 \n    518         if not self._has_trainable_params(params, diff_methods):\n\n~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pennylane\\tape\\jacobian_tape.py in _grad_method_validation(self, method)\n    198 \n    199         if nondiff_params:\n--> 200             raise ValueError(f\"Cannot differentiate with respect to parameter(s) {nondiff_params}\")\n    201 \n    202         numeric_params = {idx for idx, g in diff_methods.items() if g == \"F\"}\n\nValueError: Cannot differentiate with respect to parameter(s) {0}\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/20"}, "20": {"author": "Tom_Bromley", "date": "1623336828018", "content": "Hey @Daniel63656!\n\nCould you point me in the right direction on how I could do that?\n\nRegarding the suggestion of using MultiControlledX or PauliRot, the idea would be just to swap out the lines\nU = np.array([[m.cos(phi),  m.sin(phi)], [-m.sin(phi),  m.cos(phi)]])\nqml.ControlledQubitUnitary(U, control_wires=range(num_con),\n\nwith something like\nqml.MultiControlledX(wires=targ_wire, control_wires=range(num_con))\nqml.RY(phi, wires=targ_wire)\nqml.MultiControlledX(wires=targ_wire, control_wires=range(num_con))\n\nThis approach may not be exactly what you have above, but is similar.\nI make this suggestion because it seems like your issues involve trying to manually make a unitary and then using qml.ControlledQubitUnitary or qml.DiagonalQubitUnitary. Although this is something we\u2019re improving on, it\u2019s a use case that can quite often break differentiability. Instead, it\u2019s better to use gates that have a well defined input parameter, such as qml.RX and qml.Rot. Finding the derivatives with respect to parametrized gates is much more of an established use case in PennyLane.\nAlso, another piece of advice when debugging errors in your model is to try to break it down into the elementary nodes/layers and see if the gradient is accessible for each. For example, instead of training the whole hybrid_model, it\u2019s easier to focus on differentiating diagonal_embedding.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/21"}, "21": {"author": "Daniel63656", "date": "1623653460475", "content": "\nqml.MultiControlledX(wires=targ_wire, control_wires=range(num_con))\nqml.RY(phi, wires=targ_wire)\nqml.MultiControlledX(wires=targ_wire, control_wires=range(num_con))\n\nI doubt that that is an equivalent of the previous formulation. I added the control_values and it doesn\u2019t produce the same result.\n qml.MultiControlledX(wires=num_con, control_wires=range(num_con), control_values=binary_index)\nqml.RY(angles[idx], wires=num_con)\n qml.MultiControlledX(wires=num_con, control_wires=range(num_con), control_values=binary_index)\n            \n\nUnfortunately using this Gate I can\u2019t even print the circuit.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/22"}, "22": {"author": "Tom_Bromley", "date": "1623679859095", "content": "Hey @Daniel63656!\n\nI doubt that that is an equivalent of the previous formulation. I added the control_values and it doesn\u2019t produce the same result.\n\nAgreed. It looks like you are trying to do a multi-controlled-Y gate. PennyLane supports qml.CRY for control on an additional wire, but beyond that we need to think a bit more carefully. One approach to exactly performing a multi-controlled-Y gate is provided here in Sec. 7 (e.g., Lemma 7.9). You should be able to set this up using qml.MultiControlledX and qml.CRot, etc.\nHowever, are you aiming to perform this operation exactly, or simply to have an operation that interacts all of the qubits and with a trainable parameter? In which case, the MultiControlledX - RY - MultiControlledX approach may be sufficient. I\u2019d also recommend prioritizing getting something to work, even if it isn\u2019t exactly your expected transformation, and then evolve from there.\n\nUnfortunately using this Gate I can\u2019t even print the circuit.\n\nAh, good point! This was a bug that we have fixed in the development version of PennyLane. To access that, you can install following these installation instructions. The circuit should then be printable:\nimport pennylane as qml\n\ndev = qml.device(\"default.qubit\", wires=5)\n\ntarg_wire = 0\ncontrol_wires = range(1, 5)\n\n@qml.qnode(dev)\ndef f(phi):\n    qml.MultiControlledX(wires=targ_wire, control_wires=control_wires)\n    qml.RY(phi, wires=targ_wire)\n    qml.MultiControlledX(wires=targ_wire, control_wires=control_wires)\n    return qml.expval(qml.PauliZ(0))\n\nf(0.2)\nprint(f.draw())\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/23"}, "23": {"author": "Daniel63656", "date": "1623681138101", "content": "Hi Tom,\n\nI\u2019d also recommend prioritizing getting something to work, even if it isn\u2019t exactly your expected transformation, and then evolve from there\n\nI am just experimenting around a bit to find a more efficient qNode since the standard approach (rotations and entanglement) doesn\u2019t work so well. I came up with a more \u201csophisticated\u201d variational approach which does converge a lot faster, but I would like to have a data embedding, where the input parameters are independent from each other (which is not the case with rotating each qubit Ry, Rz, Ry what I do now). I basically just want to try if using amplitude embedding leeds to even faster convergence in combination with my variational approach \nI also hope to get a circuit where I can have 2^qubit output neurons (measuring propabilities) without just \u201cinflating\u201d more information from the qubits-expectation values. I guess with that as goal using something like amplitude encoding or a diagonal applied to the hadamard space is a key factor.\nUnfortunately, as you can see, none of my encoding attempts work in the context of differentiability", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/24"}, "24": {"author": "Tom_Bromley", "date": "1623702389568", "content": "Thanks @Daniel63656!\n\nI basically just want to try if using amplitude embedding leeds to even faster convergence in combination with my variational approach\n\nComing back to the original discussion: if you just want to train a model using an amplitude-based embedding, why don\u2019t you consider switching to the TensorFlow interface?\nThe code below shows how you can obtain the gradient of your model with respect to the quantum parameters:\nimport pennylane as qml\nimport tensorflow as tf\n\nwires = 2\nlayers = 3\n\ndev = qml.device(\"default.qubit.tf\", wires=wires)\n\n@qml.qnode(dev, interface=\"tf\", diff_method=\"backprop\")\ndef f(inputs, weights):\n    qml.QubitStateVector(inputs, wires=range(wires))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(wires))\n    return qml.probs(range(wires))\n\nweight_shapes = {\"weights\": (layers, wires, 3)}\n\nqlayer = qml.qnn.KerasLayer(f, weight_shapes, output_dim = 2 ** wires)\nclayer1 = tf.keras.layers.Dense(2 ** wires, activation=\"sigmoid\")\n\ndef normalize(x):\n    x = tf.cast(x, tf.complex128)\n    return tf.stack([x_ / tf.sqrt(tf.reduce_sum(tf.math.conj(x_) * x_)) for x_ in x])\n\nclayer_interface = tf.keras.layers.Lambda(normalize)\nclayer2 = tf.keras.layers.Dense(2, activation=\"softmax\")\n\nmodel = tf.keras.Sequential([clayer1, clayer_interface, qlayer, clayer2])\n\ninputs = tf.ones((2, 4))\n\nwith tf.GradientTape() as tape:\n    output = model(inputs)\n    \nqlayer_weights = qlayer.trainable_weights\n    \ntape.jacobian(output, qlayer_weights)\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/25"}, "25": {"author": "Daniel63656", "date": "1623741370205", "content": "I didn\u2019t consider this an option. I thought I can only use cirq with tensorflow. I would need to train the whole model in tensorflow then. I know that using Tesnorflow for something new like this requires some deeper knowledge about it (which I don\u2019t really have).\nI need to define my own training loop and can\u2019t use model.fit, right?\nBut if this makes QubitStateVector and possibly DiagonalQubitUnitary differentiable, I will certainly give it a try!\nBut the problem is that pennylane can\u2019t differentiate these unitaries, so I don\u2019t see why switching to Tensorflow would change something.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/26"}, "26": {"author": "Tom_Bromley", "date": "1623775925589", "content": "Hey @Daniel63656,\n\nI need to define my own training loop and can\u2019t use model.fit, right?\n\nThe code provided above converts the PennyLane QNode into a Keras layer, so you can use all the standard tools you would normally when dealing with Keras models. You can check out our tutorial here.\n\nBut the problem is that pennylane can\u2019t differentiate these unitaries, so I don\u2019t see why switching to Tensorflow would change something.\n\nRight, differentiating the arbitrary unitaries will probably also not work in the TensorFlow interface. However, I\u2019d recommend using parametrized gates rather than arbitrary unitaries.", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/27"}, "27": {"author": "Daniel63656", "date": "1625040567859", "content": "Hi it\u2019s me again,\nThis topic got a bit out of focus because I didn\u2019t get good training results with amplitude encoding (using transfer learning). But now I found a good approach, so the question for differentiability of amplitude state preparation is on the table again.\nI sum up what options we considered, so maybe we can just make this an open issue?\n-MottonenStatePreparation:\nThe git repository for the cast fix isn\u2019t available. But the suggested fix in the file works. Are the changes already incorporated into pennyLane?\nWith that fixed, the values become NaN directly.\nFrom docs: \u201cDue to non-trivial classical processing of the state vector, this template is not always fully differentiable.\u201d\n-qml.QubitStateVector:  decomposes into M\u00f6tt\u00f6nen-method -> same problem\n-qml.templates.AmplitudeEmbedding\nI didn\u2019t tried that one because the docs clearly state non differentiability. What is even the differnce between those three methods?\n-My own embedding attempt based on https://www.nature.com/articles/s41598-021-85474-1.pdf?origin=ppub\nPre-processing is involved (findAngles), but can be made differentiable with pyTorch. Problem are the MultiControlledRy-rotations (currently implemented by qml.ControlledQubitUnitary (not differentiable!).\nI am also pretty confused, because here Differentiation with AmplitudeEmbedding the same problem seemingly got solved by making inputs a keyword argument, which doesn\u2019t work for me at all (in fact leaving inputs as non-keyword argument works completely fine with autograd when not using amplitude encoding).\nI looked into Qiskits initializer-class\nhttps://qiskit.org/documentation/_modules/qiskit/extensions/quantum_initializer/initializer.html\nbut I\u2019m not sure if this is a new method/would be differentiable if implemented in pennylane:\n\u201cNote that Initialize is an Instruction and not a Gate since it contains a reset instruction, which is not unitary.\u201d", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/28"}, "28": {"author": "theodor", "date": "1625069926523", "content": "Hi @Daniel63656,\n\nAre the changes already incorporated into pennyLane?\n\nThe fix should be in the latest release (v0.16.0). You can find it in the second entry under Bug fixes in the release notes 1.\n\nThe three state preparations are very similar indeed.\n\n\nQubitStateVector might be supported natively on a quantum device, and if there\u2019s no need to differentiate it, it\u2019s much quicker to simply use that one instead of decomposing it using the M\u00f6tt\u00f6nen state preparation.\n\n\nAmplitudeEmbedding is a template that basically applies a QubitStateVector operation after doing some preprocessing, such as padding of the state and normalizing it.\n\n\nMottonenStatePreparation uses a method to prepare a specific state according to this paper from M\u00f6tt\u00f6nen, et al. which usually can work if the device in question does not have native support for a direct state preparation operation, but it will likely not be as fast.\n\n\n\nI am also pretty confused, because here Differentiation with AmplitudeEmbedding the same problem seemingly got solved by making inputs a keyword argument, which doesn\u2019t work for me at all\n\nThe syntax for marking differentiable inputs or not has changed, and should be done with a requires_grad flag when declared, for NumPy and Torch, or declaring the input as a tf.constant for Tensorflow. You can read more about that on the interfaces page in the documentation.\nI hope this clears some things up!1", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/29"}, "29": {"author": "Daniel63656", "date": "1625124780558", "content": "Hi theodor,\nthis clears some things up for me indeed .\n\nmight be supported natively on a quantum device\n\nYou mean an actual quantum computer?\nMy QNode returns probabilities not expectation values.\nreturn qml.probs(wires = measureWires)\nDoes in this case the automatic differentiation via parameter-shift-rule calculate the correct gradients w.r.t. to specified cost? I mean I don\u2019t return an measured observable", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/30"}, "30": {"author": "antalszava", "date": "1625239444009", "content": "Hi @Daniel63656,\n\nYou mean an actual quantum computer?\n\nNo, some statevector simulators might support the  qml.QubitStateVector operation natively (e.g., by performing matrix vector multiplications or more specialized methods). When running it on an actual quantum computer, its decomposition would be used, which is defined using qml.MottonenStatePreparation.\n\nDoes in this case the automatic differentiation via parameter-shift-rule calculate the correct gradients w.r.t. to specified cost? I mean I don\u2019t return an measured observable\n\nCould you elaborate on this question? Gradients are computed with regard to the trainable parameters in the circuit. These are specified by setting the requires_grad=True argument:\nfrom pennylane import numpy as np\n\ntrainable_param = np.array([0.123], requires_grad=True)\n", "link": "https://discuss.pennylane.ai//t/hybrid-network-not-differentiating/1079/31"}}