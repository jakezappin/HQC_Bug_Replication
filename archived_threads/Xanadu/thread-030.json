{"0": {"author": "Nikhil_Narayanan", "date": "1687786670394", "content": "This is the contents of a file that I\u2019m trying to import,\nimport pennylane as qml\nfrom catalyst import qjit\nfrom pennylane.numpy import pi, tensor\nfrom pennylane.operation import Operation\n\n\nclass StronglyEntanglingLayer(Operation):\n    num_params = 6\n    num_wires = 2\n    grad_method = None\n    grad_recipe = ([[0.5, 1, pi / 2], [-0.5, 1, -pi / 2]],)\n\n    @qjit\n    def compute_decomposition(\n        weight0: tensor,\n        weight1: tensor,\n        weight2: tensor,\n        weight3: tensor,\n        weight4: tensor,\n        weight5: tensor,\n        wires: list,\n    ):\n        op_list = [\n            qml.RZ(weight0, wires=wires[0]),\n            qml.RZ(weight1, wires=wires[1]),\n            qml.RY(weight2, wires=wires[0]),\n            qml.RY(weight3, wires=wires[1]),\n            qml.RZ(weight4, wires=wires[0]),\n            qml.RZ(weight5, wires=wires[1]),\n            qml.CNOT(wires=[wires[0], wires[1]]),\n            qml.CNOT(wires=[wires[1], wires[0]]),\n        ]\n        return op_list\n\nThis is the error message that I get shown when trying to import the above\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/api_util.py:450, in shaped_abstractify(x)\n    449 try:\n--> 450   return _shaped_abstractify_handlers[type(x)](x)\n    451 except KeyError:\n\nKeyError: <class 'type'>\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/dtypes.py:101, in _canonicalize_dtype(x64_enabled, allow_opaque_dtype, dtype)\n    100 try:\n--> 101   dtype_ = np.dtype(dtype)\n    102 except TypeError as e:\n\nTypeError: Cannot interpret '<attribute 'dtype' of 'numpy.ndarray' objects>' as a data type\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nFile /project_title/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:214, in CompiledFunction.get_runtime_signature(*args)\n    213 for arg in args:\n--> 214     r_sig.append(jax.api_util.shaped_abstractify(arg))\n    215 return r_sig\n\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/api_util.py:452, in shaped_abstractify(x)\n    451 except KeyError:\n--> 452   return _shaped_abstractify_slow(x)\n\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/api_util.py:441, in _shaped_abstractify_slow(x)\n    440 if hasattr(x, 'dtype'):\n--> 441   dtype = dtypes.canonicalize_dtype(x.dtype, allow_opaque_dtype=True)\n    442 else:\n\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/dtypes.py:117, in canonicalize_dtype(dtype, allow_opaque_dtype)\n    116 def canonicalize_dtype(dtype: Any, allow_opaque_dtype: bool = False) -> Union[DType, OpaqueDType]:\n--> 117   return _canonicalize_dtype(config.x64_enabled, allow_opaque_dtype, dtype)\n\nFile /project_title/.venv/lib/python3.11/site-packages/jax/_src/dtypes.py:103, in _canonicalize_dtype(x64_enabled, allow_opaque_dtype, dtype)\n    102 except TypeError as e:\n--> 103   raise TypeError(f'dtype {dtype!r} not understood') from e\n    105 if x64_enabled:\n\nTypeError: dtype <attribute 'dtype' of 'numpy.ndarray' objects> not understood\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 8\n      5 import matplotlib.pyplot as plt\n      7 import quantum.project_title.data.data_generation as data_gen\n----> 8 from quantum.project_title.circuit_components.ansatz import StronglyEntanglingLayer\n      9 from quantum.project_title.circuit_components.data_loader import PriceLoader\n     10 from quantum.project_title.utils.utils import (angle_encode_spot_price, \n     11                                                                normalize_option_price, \n     12                                                                decode, \n     13                                                                calculate_delta)\n\nFile /project_title/quantum/project_title/circuit_components/ansatz.py:7\n      3 from pennylane.numpy import pi, tensor\n      4 from pennylane.operation import Operation\n----> 7 class StronglyEntanglingLayer(Operation):\n      8     num_params = 6\n      9     num_wires = 2\n\nFile /project_title/quantum/project_title/circuit_components/ansatz.py:13, in StronglyEntanglingLayer()\n     10 grad_method = None\n     11 grad_recipe = ([[0.5, 1, pi / 2], [-0.5, 1, -pi / 2]],)\n---> 13 @qjit\n     14 def compute_decomposition(\n     15     weight0: tensor,\n     16     weight1: tensor,\n     17     weight2: tensor,\n     18     weight3: tensor,\n     19     weight4: tensor,\n     20     weight5: tensor,\n     21     wires: list,\n     22 ):\n     23     op_list = [\n     24         qml.RZ(weight0, wires=wires[0]),\n     25         qml.RZ(weight1, wires=wires[1]),\n   (...)\n     31         qml.CNOT(wires=[wires[1], wires[0]]),\n     32     ]\n     33     return op_list\n\nFile /project_title/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:675, in qjit(fn, target, keep_intermediate, verbose, logfile)\n    588 \"\"\"A just-in-time decorator for PennyLane and JAX programs using Catalyst.\n    589 \n    590 This decorator enables both just-in-time and ahead-of-time compilation,\n   (...)\n    671         :class:`~.pennylane_extensions.QJITDevice`.\n    672 \"\"\"\n    674 if fn is not None:\n--> 675     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n    677 def wrap_fn(fn):\n    678     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n\nFile /project_title/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:484, in QJIT.__init__(self, fn, target, keep_intermediate, compile_options)\n    482 self.user_typed = True\n    483 if target in (\"mlir\", \"binary\"):\n--> 484     self.mlir_module = self.get_mlir(*parameter_types)\n    485 if target == \"binary\":\n    486     self.compiled_function = self.compile()\n1 Reply", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/1"}, "1": {"author": "isaacdevlugt", "date": "1687789595028", "content": "Hey @Nikhil_Narayanan!\nI think there\u2019s some of the error message missing. Is that all of the output?", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/2"}, "2": {"author": "Nikhil_Narayanan", "date": "1687790587624", "content": "\n\n\n Nikhil_Narayanan:\n\nion = self.compile()\n\n\nYes my bad, the bottom of the error was cut off:\nFile /project_title/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:529, in QJIT.get_mlir(self, *args)\n    520 def get_mlir(self, *args):\n    521     \"\"\"Trace self.qfunc\n    522 \n    523     Args:\n   (...)\n    527         an MLIR module\n    528     \"\"\"\n--> 529     self.c_sig = CompiledFunction.get_runtime_signature(*args)\n    531     with Patcher(\n    532         (qml.QNode, \"__call__\", QFunc.__call__),\n    533     ):\n    534         mlir_module, ctx, jaxpr = tracer.get_mlir(self.qfunc, *self.c_sig)\n\nFile /project_title/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:218, in CompiledFunction.get_runtime_signature(*args)\n    216 except Exception as exc:\n    217     arg_type = type(arg)\n--> 218     raise TypeError(f\"Unsupported argument type: {arg_type}\") from exc\n\nTypeError: Unsupported argument type: <class 'type'>\n", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/3"}, "3": {"author": "isaacdevlugt", "date": "1687794403289", "content": "Thanks! So you\u2019re just trying to import the above operation elsewhere and that\u2019s giving you the error you\u2019re seeing?1", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/4"}, "4": {"author": "Nikhil_Narayanan", "date": "1687800682383", "content": "Yes the error happens at the import", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/5"}, "5": {"author": "isaacdevlugt", "date": "1687802613999", "content": "I think the error is probably related to the fact that your class is incomplete. Check this out: Adding new operators \u2014 PennyLane 0.30.0 documentation\nIt looks like you\u2019re missing __init__!", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/6"}, "6": {"author": "isaacdevlugt", "date": "1687804237662", "content": "Okay, just spoke to some of our development team for Catalyst. It\u2019s a relatively new project for me, so apologies that I didn\u2019t understand what the issue was immediately! \nThis is what they said:\n\nIt appears that you are trying to return a list of operations. This is not currently allowed inside functions that will be JIT-compiled.\n\nIt would be good to know what you\u2019re trying to do . What\u2019s your end goal?", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/7"}, "7": {"author": "Nikhil_Narayanan", "date": "1687947952104", "content": "I am trying to call the function as below. I\u2019m trying to train a variational circuit that does regression  while taking advantage of the JAX features in Catalyst\n@qml.qnode(dev, diff_method = \"parameter-shift\")\ndef qnn(phi, *weights):\n    \"\"\"\n    Input a numpy array feature (which encodes a single normalized angle encoded spot price)\n    \"\"\"\n    qml.RY(phi, wires=0)\n    qml.RY(phi, wires=1)\n    qml.IsingXX(phi, wires=[0, 1])\n    qml.RY(phi, wires=0)\n    qml.RY(phi, wires=1)\n    for i in range(3):\n        StronglyEntanglingLayer(*weights[(i * 6):((i + 1) * 6)], wires=[0,1])\n    return qml.expval(qml.PauliZ(0))\n\nMoreover, my other functions are as below:\n@qjit\ndef network_fn(angle_encoded_spot, *weights):\n    quantum_out = qnn(angle_encoded_spot, *weights)\n    return (quantum_out + 1) / 2\n\n# Cost function using output of Q-Node\n@qjit\ndef param_shift_cost(target, angle_encoded_spot, *weights):\n    output = network_fn(angle_encoded_spot, *weights)\n    return (output - target) ** 2 / target\n", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/8"}, "8": {"author": "isaacdevlugt", "date": "1687964055214", "content": "Hey @Nikhil_Narayanan! I spoke to some catalyst developers and here\u2019s what the solution is.\n\nremove @qjit from custom_decomposition.\nadd @staticmethod to custom_decomposition\n\ncompute_decomposition is QJIT\u2019d regardless, so no need to do it here :). One more thing to change is network_fn:\n@qjit\ndef network_fn(angle_encoded_spot, *weights):\n    quantum_out = qnn(angle_encoded_spot, *weights)\n    return (quantum_out[0] + 1) / 2\n\nSide note: in the next release of Catalyst, you\u2019ll be able to do this:\n@qjit\ndef network_fn(angle_encoded_spot, *weights):\n    quantum_out = qnn(angle_encoded_spot, *weights)\n    return (quantum_out + 1) / 2\n\nHere\u2019s a complete example:\nfrom catalyst import qjit, grad\nimport pennylane as qml\nimport numpy as np\nfrom pennylane.numpy import pi, tensor\n\nfrom jax import numpy as jnp\nfrom catalyst import qjit\n\nclass StronglyEntanglingLayer(qml.operation.Operation):\n    num_params = 6\n    num_wires = 2\n    grad_method = None\n    grad_recipe = ([[0.5, 1, np.pi / 2], [-0.5, 1, -np.pi / 2]],)\n\n    def __init__(self, *weights, wires=None):\n        super().__init__(*weights, wires=wires)\n\n    @staticmethod\n    def compute_decomposition(w0, w1, w2, w3, w4, w5, wires):\n        op_list = [\n            qml.RZ(w0, wires=wires[0]),\n            qml.RZ(w1, wires=wires[1]),\n            qml.RY(w2, wires=wires[0]),\n            qml.RY(w3, wires=wires[1]),\n            qml.RZ(w4, wires=wires[0]),\n            qml.RZ(w5, wires=wires[1]),\n            qml.CNOT(wires=[wires[0], wires[1]]),\n            qml.CNOT(wires=[wires[1], wires[0]]),\n        ]\n        return op_list\n\n@qml.qnode(qml.device(\"lightning.qubit\", wires=2), diff_method = \"parameter-shift\")\ndef qnn(phi, weights):\n    \"\"\"\n    Input a numpy array feature (which encodes a single normalized angle encoded spot price)\n    \"\"\"\n    qml.RY(phi, wires=0)\n    qml.RY(phi, wires=1)\n    qml.IsingXX(phi, wires=[0, 1])\n    qml.RY(phi, wires=0)\n    qml.RY(phi, wires=1)\n    for i in range(3):\n        StronglyEntanglingLayer(*weights[(i * 6):((i + 1) * 6)], wires=[0,1])\n    return qml.expval(qml.PauliZ(0))\n\n@qjit\ndef network_fn(angle_encoded_spot, weights):\n    quantum_out = qnn(angle_encoded_spot, weights)\n    return (quantum_out + 1) / 2\n\n# Cost function using output of Q-Node\n@qjit\ndef param_shift_cost(target, angle_encoded_spot, weights):\n    output = network_fn(angle_encoded_spot, weights)\n    return (output - target) ** 2 / target\n\nprint(param_shift_cost(1.0, 3.14, jnp.array(list(range(18)))))\nprint(param_shift_cost.mlir)\n\nLet me know if this helps!", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/9"}, "9": {"author": "Nikhil_Narayanan", "date": "1687975675266", "content": "\n\n\npennylane.ai\n\n\n\nIntroducing Catalyst: quantum just-in-time compilation 1\nCatalyst, our next-generation compilation engine, brings just-in-time compilation to quantum programming. Automatically compile hybrid programs for execution on CPUs, GPUs, and QPUs, and unlock new ways of programming quantum computers.\n\n\n\n\n\n\nI want to try to train my network using Adam optimizer - I am planning on using optax as shown here: Stochastic optimization \u2014 JAXopt 0.7 documentation 1. I will try this tomorrow, but am unsure whether this would be supported to QJIT using catalyst?", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/10"}, "10": {"author": "isaacdevlugt", "date": "1688074113707", "content": "@Nikhil_Narayanan did my previous response help? Let me know if you run into any more issues!", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/11"}, "11": {"author": "Nikhil_Narayanan", "date": "1688129290322", "content": "I think my main question currently is if jaxopt and optax are compatible with Catalyst currently?\nHi your previous response helped, but I\u2019m having a lot of trouble in the training step; I imported jaxopt and optax and below is the code which I\u2019m using to train\ndef data_iterator(rng_key):\n     \n    rng_key, subkey = jax.random.split(rng_key)\n    perm = jax.random.permutation(rng_key, jnp.array(range(len(training_x))))\n    for index in perm:\n        yield (training_x[index], training_y[index])\n\n@qjit\ndef optimisation():\n    \n    # initial weights\n    init_weights = jnp.array([0.6191368085366578, \n                -0.22716372247007463, \n                0.39964323632317433, \n                1.0533182225902902, \n                1.1384541354087307, \n                -0.31927423082968687, \n                -0.38598253427438795, \n                1.567254004140948, \n                1.8395378005048666, \n                0.4180817001090079, \n                1.062053061058973, \n                0.5349723405892166, \n                0.36431094419401455, \n                1.629228622648432, \n                0.2990813054605557, \n                -0.6528184516978838, \n                0.6739291834199453, \n                0.27512818131726857])\n    \n    # define optimizer\n    opt = optax.adam(0.01)\n    solver = OptaxSolver(opt=opt, fun=param_shift_cost, maxiter=80)\n    rng_key = jax.random.PRNGKey(42)\n\n    iterator = data_iterator(rng_key)\n    solver.run_iterator(init_weights, iterator)\n\nHowever, I get the following errors when I run the above:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[27], line 8\n      5     for index in perm:\n      6         yield (training_x[index], training_y[index])\n----> 8 @qjit\n      9 def optimisation():\n     10     \n     11     # initial weights\n     12     init_weights = jnp.array([0.6191368085366578, \n     13                 -0.22716372247007463, \n     14                 0.39964323632317433, \n   (...)\n     28                 0.6739291834199453, \n     29                 0.27512818131726857])\n     31     # define optimizer\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:675, in qjit(fn, target, keep_intermediate, verbose, logfile)\n    588 \"\"\"A just-in-time decorator for PennyLane and JAX programs using Catalyst.\n    589 \n    590 This decorator enables both just-in-time and ahead-of-time compilation,\n   (...)\n    671         :class:`~.pennylane_extensions.QJITDevice`.\n    672 \"\"\"\n    674 if fn is not None:\n--> 675     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n    677 def wrap_fn(fn):\n    678     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:484, in QJIT.__init__(self, fn, target, keep_intermediate, compile_options)\n    482 self.user_typed = True\n    483 if target in (\"mlir\", \"binary\"):\n--> 484     self.mlir_module = self.get_mlir(*parameter_types)\n    485 if target == \"binary\":\n    486     self.compiled_function = self.compile()\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:534, in QJIT.get_mlir(self, *args)\n    529 self.c_sig = CompiledFunction.get_runtime_signature(*args)\n    531 with Patcher(\n    532     (qml.QNode, \"__call__\", QFunc.__call__),\n    533 ):\n--> 534     mlir_module, ctx, jaxpr = tracer.get_mlir(self.qfunc, *self.c_sig)\n    536 mod = mlir_module.operation\n    537 self._jaxpr = jaxpr\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/jax_tracer.py:64, in get_mlir(func, *args, **kwargs)\n     61 jprim.mlir_fn_cache.clear()\n     63 with TracingContext():\n---> 64     jaxpr = jax.make_jaxpr(func)(*args, **kwargs)\n     66 nrep = jaxpr_replicas(jaxpr)\n     67 effects = [eff for eff in jaxpr.effects if eff in jax.core.ordered_effects]\n\n    [... skipping hidden 6 frame]\n\nCell In[27], line 37, in optimisation()\n     34 rng_key = jax.random.PRNGKey(42)\n     36 iterator = data_iterator(rng_key)\n---> 37 solver.run_iterator(init_weights, iterator)\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/base.py:399, in StochasticSolver.run_iterator(self, init_params, iterator, *args, **kwargs)\n    396 # TODO(mblondel): try and benchmark lax.fori_loop with host_call for `next`.\n    397 for data in itertools.islice(iterator, 0, self.maxiter):\n--> 399   params, state = self.update(params, state, *args, **kwargs, data=data)\n    401 return OptStep(params=params, state=state)\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/optax_wrapper.py:141, in OptaxSolver.update(self, params, state, *args, **kwargs)\n    138 if self.pre_update:\n    139   params, state = self.pre_update(params, state, *args, **kwargs)\n--> 141 (value, aux), grad = self._value_and_grad_fun(params, *args, **kwargs)\n    143 delta, opt_state = self.opt.update(grad, state.internal_state, params)\n    144 params = self._apply_updates(params, delta)\n\n    [... skipping hidden 8 frame]\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/base.py:70, in _add_aux_to_fun.<locals>.fun_with_aux(*a, **kw)\n     69 def fun_with_aux(*a, **kw):\n---> 70   return fun(*a, **kw), None\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:561, in QJIT.__call__(self, *args, **kwargs)\n    559 def __call__(self, *args, **kwargs):\n    560     if TracingContext.is_tracing():\n--> 561         return self.qfunc(*args, **kwargs)\n    563     if any(isinstance(arg, jax.core.Tracer) for arg in args):\n    564         raise ValueError(\n    565             \"Cannot use JAX to trace through a qjit compiled function. If you attempted \"\n    566             \"to use jax.jit or jax.grad, please use their equivalent from Catalyst.\"\n    567         )\n\nTypeError: param_shift_cost() got multiple values for argument 'data'\n2 Replies", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/12"}, "12": {"author": "isaacdevlugt", "date": "1688132724300", "content": "Are you able to get your code working if you loop manually? I.e., what\u2019s depicted here: Stochastic optimization \u2014 JAXopt 0.7 documentation\nIt would be good to see if your code works manually updating over one data point (no batching).", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/13"}, "13": {"author": "Nikhil_Narayanan", "date": "1688139975935", "content": "I\u2019ve been trying the following with no luck (without batching) - I\u2019ll continue trying to debug and let you know if I get it to work\ndef data_iterator(rng_key):  \n    rng_key, subkey = jax.random.split(rng_key)\n    perm = jax.random.permutation(rng_key, jnp.array(range(len(training_x))))\n    for index in perm:\n        yield (training_x[index], training_y[index])\n\n@qjit\ndef optimisation():\n    \n    # initial weights\n    init_weights = jnp.array([0.6191368085366578, \n                -0.22716372247007463, \n                0.39964323632317433, \n                1.0533182225902902, \n                1.1384541354087307, \n                -0.31927423082968687, \n                -0.38598253427438795, \n                1.567254004140948, \n                1.8395378005048666, \n                0.4180817001090079, \n                1.062053061058973, \n                0.5349723405892166, \n                0.36431094419401455, \n                1.629228622648432, \n                0.2990813054605557, \n                -0.6528184516978838, \n                0.6739291834199453, \n                0.27512818131726857])\n    \n    # define optimizer\n    opt = optax.adam(0.01)\n    solver = OptaxSolver(opt=opt, fun=param_shift_cost, maxiter=80)\n    rng_key = jax.random.PRNGKey(42)\n\n    iterator = data_iterator(rng_key)\n    solver.run(init_weights, angle_encoded_spot = training_x[0], target = training_y[0])\n\nBelow is the error that I got:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[47], line 7\n      4     for index in perm:\n      5         yield (training_x[index], training_y[index])\n----> 7 @qjit\n      8 def optimisation():\n      9     \n     10     # initial weights\n     11     init_weights = jnp.array([0.6191368085366578, \n     12                 -0.22716372247007463, \n     13                 0.39964323632317433, \n   (...)\n     27                 0.6739291834199453, \n     28                 0.27512818131726857])\n     30     # define optimizer\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:675, in qjit(fn, target, keep_intermediate, verbose, logfile)\n    588 \"\"\"A just-in-time decorator for PennyLane and JAX programs using Catalyst.\n    589 \n    590 This decorator enables both just-in-time and ahead-of-time compilation,\n   (...)\n    671         :class:`~.pennylane_extensions.QJITDevice`.\n    672 \"\"\"\n    674 if fn is not None:\n--> 675     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n    677 def wrap_fn(fn):\n    678     return QJIT(fn, target, keep_intermediate, CompileOptions(verbose, logfile))\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:484, in QJIT.__init__(self, fn, target, keep_intermediate, compile_options)\n    482 self.user_typed = True\n    483 if target in (\"mlir\", \"binary\"):\n--> 484     self.mlir_module = self.get_mlir(*parameter_types)\n    485 if target == \"binary\":\n    486     self.compiled_function = self.compile()\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:534, in QJIT.get_mlir(self, *args)\n    529 self.c_sig = CompiledFunction.get_runtime_signature(*args)\n    531 with Patcher(\n    532     (qml.QNode, \"__call__\", QFunc.__call__),\n    533 ):\n--> 534     mlir_module, ctx, jaxpr = tracer.get_mlir(self.qfunc, *self.c_sig)\n    536 mod = mlir_module.operation\n    537 self._jaxpr = jaxpr\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/jax_tracer.py:64, in get_mlir(func, *args, **kwargs)\n     61 jprim.mlir_fn_cache.clear()\n     63 with TracingContext():\n---> 64     jaxpr = jax.make_jaxpr(func)(*args, **kwargs)\n     66 nrep = jaxpr_replicas(jaxpr)\n     67 effects = [eff for eff in jaxpr.effects if eff in jax.core.ordered_effects]\n\n    [... skipping hidden 6 frame]\n\nCell In[47], line 37, in optimisation()\n     35 iterator = data_iterator(rng_key)\n     36 print(training_x[0])\n---> 37 solver.run(init_weights, angle_encoded_spot = training_x[0], target = training_y[0])\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/base.py:354, in IterativeSolver.run(self, init_params, *args, **kwargs)\n    347   decorator = idf.custom_root(\n    348       self.optimality_fun,\n    349       has_aux=True,\n    350       solve=self.implicit_diff_solve,\n    351       reference_signature=reference_signature)\n    352   run = decorator(run)\n--> 354 return run(init_params, *args, **kwargs)\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/base.py:316, in IterativeSolver._run(self, init_params, *args, **kwargs)\n    298 # We unroll the very first iteration. This allows `init_val` and `body_fun`\n    299 # below to have the same output type, which is a requirement of\n    300 # lax.while_loop and lax.scan.\n   (...)\n    311 # of a `lax.cond` for now in order to avoid staging the initial\n    312 # update and the run loop. They might not be staging compatible.\n    314 zero_step = self._make_zero_step(init_params, state)\n--> 316 opt_step = self.update(init_params, state, *args, **kwargs)\n    317 init_val = (opt_step, (args, kwargs))\n    319 jit, unroll = self._get_loop_options()\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/optax_wrapper.py:141, in OptaxSolver.update(self, params, state, *args, **kwargs)\n    138 if self.pre_update:\n    139   params, state = self.pre_update(params, state, *args, **kwargs)\n--> 141 (value, aux), grad = self._value_and_grad_fun(params, *args, **kwargs)\n    143 delta, opt_state = self.opt.update(grad, state.internal_state, params)\n    144 params = self._apply_updates(params, delta)\n\n    [... skipping hidden 8 frame]\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/jaxopt/_src/base.py:70, in _add_aux_to_fun.<locals>.fun_with_aux(*a, **kw)\n     69 def fun_with_aux(*a, **kw):\n---> 70   return fun(*a, **kw), None\n\nFile /quantum_risk_engine/.venv/lib/python3.11/site-packages/catalyst/compilation_pipelines.py:561, in QJIT.__call__(self, *args, **kwargs)\n    559 def __call__(self, *args, **kwargs):\n    560     if TracingContext.is_tracing():\n--> 561         return self.qfunc(*args, **kwargs)\n    563     if any(isinstance(arg, jax.core.Tracer) for arg in args):\n    564         raise ValueError(\n    565             \"Cannot use JAX to trace through a qjit compiled function. If you attempted \"\n    566             \"to use jax.jit or jax.grad, please use their equivalent from Catalyst.\"\n    567         )\n\nTypeError: param_shift_cost() got multiple values for argument 'angle_encoded_spot'\n", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/14"}, "14": {"author": "isaacdevlugt", "date": "1688148724388", "content": "Can you provide a small dummy dataset that we can use to try and replicate the error on our end?", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/15"}, "15": {"author": "josh", "date": "1688150843300", "content": "\n\n\n Nikhil_Narayanan:\n\nI think my main question currently is if jaxopt and optax are compatible with Catalyst currently?\n\n\nHey @Nikhil_Narayanan! There is a slightly subtlety to your question \u2013 when you say compatible, do you mean inside the qjit or outside the qjit?\n\n\nOutside the qjit, the qjitted function will look like a regular Python function, so will work with either jaxopt or optax. However, JAX won\u2019t know how to differentiate a qjit function natively, so you would need to pass the optimizer both the cost function and the gradient function. For example,\ndef cost(params)\n\n@qjit\ndef cost_and_grad(params):\n    grad = catalyst.grad(cost, argnum=0)\n    return cost(params), grad(params)[0]\n\nopt = jaxopt.GradientDescent(cost_and_grad, stepsize=0.4, value_and_grad=True)\n\n\n\nInside the qjit, it is a little more complicated \u2013 you need to make sure the optimizer you are working with is jax.jit compatible. This is the case for jaxopt (and you can see an example in the docs 1), but I haven\u2019t tested optax. In either case, you still need to use the trick above.\n\n\n\nNote: Catalyst version 0.2.0, coming out soon, has better JAX integration, and will make it easier to integrate with the optimizers 1", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/16"}, "16": {"author": "Nikhil_Narayanan", "date": "1688393051535", "content": "Thank you @isaacdevlugt and @josh! This clarifies it.1", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/17"}, "17": {"author": "isaacdevlugt", "date": "1688477029549", "content": "Awesome! Glad we were able to help here. And thanks for taking Catalyst for a spin! ", "link": "https://discuss.pennylane.ai//t/extending-operation-with-qjit/3100/18"}}