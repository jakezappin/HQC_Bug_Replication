{"0": {"author": "Kuma-quant", "date": "1624170259985", "content": "I\u2019m now evaluating the performance of quantum nerural networks in simulation.\nHowever, the computiational time easily reachs to a few hours.\nIt is difficult to exploit the hyper parameter (e.g. num. of layers, qubits, gate type).\nThe condition is,\n\nLap-top PC\n2 qubits\n12 variational parameters (Two StronglyEntanglingLayers)\n2 dimentional input feature x 16384 data (corresponds to 1 epoch)\nloss func. : MSE\nlightning.qubit\ndiff_method = \u2018adjoint\u2019\noptimizer: Adam (from qml.optimizers)\n\nThe computational time is approximately 100 sec /epoch.\nTherefore, in case of ~100 epochs, the total time becomes 2~3 hours.\nI have tested a lot of ways.\n\n\nqulacs.simulator is fast, but the diff_method should be paramer-shift. Threrefore, it is not fast in case of gradient-descent based learning.\nTo date, lightning.qubit would be fastest.\n\n\nA fastest diff_method is adjoint. \u201cBackprop\u201d is not fast, at least, when the number of parameter is not so large.\n\n\nHow can I do for further speeding up?\n\u201cUsing better machine as GPU cluster\u201d  is only solution?", "link": "https://discuss.pennylane.ai//t/how-can-i-make-variational-learning-faster/1131/1"}, "1": {"author": "Kuma-quant", "date": "1624171366440", "content": "Here is a sample code.\nimport pennylane as qml\nfrom pennylane import numpy as np\n\nnum_of_data = 16384\nX =  np.random.uniform(high=1, size=(num_of_data,2))\nY =  np.random.uniform(high=1, size=(num_of_data,1))\n\n########  parameters#############\nn_qubits = 2 ## num_qubit\nn_layers = 2 # num_layer\ndev = qml.device(\"lightning.qubit\", wires=n_qubits, shots=None) # define a device\n\n# Initial circuit parameters\nvar_init = np.random.uniform(high=2 * np.pi, size=(n_layers, n_qubits, 3))\n\n@qml.qnode(dev, diff_method='adjoint')\ndef quantum_neural_net(var, x):\n    qml.templates.AngleEmbedding(x, wires=range(n_qubits))\n    qml.templates.StronglyEntanglingLayers(var, wires=range(n_qubits))\n    return qml.expval(qml.PauliZ(0))\n        \ndef square_loss(desired, predictions):\n    loss = 0\n    for l, p in zip(desired, predictions):\n        loss = loss + (l - p) ** 2\n    loss = loss / len(desired)\n    return loss\n\ndef cost(var, features, desired):\n    preds = [quantum_neural_net(var, x) for x in features]\n    return square_loss(desired, preds)\n\nopt = qml.AdamOptimizer(0.1, beta1=0.9, beta2=0.999)\nimport time\n\nhist_cost = []\nvar = var_init\nfor it in range(50):\n    t1 = time.time() \n    var, _cost = opt.step_and_cost(lambda v: cost(v, X, Y), var)\n    t2 = time.time() \n    elapsed_time = t2-t1\n    print(\"Iter:\"+str(it)+\", cost=\"+str(_cost.numpy()))\n    print(f\"Time\uff1a{elapsed_time}\")\n    hist_cost.append(_cost)\n\nIter:0, cost=[0.22944678] Time : 120.97000002861023 sec", "link": "https://discuss.pennylane.ai//t/how-can-i-make-variational-learning-faster/1131/2"}, "2": {"author": "christina", "date": "1624285662853", "content": "Hi @Kuma-quant \nI\u2019m running some profiling to check out what the bottlenecks are, but I have two ideas that might help.\nFirst, try initializing the QNode with mutable=False.  The structure of the circuit doesn\u2019t change with you parameters, so you don\u2019t need to rebuild the circuit each time.\n@qml.qnode(dev, diff_method=\"adjoint\", mutable=False)\nNext, we have a release coming out tonight that will have about a 15% decrease in adjoint speed. So either get that release once it gets uploaded, or install PennyLane from source right now.\nHope those help ", "link": "https://discuss.pennylane.ai//t/how-can-i-make-variational-learning-faster/1131/3"}, "3": {"author": "Kuma-quant", "date": "1624369366788", "content": "Dear christina-san,\nThank you for kind advices!\nI set the mutable option as False, and then I got approximately 15% speeding up.\nFantastic.\nI also upgraded PennyLane to 0.16.0.\nThe speed looks improved a little.\nI\u2019ll check the bottlenecks in detail for further speeding up.\nThanks.", "link": "https://discuss.pennylane.ai//t/how-can-i-make-variational-learning-faster/1131/4"}, "4": {"author": "Kuma-quant", "date": "1624170259985", "content": "I\u2019m now evaluating the performance of quantum nerural networks in simulation.\nHowever, the computiational time easily reachs to a few hours.\nIt is difficult to exploit the hyper parameter (e.g. num. of layers, qubits, gate type).\nThe condition is,\n\nLap-top PC\n2 qubits\n12 variational parameters (Two StronglyEntanglingLayers)\n2 dimentional input feature x 16384 data (corresponds to 1 epoch)\nloss func. : MSE\nlightning.qubit\ndiff_method = \u2018adjoint\u2019\noptimizer: Adam (from qml.optimizers)\n\nThe computational time is approximately 100 sec /epoch.\nTherefore, in case of ~100 epochs, the total time becomes 2~3 hours.\nI have tested a lot of ways.\n\n\nqulacs.simulator is fast, but the diff_method should be paramer-shift. Threrefore, it is not fast in case of gradient-descent based learning.\nTo date, lightning.qubit would be fastest.\n\n\nA fastest diff_method is adjoint. \u201cBackprop\u201d is not fast, at least, when the number of parameter is not so large.\n\n\nHow can I do for further speeding up?\n\u201cUsing better machine as GPU cluster\u201d  is only solution?", "link": "https://discuss.pennylane.ai//t/how-can-i-make-variational-learning-faster/1131/5"}}