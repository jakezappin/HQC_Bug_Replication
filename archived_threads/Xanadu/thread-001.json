{"0": {"author": "Muhammad_Kashif", "date": "1654007667729", "content": "Hi all,\nI am working on a problem where my input feature size is 64. I wanted to encode my features in qubit amplitudes. Given the input size I need atleast 6 qubits to encode my data. Works fine\u2026 \nHowever when I increased the number of qubits, the input size is same (64). I did use the pad-with argument in AmplitudeEmbedding function. around a week back it was working fine. can some please have a look. Below is my code snippet and error traceback.\nSample Code:\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\ndataset import and split\nx_train, x_test = datasets.load_digits().data, datasets.load_digits().target\nx_train, x_test, y_train, y_test = train_test_split(\n        x_train, x_test, test_size=0.25, random_state=42)\n\nQlayer\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n\nQNODE\nn_qubits = 8\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits), rotation = qml.RY)\n    return [qml.expval(qml.PauliZ(wires=[i])) for i in range(n_qubits)]\n        n_layers = 4\n        weight_shapes = {\"weights\": (n_layers, n_qubits)}\n\nModel\nclayer = tf.keras.layers.Dense(10, activation=\"softmax\")\nmodel = tf.keras.models.Sequential([qlayer, clayer])\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.2)\nmodel.compile(opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\ntraining\nhistory = model.fit(x_train, y_train, epochs=50, batch_size=16, validation_data=(x_test, y_test))\n\nError Traceback:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-120-394f1966082d> in <module>\n----> 1 history = model.fit(x_train, y_train, epochs=50, batch_size=16, validation_data=(x_test, y_test))\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1181                 _r=1):\n   1182               callbacks.on_train_batch_begin(step)\n-> 1183               tmp_logs = self.train_function(iterator)\n   1184               if data_handler.should_sync:\n   1185                 context.async_wait()\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in train_function(iterator)\n    853       def train_function(iterator):\n    854         \"\"\"Runs a training execution with one step.\"\"\"\n--> 855         return step_function(self, iterator)\n    856 \n    857     else:\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in step_function(model, iterator)\n    843 \n    844       data = next(iterator)\n--> 845       outputs = model.distribute_strategy.run(run_step, args=(data,))\n    846       outputs = reduce_per_replica(\n    847           outputs, self.distribute_strategy, reduction='first')\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in run(***failed resolving arguments***)\n   1283       fn = autograph.tf_convert(\n   1284           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n-> 1285       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n   1286 \n   1287   def reduce(self, reduce_op, value, axis):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\n   2831       kwargs = {}\n   2832     with self._container_strategy().scope():\n-> 2833       return self._call_for_each_replica(fn, args, kwargs)\n   2834 \n   2835   def _call_for_each_replica(self, fn, args, kwargs):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\n   3606   def _call_for_each_replica(self, fn, args, kwargs):\n   3607     with ReplicaContext(self._container_strategy(), replica_id_in_sync_group=0):\n-> 3608       return fn(*args, **kwargs)\n   3609 \n   3610   def _reduce_to(self, reduce_op, value, destinations, options):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\n    595   def wrapper(*args, **kwargs):\n    596     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n--> 597       return func(*args, **kwargs)\n    598 \n    599   if inspect.isfunction(func) or inspect.ismethod(func):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in run_step(data)\n    836 \n    837       def run_step(data):\n--> 838         outputs = model.train_step(data)\n    839         # Ensure counter is updated only if `train_step` succeeds.\n    840         with ops.control_dependencies(_minimum_control_deps(outputs)):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in train_step(self, data)\n    793     # Run forward pass.\n    794     with backprop.GradientTape() as tape:\n--> 795       y_pred = self(x, training=True)\n    796       loss = self.compiled_loss(\n    797           y, y_pred, sample_weight, regularization_losses=self.losses)\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\n   1028         with autocast_variable.enable_auto_cast_variables(\n   1029             self._compute_dtype_object):\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\n   1031 \n   1032         if self._activity_regularizer:\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py in call(self, inputs, training, mask)\n    378       if not self.built:\n    379         self._init_graph_network(self.inputs, self.outputs)\n--> 380       return super(Sequential, self).call(inputs, training=training, mask=mask)\n    381 \n    382     outputs = inputs  # handle the corner case where self.layers is empty\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in call(self, inputs, training, mask)\n    418         a list of tensors if there are more than one outputs.\n    419     \"\"\"\n--> 420     return self._run_internal_graph(\n    421         inputs, training=training, mask=mask)\n    422 \n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in _run_internal_graph(self, inputs, training, mask)\n    554 \n    555         args, kwargs = node.map_arguments(tensor_dict)\n--> 556         outputs = node.layer(*args, **kwargs)\n    557 \n    558         # Update tensor_dict.\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\n   1028         with autocast_variable.enable_auto_cast_variables(\n   1029             self._compute_dtype_object):\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\n   1031 \n   1032         if self._activity_regularizer:\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in call(self, inputs)\n    299             reconstructor = []\n    300             for x in tf.unstack(inputs):\n--> 301                 reconstructor.append(self.call(x))\n    302             return tf.stack(reconstructor)\n    303 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in call(self, inputs)\n    302             return tf.stack(reconstructor)\n    303 \n--> 304         return self._evaluate_qnode(inputs)\n    305 \n    306     def _evaluate_qnode(self, x):\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in _evaluate_qnode(self, x)\n    317             **{k: 1.0 * w for k, w in self.qnode_weights.items()},\n    318         }\n--> 319         return self.qnode(**kwargs)\n    320 \n    321     def compute_output_shape(self, input_shape):\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnode.py in __call__(self, *args, **kwargs)\n    564 \n    565         # construct the tape\n--> 566         self.construct(args, kwargs)\n    567 \n    568         cache = self.execute_kwargs.get(\"cache\", False)\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnode.py in construct(self, args, kwargs)\n    481 \n    482         with self.tape:\n--> 483             self._qfunc_output = self.func(*args, **kwargs)\n    484         self._tape._qfunc_output = self._qfunc_output\n    485 \n\n<ipython-input-117-45acf92435fe> in qnode(inputs, weights)\n      4 def qnode(inputs, weights):\n      5 \n----> 6     qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n      7     qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits), rotation = qml.RY)\n      8 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py in __init__(self, features, wires, pad_with, normalize, do_queue, id)\n    127         self.pad_with = pad_with\n    128         self.normalize = normalize\n--> 129         features = self._preprocess(features, wires, pad_with, normalize)\n    130         super().__init__(features, wires=wires, do_queue=do_queue, id=id)\n    131 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py in _preprocess(features, wires, pad_with, normalize)\n    206                 padding = [pad_with] * (2 ** len(wires) - n_features)\n    207                 if (\n--> 208                     hasattr(feature_set, \"device\") and feature_set.device.type == \"cuda\"\n    209                 ):  # pragma: no cover\n    210                     ## Torch tensor, send to same GPU\n\nAttributeError: 'str' object has no attribute 'type'\n\nAny help would be appreciated.\nThanks", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1654124048621", "content": "Hi @Muhammad_Kashif, I\u2019m not being able to fix your problem. Can you confirm that week ago it was working with the exact same code and now it\u2019s not working?\nCould you also please share the output of qml.about() ?", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/2"}, "2": {"author": "Muhammad_Kashif", "date": "1654150383621", "content": "Hi @CatalinaAlbornoz,\nThanks for looking into my query and responding.\nYes, I am pretty sure that almost the same code was working around a week back. I went upto 14 qubits and it was working. But now, i am (kind of) being forced to use the exactly n qubits for 2^n features when I am using amplitude encoding. Going beyond n results in error.\nSecondly, I checked that the AmplitudeEmbedding separately in the qnode works fine for the same input data as shown below .\nn_qubits = 10\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n@qml.qnode(dev)\ndef qnode(inputs,weights):\n    \n    qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits), rotation = qml.RY)\n    return [qml.expval(qml.PauliZ(wires=[i])) for i in range(n_qubits)]\n\nshape = qml.BasicEntanglerLayers.shape(n_layers=2, n_wires=10)\nw = np.random.random(size=shape)\nexp_val = qnode(inputs=x_train[1], weights=w) \nprint(exp_val)\n\nResult:\n[ 0.01450765  0.01874857  0.0038299   0.01640479  0.02875179 -0.00828447   0.02064254 -0.04912628  0.01601941 -0.06728389]\nSince the above code snippet works fine even with 10 qubits (which is more than 6 qubits for input size of 64), i believe the issue is somewhere, when we convert that qnode (utilizing pad_with in amplitude embedding) to KerasLayer and use it in model and train.\nBelow is the output of qml.about():\nName: PennyLane\nVersion: 0.23.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: /usr/local/lib/python3.8/site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, retworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning\n\nPlatform info:           Linux-5.8.0-59-generic-x86_64-with-glibc2.10\nPython version:          3.8.12\nNumpy version:           1.22.3\nScipy version:           1.7.3\nInstalled devices:\n- default.gaussian (PennyLane-0.23.1)\n- default.mixed (PennyLane-0.23.1)\n- default.qubit (PennyLane-0.23.1)\n- default.qubit.autograd (PennyLane-0.23.1)\n- default.qubit.jax (PennyLane-0.23.1)\n- default.qubit.tf (PennyLane-0.23.1)\n- default.qubit.torch (PennyLane-0.23.1)\n- lightning.qubit (PennyLane-Lightning-0.23.0).\n\nI hope this helps you diagnose and resolve the issue. Thanks for your help.\nRegards\u2026", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/3"}, "3": {"author": "Guillermo_Alonso", "date": "1654179069843", "content": "Hey! It seems that there is a problem with the types, for the moment, until we find the bug, it would be enough that you change this line\n    qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n\n\nfor this other\n    qml.templates.AmplitudeEmbedding([a for a in inputs], wires=range(n_qubits), pad_with = 0.,  normalize = True)\n\n\nlet me know if you have any problems 1", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/4"}, "4": {"author": "Muhammad_Kashif", "date": "1654243640467", "content": "Hi @Guillermo_Alonso,\nThe solution works fine. Thanks\u2026", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/5"}, "5": {"author": "Muhammad_Kashif", "date": "1654007667729", "content": "Hi all,\nI am working on a problem where my input feature size is 64. I wanted to encode my features in qubit amplitudes. Given the input size I need atleast 6 qubits to encode my data. Works fine\u2026 \nHowever when I increased the number of qubits, the input size is same (64). I did use the pad-with argument in AmplitudeEmbedding function. around a week back it was working fine. can some please have a look. Below is my code snippet and error traceback.\nSample Code:\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\ndataset import and split\nx_train, x_test = datasets.load_digits().data, datasets.load_digits().target\nx_train, x_test, y_train, y_test = train_test_split(\n        x_train, x_test, test_size=0.25, random_state=42)\n\nQlayer\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n\nQNODE\nn_qubits = 8\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits), rotation = qml.RY)\n    return [qml.expval(qml.PauliZ(wires=[i])) for i in range(n_qubits)]\n        n_layers = 4\n        weight_shapes = {\"weights\": (n_layers, n_qubits)}\n\nModel\nclayer = tf.keras.layers.Dense(10, activation=\"softmax\")\nmodel = tf.keras.models.Sequential([qlayer, clayer])\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.2)\nmodel.compile(opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\ntraining\nhistory = model.fit(x_train, y_train, epochs=50, batch_size=16, validation_data=(x_test, y_test))\n\nError Traceback:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-120-394f1966082d> in <module>\n----> 1 history = model.fit(x_train, y_train, epochs=50, batch_size=16, validation_data=(x_test, y_test))\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1181                 _r=1):\n   1182               callbacks.on_train_batch_begin(step)\n-> 1183               tmp_logs = self.train_function(iterator)\n   1184               if data_handler.should_sync:\n   1185                 context.async_wait()\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in train_function(iterator)\n    853       def train_function(iterator):\n    854         \"\"\"Runs a training execution with one step.\"\"\"\n--> 855         return step_function(self, iterator)\n    856 \n    857     else:\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in step_function(model, iterator)\n    843 \n    844       data = next(iterator)\n--> 845       outputs = model.distribute_strategy.run(run_step, args=(data,))\n    846       outputs = reduce_per_replica(\n    847           outputs, self.distribute_strategy, reduction='first')\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in run(***failed resolving arguments***)\n   1283       fn = autograph.tf_convert(\n   1284           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n-> 1285       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n   1286 \n   1287   def reduce(self, reduce_op, value, axis):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\n   2831       kwargs = {}\n   2832     with self._container_strategy().scope():\n-> 2833       return self._call_for_each_replica(fn, args, kwargs)\n   2834 \n   2835   def _call_for_each_replica(self, fn, args, kwargs):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\n   3606   def _call_for_each_replica(self, fn, args, kwargs):\n   3607     with ReplicaContext(self._container_strategy(), replica_id_in_sync_group=0):\n-> 3608       return fn(*args, **kwargs)\n   3609 \n   3610   def _reduce_to(self, reduce_op, value, destinations, options):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\n    595   def wrapper(*args, **kwargs):\n    596     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n--> 597       return func(*args, **kwargs)\n    598 \n    599   if inspect.isfunction(func) or inspect.ismethod(func):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in run_step(data)\n    836 \n    837       def run_step(data):\n--> 838         outputs = model.train_step(data)\n    839         # Ensure counter is updated only if `train_step` succeeds.\n    840         with ops.control_dependencies(_minimum_control_deps(outputs)):\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in train_step(self, data)\n    793     # Run forward pass.\n    794     with backprop.GradientTape() as tape:\n--> 795       y_pred = self(x, training=True)\n    796       loss = self.compiled_loss(\n    797           y, y_pred, sample_weight, regularization_losses=self.losses)\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\n   1028         with autocast_variable.enable_auto_cast_variables(\n   1029             self._compute_dtype_object):\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\n   1031 \n   1032         if self._activity_regularizer:\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py in call(self, inputs, training, mask)\n    378       if not self.built:\n    379         self._init_graph_network(self.inputs, self.outputs)\n--> 380       return super(Sequential, self).call(inputs, training=training, mask=mask)\n    381 \n    382     outputs = inputs  # handle the corner case where self.layers is empty\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in call(self, inputs, training, mask)\n    418         a list of tensors if there are more than one outputs.\n    419     \"\"\"\n--> 420     return self._run_internal_graph(\n    421         inputs, training=training, mask=mask)\n    422 \n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in _run_internal_graph(self, inputs, training, mask)\n    554 \n    555         args, kwargs = node.map_arguments(tensor_dict)\n--> 556         outputs = node.layer(*args, **kwargs)\n    557 \n    558         # Update tensor_dict.\n\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\n   1028         with autocast_variable.enable_auto_cast_variables(\n   1029             self._compute_dtype_object):\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\n   1031 \n   1032         if self._activity_regularizer:\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in call(self, inputs)\n    299             reconstructor = []\n    300             for x in tf.unstack(inputs):\n--> 301                 reconstructor.append(self.call(x))\n    302             return tf.stack(reconstructor)\n    303 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in call(self, inputs)\n    302             return tf.stack(reconstructor)\n    303 \n--> 304         return self._evaluate_qnode(inputs)\n    305 \n    306     def _evaluate_qnode(self, x):\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnn\\keras.py in _evaluate_qnode(self, x)\n    317             **{k: 1.0 * w for k, w in self.qnode_weights.items()},\n    318         }\n--> 319         return self.qnode(**kwargs)\n    320 \n    321     def compute_output_shape(self, input_shape):\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnode.py in __call__(self, *args, **kwargs)\n    564 \n    565         # construct the tape\n--> 566         self.construct(args, kwargs)\n    567 \n    568         cache = self.execute_kwargs.get(\"cache\", False)\n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\qnode.py in construct(self, args, kwargs)\n    481 \n    482         with self.tape:\n--> 483             self._qfunc_output = self.func(*args, **kwargs)\n    484         self._tape._qfunc_output = self._qfunc_output\n    485 \n\n<ipython-input-117-45acf92435fe> in qnode(inputs, weights)\n      4 def qnode(inputs, weights):\n      5 \n----> 6     qml.templates.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with = 0.,  normalize = True)\n      7     qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits), rotation = qml.RY)\n      8 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py in __init__(self, features, wires, pad_with, normalize, do_queue, id)\n    127         self.pad_with = pad_with\n    128         self.normalize = normalize\n--> 129         features = self._preprocess(features, wires, pad_with, normalize)\n    130         super().__init__(features, wires=wires, do_queue=do_queue, id=id)\n    131 \n\n~\\Anaconda3\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py in _preprocess(features, wires, pad_with, normalize)\n    206                 padding = [pad_with] * (2 ** len(wires) - n_features)\n    207                 if (\n--> 208                     hasattr(feature_set, \"device\") and feature_set.device.type == \"cuda\"\n    209                 ):  # pragma: no cover\n    210                     ## Torch tensor, send to same GPU\n\nAttributeError: 'str' object has no attribute 'type'\n\nAny help would be appreciated.\nThanks", "link": "https://discuss.pennylane.ai//t/pad-with-causing-error-in-amplitude-embedding/1933/6"}}