{"0": {"author": "poig", "date": "1675961887731", "content": "Any help will be appreciate.\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Fri_Jan__6_19:04:39_Pacific_Standard_Time_2023\nCuda compilation tools, release 12.0, V12.0.140\nBuild cuda_12.0.r12.0/compiler.32267302_0\noutput:\n98     outputs = model(inputs)\n     99     #nn.sigmoid(outputs)\n    100     #print(outputs)\n    101     if len(outputs.shape) == 1:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torchvision\\models\\densenet.py:218, in DenseNet.forward(self, x)\n    216 out = F.adaptive_avg_pool2d(out, (1, 1))\n    217 out = torch.flatten(out, 1)\n--> 218 out = self.classifier(out)\n    219 return out\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nCell In[26], line 16, in Quantumnet.forward(self, input_features)\n     14 q_out = q_out.to(device)\n     15 for elem in q_in:\n---> 16     q_out_elem = q_net(elem,self.q_params).float().unsqueeze(0)\n     17     q_out = torch.cat((q_out, q_out_elem))\n     18 return self.post_net(q_out)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\qnode.py:847, in QNode.__call__(self, *args, **kwargs)\n    843     self._update_original_device()\n    845     return res\n--> 847 res = qml.execute(\n    848     [self.tape],\n    849     device=self.device,\n    850     gradient_fn=self.gradient_fn,\n    851     interface=self.interface,\n    852     gradient_kwargs=self.gradient_kwargs,\n    853     override_shots=override_shots,\n    854     **self.execute_kwargs,\n    855 )\n    857 if old_interface == \"auto\":\n    858     self.interface = \"auto\"\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:651, in execute(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n    647     return batch_fn(res)\n    649 if gradient_fn == \"backprop\" or interface is None:\n    650     return batch_fn(\n--> 651         qml.interfaces.cache_execute(\n    652             batch_execute, cache, return_tuple=False, expand_fn=expand_fn\n    653         )(tapes)\n    654     )\n    656 # the default execution function is batch_execute\n    657 execute_fn = qml.interfaces.cache_execute(batch_execute, cache, expand_fn=expand_fn)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:206, in cache_execute.<locals>.wrapper(tapes, **kwargs)\n    202         return (res, []) if return_tuple else res\n    204 else:\n    205     # execute all unique tapes that do not exist in the cache\n--> 206     res = fn(execution_tapes.values(), **kwargs)\n    208 final_res = []\n    210 for i, tape in enumerate(tapes):\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:131, in cache_execute.<locals>.fn(tapes, **kwargs)\n    129 def fn(tapes: Sequence[QuantumTape], **kwargs):  # pylint: disable=function-redefined\n    130     tapes = [expand_fn(tape) for tape in tapes]\n--> 131     return original_fn(tapes, **kwargs)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)\n     76 @wraps(func)\n     77 def inner(*args, **kwds):\n     78     with self._recreate_cm():\n---> 79         return func(*args, **kwds)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\_qubit_device.py:656, in QubitDevice.batch_execute(self, circuits)\n    653     self.reset()\n    655     # TODO: Insert control on value here\n--> 656     res = self.execute(circuit)\n    657     results.append(res)\n    659 if self.tracker.active:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:235, in DefaultQubitTorch.execute(self, circuit, **kwargs)\n    226         if params_cuda_device != specified_device_cuda:\n    228             warnings.warn(\n    229                 f\"Torch device {self._torch_device} specified \"\n    230                 \"upon PennyLane device creation does not match the \"\n    231                 \"Torch device of the gate parameters; \"\n    232                 f\"{self._torch_device} will be used.\"\n    233             )\n--> 235 return super().execute(circuit, **kwargs)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\_qubit_device.py:432, in QubitDevice.execute(self, circuit, **kwargs)\n    429 self.check_validity(circuit.operations, circuit.observables)\n    431 # apply all circuit operations\n--> 432 self.apply(circuit.operations, rotations=circuit.diagonalizing_gates, **kwargs)\n    434 # generate computational basis samples\n    435 if self.shots is not None or circuit.is_sampled:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:269, in DefaultQubit.apply(self, operations, rotations, **kwargs)\n    267                 self._debugger.snapshots[len(self._debugger.snapshots)] = state_vector\n    268     else:\n--> 269         self._state = self._apply_operation(self._state, operation)\n    271 # store the pre-rotated state\n    272 self._pre_rotated_state = self._state\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:297, in DefaultQubit._apply_operation(self, state, operation)\n    294     axes = [ax + shift for ax in self.wires.indices(wires)]\n    295     return self._apply_ops[operation.base_name](state, axes, inverse=operation.inverse)\n--> 297 matrix = self._asarray(self._get_unitary_matrix(operation), dtype=self.C_DTYPE)\n    299 if operation in diagonal_in_z_basis:\n    300     return self._apply_diagonal_unitary(state, matrix, wires)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:309, in DefaultQubitTorch._get_unitary_matrix(self, unitary)\n    307 if unitary in diagonal_in_z_basis:\n    308     return self._asarray(unitary.eigvals(), dtype=self.C_DTYPE)\n--> 309 return self._asarray(unitary.matrix(), dtype=self.C_DTYPE)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\operation.py:1529, in Operation.matrix(self, wire_order)\n   1528 def matrix(self, wire_order=None):\n-> 1529     canonical_matrix = self.compute_matrix(*self.parameters, **self.hyperparameters)\n   1531     if self.inverse:\n   1532         canonical_matrix = qml.math.conj(qml.math.moveaxis(canonical_matrix, -2, -1))\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops.py:216, in RY.compute_matrix(theta)\n    214 c = (1 + 0j) * c\n    215 s = (1 + 0j) * s\n--> 216 return qml.math.stack([stack_last([c, -s]), stack_last([s, c])], axis=-2)\n\nRuntimeError: \n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n......\n.....\n\nnvrtc: error: failed to open nvrtc-builtins64_117.dll.\n  Make sure that nvrtc-builtins64_117.dll is installed correctly.\n\n\n\n\n Solved by mlxd in post #2 \n\n\n                Hi @poig \nUnfortunately the error you post looks NVIDIA driver related, and will be hard for us to reason the issue. \nThough, I may suggest downgrading your CUDA version from 12 to 11, and retrying, since it looks like you are using Torch, which may not yet have CUDA 12 support.\n              \n", "link": "https://discuss.pennylane.ai//t/nvrtc-error-failed-to-open-nvrtc-builtins64-117-dll/2529/1"}, "1": {"author": "mlxd", "date": "1676038477465", "content": "Hi @poig\nUnfortunately the error you post looks NVIDIA driver related, and will be hard for us to reason the issue.\nThough, I may suggest downgrading your CUDA version from 12 to 11, and retrying, since it looks like you are using Torch, which may not yet have CUDA 12 support.Solution", "link": "https://discuss.pennylane.ai//t/nvrtc-error-failed-to-open-nvrtc-builtins64-117-dll/2529/2"}, "2": {"author": "poig", "date": "1676042450465", "content": "thanks for the reminder, because I only got the bugs when running pennylane, I downgraded to cuda11.7 driver and restarted, it works.\nThanks again.", "link": "https://discuss.pennylane.ai//t/nvrtc-error-failed-to-open-nvrtc-builtins64-117-dll/2529/3"}, "3": {"author": "isaacdevlugt", "date": "1676065876998", "content": "@poig glad to hear this works for you now!", "link": "https://discuss.pennylane.ai//t/nvrtc-error-failed-to-open-nvrtc-builtins64-117-dll/2529/4"}, "4": {"author": "poig", "date": "1675961887731", "content": "Any help will be appreciate.\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Fri_Jan__6_19:04:39_Pacific_Standard_Time_2023\nCuda compilation tools, release 12.0, V12.0.140\nBuild cuda_12.0.r12.0/compiler.32267302_0\noutput:\n98     outputs = model(inputs)\n     99     #nn.sigmoid(outputs)\n    100     #print(outputs)\n    101     if len(outputs.shape) == 1:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torchvision\\models\\densenet.py:218, in DenseNet.forward(self, x)\n    216 out = F.adaptive_avg_pool2d(out, (1, 1))\n    217 out = torch.flatten(out, 1)\n--> 218 out = self.classifier(out)\n    219 return out\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nCell In[26], line 16, in Quantumnet.forward(self, input_features)\n     14 q_out = q_out.to(device)\n     15 for elem in q_in:\n---> 16     q_out_elem = q_net(elem,self.q_params).float().unsqueeze(0)\n     17     q_out = torch.cat((q_out, q_out_elem))\n     18 return self.post_net(q_out)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\qnode.py:847, in QNode.__call__(self, *args, **kwargs)\n    843     self._update_original_device()\n    845     return res\n--> 847 res = qml.execute(\n    848     [self.tape],\n    849     device=self.device,\n    850     gradient_fn=self.gradient_fn,\n    851     interface=self.interface,\n    852     gradient_kwargs=self.gradient_kwargs,\n    853     override_shots=override_shots,\n    854     **self.execute_kwargs,\n    855 )\n    857 if old_interface == \"auto\":\n    858     self.interface = \"auto\"\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:651, in execute(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\n    647     return batch_fn(res)\n    649 if gradient_fn == \"backprop\" or interface is None:\n    650     return batch_fn(\n--> 651         qml.interfaces.cache_execute(\n    652             batch_execute, cache, return_tuple=False, expand_fn=expand_fn\n    653         )(tapes)\n    654     )\n    656 # the default execution function is batch_execute\n    657 execute_fn = qml.interfaces.cache_execute(batch_execute, cache, expand_fn=expand_fn)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:206, in cache_execute.<locals>.wrapper(tapes, **kwargs)\n    202         return (res, []) if return_tuple else res\n    204 else:\n    205     # execute all unique tapes that do not exist in the cache\n--> 206     res = fn(execution_tapes.values(), **kwargs)\n    208 final_res = []\n    210 for i, tape in enumerate(tapes):\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\interfaces\\execution.py:131, in cache_execute.<locals>.fn(tapes, **kwargs)\n    129 def fn(tapes: Sequence[QuantumTape], **kwargs):  # pylint: disable=function-redefined\n    130     tapes = [expand_fn(tape) for tape in tapes]\n--> 131     return original_fn(tapes, **kwargs)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)\n     76 @wraps(func)\n     77 def inner(*args, **kwds):\n     78     with self._recreate_cm():\n---> 79         return func(*args, **kwds)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\_qubit_device.py:656, in QubitDevice.batch_execute(self, circuits)\n    653     self.reset()\n    655     # TODO: Insert control on value here\n--> 656     res = self.execute(circuit)\n    657     results.append(res)\n    659 if self.tracker.active:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:235, in DefaultQubitTorch.execute(self, circuit, **kwargs)\n    226         if params_cuda_device != specified_device_cuda:\n    228             warnings.warn(\n    229                 f\"Torch device {self._torch_device} specified \"\n    230                 \"upon PennyLane device creation does not match the \"\n    231                 \"Torch device of the gate parameters; \"\n    232                 f\"{self._torch_device} will be used.\"\n    233             )\n--> 235 return super().execute(circuit, **kwargs)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\_qubit_device.py:432, in QubitDevice.execute(self, circuit, **kwargs)\n    429 self.check_validity(circuit.operations, circuit.observables)\n    431 # apply all circuit operations\n--> 432 self.apply(circuit.operations, rotations=circuit.diagonalizing_gates, **kwargs)\n    434 # generate computational basis samples\n    435 if self.shots is not None or circuit.is_sampled:\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:269, in DefaultQubit.apply(self, operations, rotations, **kwargs)\n    267                 self._debugger.snapshots[len(self._debugger.snapshots)] = state_vector\n    268     else:\n--> 269         self._state = self._apply_operation(self._state, operation)\n    271 # store the pre-rotated state\n    272 self._pre_rotated_state = self._state\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:297, in DefaultQubit._apply_operation(self, state, operation)\n    294     axes = [ax + shift for ax in self.wires.indices(wires)]\n    295     return self._apply_ops[operation.base_name](state, axes, inverse=operation.inverse)\n--> 297 matrix = self._asarray(self._get_unitary_matrix(operation), dtype=self.C_DTYPE)\n    299 if operation in diagonal_in_z_basis:\n    300     return self._apply_diagonal_unitary(state, matrix, wires)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:309, in DefaultQubitTorch._get_unitary_matrix(self, unitary)\n    307 if unitary in diagonal_in_z_basis:\n    308     return self._asarray(unitary.eigvals(), dtype=self.C_DTYPE)\n--> 309 return self._asarray(unitary.matrix(), dtype=self.C_DTYPE)\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\operation.py:1529, in Operation.matrix(self, wire_order)\n   1528 def matrix(self, wire_order=None):\n-> 1529     canonical_matrix = self.compute_matrix(*self.parameters, **self.hyperparameters)\n   1531     if self.inverse:\n   1532         canonical_matrix = qml.math.conj(qml.math.moveaxis(canonical_matrix, -2, -1))\n\nFile ~\\anaconda3\\envs\\qamp2022_gpu\\lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops.py:216, in RY.compute_matrix(theta)\n    214 c = (1 + 0j) * c\n    215 s = (1 + 0j) * s\n--> 216 return qml.math.stack([stack_last([c, -s]), stack_last([s, c])], axis=-2)\n\nRuntimeError: \n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n......\n.....\n\nnvrtc: error: failed to open nvrtc-builtins64_117.dll.\n  Make sure that nvrtc-builtins64_117.dll is installed correctly.\n\n\n\n\n Solved by mlxd in post #2 \n\n\n                Hi @poig \nUnfortunately the error you post looks NVIDIA driver related, and will be hard for us to reason the issue. \nThough, I may suggest downgrading your CUDA version from 12 to 11, and retrying, since it looks like you are using Torch, which may not yet have CUDA 12 support.\n              \n", "link": "https://discuss.pennylane.ai//t/nvrtc-error-failed-to-open-nvrtc-builtins64-117-dll/2529/5"}}