{"0": {"author": "andrew", "date": "1609068239117", "content": "Hello, I hope everyone on the Xanadu team is having a good holiday season.\nI have a few questions regarding using PyTorch gradients with PennyLane:\n\nI cannot find the source of this at the moment, but I recall seeing that if you want to calculate the gradient in a loss function you will need to use PennyLane with PyTorch. Is this still the case?\nIf you use PennyLane with PyTorch you use have to use torch.autograd and all the PyTorch optimisers. How does this effect the use of the Parameter Shift rules. If we were to use torch.autograd, then send this to real quantum hardware, would we still use parameter shift rules and do the gradient calculation on real hardware? Or, is does the gradient calculation end up happening classically somewhere?\nWe can use Strawberry Fields as a backend to PennyLane, and also use PyTorch at the same time. I have run into a problem with this specific setup. I can create a model like:\n\ndef layer(v):\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n\n    qml.Displacement(v[3], 0.0, wires=0)\n    qml.Kerr(v[4], wires=0)\n\n@qml.qnode(dev, interface='torch')\ndef quantum_neural_net(var, x=None):\n    qml.Displacement(x, 0.0, wires=0)\n\n    for v in var:\n        layer(v)\n\n    return qml.expval(qml.X(0))\n\nThen probe it with:\nnum_layers = 2\ntheta_weights = torch.tensor(0.05*np.random.randn(num_layers, 5)\ni = Variable(torch.tensor(1.0), requires_grad=True)\nO = quantum_neural_net(theta_weights, x=i) \n\nIn this toy model though, when i try to find the gradient, it thinks I have broken the graph:\ndx = torch.autograd.grad(outputs=O, inputs=i, \n                                  grad_outputs=O.data.new(O.shape).fill_(1),\n                                  create_graph=True, retain_graph=True)[0]\n\nRuntimeError: One of the differentiated Tensors\nappears to not have been used in the graph.\nSet allow_unused=True if this is the desired behavior.\n\nSo, my question is, is the combination of PyTorch/SF and torch.autograd gradient calculation not available at the moment?\nThank you!\nps, I understand you are all on your break now, I don\u2019t expect a reply or anything until you are all back! Have a good break.", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/1"}, "1": {"author": "josh", "date": "1609165914641", "content": "Hi @andrew!\n\nI cannot find the source of this at the moment, but I recall seeing that if you want to calculate the gradient in a loss function you will need to use PennyLane with PyTorch. Is this still the case?\n\nYou might need to elaborate here, but if you mean simply compute the gradient of a hybrid classical-quantum cost function, PennyLane supports both autograd (the default) and TensorFlow.\nAutograd\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef circuit(weights):\n    qml.RX(weights[0], wires=0)\n    qml.RY(weights[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(weights):\n    return np.sum(circuit(weights) ** 2 - np.sin(weights))\n\nweights = np.array([0.1, 0.2], requires_grad=True)\ngrad_fn = qml.grad(cost)\nprint(grad_fn(weights))\n\nTensorFlow\nimport pennylane as qml\nimport tensorflow as tf\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev, interface=\"tf\")\ndef circuit(weights):\n    qml.RX(weights[0], wires=0)\n    qml.RY(weights[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(weights):\n    return tf.reduce_sum(circuit(weights) ** 2 - tf.sin(weights))\n\nweights = tf.Variable([0.1, 0.2], dtype=tf.float64)\n\nwith tf.GradientTape() as tape:\n    loss = cost(weights)\n\ngrad = tape.gradient(loss, weights)\nprint(grad)\n\nWhichever framework you choose is up to you and your personal preference  More details are available here: Gradients and training \u2014 PennyLane 0.33.0 documentation 2\n\nIf we were to use torch.autograd, then send this to real quantum hardware, would we still use parameter shift rules and do the gradient calculation on real hardware?\n\nThat\u2019s correct \u2014 if you use PyTorch and PennyLane, and your device is a real hardware device, then the hardware-compatible parameter-shift rule is used automatically.\nIf you would like some more fine-grained control, you can specify the diff_method when creating a QNode:\n@qml.qnode(dev, diff_method=\"parameter-shift\")\n\nOther options include:\n\n\"finite-diff\" - uses numerical finite differences\n\"reversible\" - uses a form of backpropagation that is more memory efficient (simulator only)\n\"backprop\" - uses standard backpropagation (simulator only).\n\n\nThis demo compares and contrasts some of these methods in more detail if you are interested: Quantum gradients with backpropagation | PennyLane Demos 4\n\nSo, my question is, is the combination of PyTorch/SF and torch.autograd gradient calculation not available at the moment?\n\nThis should work, yes!\nYour example appears to work for me after changing the x argument to be non-differentiable. In PennyLane, keyword arguments to QNodes are always non-differentiable parameters \u2014 if you wish for it to be differentiable, it will need to be a positional argument.\nCan you try the following code snippet, and let me know if it works for you?\nimport pennylane as qml\nimport torch\nimport numpy as np\n\ndev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=5)\n\ndef layer(v):\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n\n    qml.Displacement(v[3], 0.0, wires=0)\n    qml.Kerr(v[4], wires=0)\n\n@qml.qnode(dev, interface='torch')\ndef quantum_neural_net(var, x=None):\n    qml.Displacement(x, 0.0, wires=0)\n\n    for v in var:\n        layer(v)\n\n    return qml.expval(qml.X(0))\n\nnum_layers = 2\ntheta_weights = torch.tensor(0.05*np.random.randn(num_layers, 5), requires_grad=True)\n\ni = torch.tensor(1.0, requires_grad=False)\nloss = quantum_neural_net(theta_weights, x=i)\n\nprint(\"Loss:\", loss)\n\nloss.backward()\nprint(\"Gradient:\", theta_weights.grad)\n\nThis gives me the following output:\nGradient: tensor([[-0.0924, -1.5613, -0.0922,  1.7454, -0.3835],\n        [-0.0905, -1.6156, -0.0655,  1.7957, -0.2594]], dtype=torch.float64)\n\n\nNote: The Kerr interaction does not support the parameter-shift rule, and its inclusion in the circuit will cause PennyLane to fallback to finite differences as a fallback.1 Reply", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/2"}, "2": {"author": "josh", "date": "1609166391912", "content": "\nNote: The Kerr interaction does not support the parameter-shift rule, and its inclusion in the circuit will cause PennyLane to fallback to finite differences as a fallback.\n\nA thought I just had based on my \u2018note\u2019 above; if you would prefer analytic gradients for a non-Gaussian photonic circuit, you could switch over to the strawberryfields.tf device and TensorFlow.\nThis combination allows you to set diff_method=\"backprop\", so TensorFlow will perform standard backpropagation for analytic results:\nimport pennylane as qml\nimport tensorflow as tf\nimport numpy as np\n\ndev = qml.device(\"strawberryfields.tf\", wires=1, cutoff_dim=5)\n\ndef layer(v):\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n\n    qml.Displacement(v[3], 0.0, wires=0)\n    qml.Kerr(v[4], wires=0)\n\n@qml.qnode(dev, interface='tf', diff_method=\"backprop\")\ndef quantum_neural_net(var, x=None):\n    qml.Displacement(x, 0.0, wires=0)\n\n    for v in var:\n        layer(v)\n\n    return qml.expval(qml.X(0))\n\nnum_layers = 2\ntheta_weights = tf.Variable(0.05*np.random.randn(num_layers, 5))\n\ni = tf.constant(1.0)\n\nwith tf.GradientTape() as tape:\n\tweights = tf.convert_to_tensor(theta_weights)\n\tloss = quantum_neural_net(weights, x=i)\n\nprint(\"Loss:\", loss)\n\ngrad = tape.gradient(loss, theta_weights)\nprint(\"Gradient:\", grad)\n\nwith output\nLoss: tf.Tensor([1.4142735], shape=(1,), dtype=float32)\nGradient: tf.Tensor(\n[[-0.94458413 -0.32569608 -0.8647036   0.59947348 -3.35271811]\n [-0.87427759 -1.62147141 -0.83985192  1.61316371 -3.1067791 ]], shape=(2, 5), dtype=float64)\n\n(it won\u2019t agree with the output from the post above, as I did not set the random seed in between runs!).\nFor more details, see the strawberryfields.tf documentation.", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/3"}, "3": {"author": "andrew", "date": "1609169464251", "content": "\nYou might need to elaborate here, but if you mean simply compute the gradient of a hybrid classical-quantum cost function, PennyLane supports both autograd (the default) and TensorFlow.\n\nAh apologies for not being clear. I mean if I created a cost function that itself depends on a loss, something that could look something like:\ndef cost(circuit_out, circuit_in):\n    grad = torch.autograd.grad(outputs=circuit_out, inputs=circuit_in)\n    return np.sum(grad - circuit_in)\n\nYou would then treat this cost \u201cnormally\u201d, as you have in your examples. I ask as a follow up to a comment made here :\n\nYes, that constraint comes from Autograd, which is the default interface in PennyLane\n\nI just wanted to be sure I interpreted this correctly, and that this is still the case.\n\nCan you try the following code snippet, and let me know if it works for you?\n\nYes, that works now! Thank you very much!", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/4"}, "4": {"author": "josh", "date": "1609170360604", "content": "\nAh apologies for not being clear. I mean if I created a cost function that itself depends on a loss, something that could look something like:\n\nAh, I see!\nCurrently, differentiating loss functions that depend on the gradient is not supported in PennyLane. This is because the second derivative of QNodes is not yet supported, however this is something we are working on adding: https://github.com/PennyLaneAI/pennylane/pull/961 1\nThere is one exception, however: if you use diff_method=\"backprop\", then higher derivatives and these types of loss functions will work. The following simulators support backpropagation:\n\ndefault.qubit.tf (must be used with interface=\"tf\")\ndefault.qubit.autograd (must be used with interface=\"autograd\")\nstrawberryfields.tf (must be used with interface=\"tf\").\n\n\nNote that in this case, \u2018autograd\u2019 refers to the NumPy-based Autograd library that is available via from pennylane import numpy as np, not PyTorch Autograd! Apologies for the confusion.\nAt the moment, there is no PyTorch-compatible simulator that supports backprop, since this requires complex number support which is not yet fully available in PyTorch.1", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/5"}, "5": {"author": "andrew", "date": "1609171868646", "content": "Thanks Josh!\nSo, if i used something like: strawberryfields.tf  with interface=\"tf\" and diff_method=\"backprop\" it should work, then in the future there may be a diff_method=\"parameter-shift\" option that will run on real hardware?\nTo further clarify, using PyTorch backend will not work? I have a small example running using PyTorch with PennyLane where the loss function depends on calculated gradients. This doesn\u2019t give any errors, but also never seems to converge to a result. Is this just because it is simply not possible when using PyTorch, because of its lack of complex number support, which is necessary for the simulators?", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/6"}, "6": {"author": "josh", "date": "1609734095219", "content": "\nSo, if i used something like: strawberryfields.tf with interface=\"tf\" and diff_method=\"backprop\" it should work\n\nYep, that\u2019s right!\n\nthen in the future there may be a diff_method=\"parameter-shift\" option that will run on real hardware?\n\nWe have a parameter-shift rule that works with Xanadu\u2019s current generation of hardware (the GBS X-series chips), but the parameter-shift rule currently does not extend to non-Gaussian operations available in the TensorFlow backend (such as the Fock and Kerr operations). This is an active area of research, however, so I can\u2019t comment too long term!\n\nI have a small example running using PyTorch with PennyLane where the loss function depends on calculated gradients. This doesn\u2019t give any errors, but also never seems to converge to a result.\n\nI\u2019m somewhat surprised this works. Could you post a small minimal example?", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/7"}, "7": {"author": "andrew", "date": "1609759288781", "content": "\nthe parameter-shift rule currently does not extend to non-Gaussian operations available in the TensorFlow backend\n\nok I see - so to run something on the X8 chip your model must be made of only Gaussian operations? Normal procedure for non-guassian operations is a simulator with a classical gradient finding method.\n\nCould you post a small minimal example?\n\nSure, of course! As I said though, it doesn\u2019t give an error, but it also does not converge to the result I expect. The method though does work if just use \u201cpure\u201d PyTorch (ie, a classical NN)\nThe point of the model is to solve an ODE. By minimising a loss function that compares the gradient of the network output and the value of the original function you can do this.\nimport pennylane as qml\nimport torch\nimport numpy as np\n\ndev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10)\n\n# make our model:\ndef layer(v):\n    # Matrix multiplication of input layer\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n    # Bias\n    qml.Displacement(v[3], 0.0, wires=0)\n    # Element-wise nonlinear transformation\n    qml.Kerr(v[4], wires=0)\n\n@qml.qnode(dev, interface='torch')\ndef quantum_neural_net(var, x):\n    # Encode input x into quantum state\n    qml.Displacement(x, 0.0, wires=0)\n    # \"layer\" subcircuits\n    for v in var:\n        layer(v)\n    return qml.expval(qml.X(0))\n\nget some data:\nn_events = 51\nx_data = torch.linspace(-1,1,51,requires_grad=True)\n\ndeclare ode we want to solve (very minimal example):\n#dy/dx = x\ndef f_x(x):\n    return x\n\nThis is our loss function. It has 2 main parts. It calculates the gradient of the network output against the input. Then, compares this against the value of the above function we wish to solve\ndef func_cost(X):\n    output, dxdy = [], []\n\n    # find the gradient between the network output and input\n    for x in X:\n        model_output = quantum_neural_net(theta_weights, x) \n        output.append(model_output)\n        _d_psy_t = torch.autograd.grad(outputs=model_output, inputs=x, \n                              grad_outputs=model_output.data.new(model_output.shape).fill_(1),\n                              create_graph=True, retain_graph=True)[0]\n        dxdy.append(_d_psy_t)\n    \n    # find error between gradient and value of original function\n    error = 0\n    for x, y in zip(dxdy, X):\n        err_sqr = (x - f_x(y))**2\n        error += err_sqr\n    # enforce a boundary condition:\n    _loss = error/n_events +  (output[25] -1)**2\n\n    return _loss\n\nand then this runs it:\nnum_layers = 3\ntheta_weights = torch.tensor(0.05*np.random.randn(num_layers, 5), requires_grad=True)#torch.rand(num_layers, 5, requires_grad=True)\nopt = torch.optim.Adam([theta_weights], lr = 0.005)\n\nopt.zero_grad()\nloss = func_cost(x_data)\nloss.backward()\nopt.step()\n\nprint(loss.data.item())\n\nThis was based on a PyTorch script i have that does find a solution, and at the moment the code above does not give an error. Maybe an error should be getting flagged and it isn\u2019t? Or the graph is getting broken at some point, so SF does count the gradient inside the loss function?\nI have examples now though that work if I use a wider network and strawberryfields.tf interface, but I am curious as to whats happening here", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/8"}, "8": {"author": "andrew", "date": "1609761267830", "content": "\nNormal procedure for non-guassian operations is a simulator with a classical gradient finding method\n\nOn further reading of \u201cContinuous-variable quantum neural networks\u201d I see this is what is done there (\u201cThe networks are numerically\nsimulated using the Strawberry Fields software platform\u201d).", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/9"}, "9": {"author": "Tom_Bromley", "date": "1609874666621", "content": "Hey @andrew!\n\nok I see - so to run something on the X8 chip your model must be made of only Gaussian operations?\n\nYes, the current X8 chip is composed of Gaussian input states, Gaussian operations, and photon counting (non-Gaussian) measurements. You can find more information here. The chip is accessed in PennyLane using the PennyLane-SF plugin.\nRegarding the code snippet you shared, it does indeed seem odd that you\u2019re getting a gradient of a function that includes the gradient of the quantum circuit. It would be interesting to see how the gradient compares to an identical circuit run using strawberryfields.tf and the TF interface? I also wonder if Torch is just ignoring the contributions from the gradient when differentiating the cost function.", "link": "https://discuss.pennylane.ai//t/using-pytorch-gradients/747/10"}}