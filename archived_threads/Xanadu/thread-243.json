{"0": {"author": "acn", "date": "1685684291049", "content": "Hi,\nI am trying to optimise a variational circuit by batching the inputs. A natural way I feel to speed this process up would be to execute each input in parallel during the optimisation. However trying to implement this using both multiprocessing and pathos seems to not work. Here\u2019s a minimal example to illustrate this\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.templates import AmplitudeEmbedding\nimport multiprocessing as mp\nfrom functools import partial\nfrom pathos.multiprocessing import ProcessingPool as Pool\n\ndev = qml.device(\"default.qubit\", wires=2)\nvar = np.random.randn(2, requires_grad=True)\nopt = qml.AdamOptimizer()\nxes = np.array(([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]))\nbatch_size = 2\n\n@qml.qnode(dev)\ndef circuit(var, x):\n   AmplitudeEmbedding(features=x, wires=range(2), normalize=True)\n   qml.RX(var[0], wires=0)\n   qml.RX(var[1], wires=0)\n   return qml.expval(qml.PauliZ(0))\n\n# To avoid the pickler complaining even more than it would otherwise\ndef circuit_wrapper(var, x):\n   return circuit(var, x)\n\ndef cost_serial(var, X):\n   return np.sum([circuit(var,x) for x in X])\n\ndef cost_parallel(var, X, pool):\n   mapper = partial(circuit_wrapper, var)\n   return np.sum(pool.map(mapper, X))\n\n# First step serially\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   var = opt.step(lambda v : cost_serial(v, X), var)\n# This will work without error\n\n# Try and just do a standard evaluation in parallel using multiprocessing\npool = mp.Pool(4)\nmapper = partial(circuit_wrapper, var)\nresult = np.sum(pool.map(mapper, X))\nprint(\"The result of our dummy circuit using multiprocessing is {:.3f}\".format(result))\n# This will work without error\n\n# Now try and do the circuit evaluation in an optimiser\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   try:\n       var = opt.step(lambda v : cost_parallel(v, X, pool), var)\n   except Exception as e:\n       print(e)\n# This will fail\n\n# Now let's try and use pathos\npool = Pool(4)\nmapper = partial(circuit_wrapper, var)\nresult = np.sum(pool.map(mapper, X))\nprint(\"The result of our dummy circuit using pathos is {:.3f}\".format(result))\n# This will work without error\n\n# Now try and do the circuit evaluation in an optimiser\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   try:\n       var = opt.step(lambda v : cost_parallel(v, X, pool), var)\n   except Exception as e:\n       print(e)\n# This will fail\n\nmultiprocessing will fail with Can't pickle local object 'VJPNode.initialize_root.<locals>.<lambda>' which is not that surprising given what I\u2019ve seen online.\nUsing pathos gives a far stranger error though\nTraceback (most recent call last):\n  File \"[redacted]/lib/python3.11/site-packages/autograd/tracer.py\", line 118, in new_box\n    return box_type_mappings[type(value)](value, trace, node)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\nKeyError: <class 'pennylane.numpy.tensor.tensor'>\n\nI think on a technical level I know what\u2019s going on here and why pathos won\u2019t work. The optimiser registers the parameters with autograd but the subprocesses are spawning off their own copies of the parameters which autograd doesn\u2019t know about\u2026I could be wrong about this but that\u2019s my guess after digging around for a bit.\nThat\u2019s a bit of an aside though. fundamentally I would like to be able to run these circuits in parallel during the optimisation by whatever means works. What are people\u2019s suggestions here on how best to achieve this?", "link": "https://discuss.pennylane.ai//t/parallel-circuit-execution-during-optimisation/2998/1"}, "1": {"author": "isaacdevlugt", "date": "1685714515582", "content": "Hey @acn! Welcome to the forum \nI\u2019m not sure which version of PennyLane you\u2019re using, but in v0.24 we introduced support for parameter broadcasting . I recommend using v0.30 (the most current version), though.\nHere\u2019s an example!\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev)\ndef circuit(features):\n    qml.AngleEmbedding(features, wires=range(2))\n    return [qml.expval(qml.PauliZ(i)) for i in range(2)]\n\nbatch1 = [0.1, 0.2]\nbatch2 = [0.3, 0.4]\nbatches = [batch1, batch2]\n\nprint(circuit(batches))\nprint(dev.num_executions)\n\n'''\n[tensor([0.99500417, 0.95533649], requires_grad=True), tensor([0.98006658, 0.92106099], requires_grad=True)]\n1\n'''\n\nAs you can see, the device only has one execution even though there are two batches of parameters. There are some gaps of support for parameter broadcasting that we are working on for future releases, but if you\u2019re using default.qubit you should be fine ", "link": "https://discuss.pennylane.ai//t/parallel-circuit-execution-during-optimisation/2998/2"}, "2": {"author": "acn", "date": "1685684291049", "content": "Hi,\nI am trying to optimise a variational circuit by batching the inputs. A natural way I feel to speed this process up would be to execute each input in parallel during the optimisation. However trying to implement this using both multiprocessing and pathos seems to not work. Here\u2019s a minimal example to illustrate this\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.templates import AmplitudeEmbedding\nimport multiprocessing as mp\nfrom functools import partial\nfrom pathos.multiprocessing import ProcessingPool as Pool\n\ndev = qml.device(\"default.qubit\", wires=2)\nvar = np.random.randn(2, requires_grad=True)\nopt = qml.AdamOptimizer()\nxes = np.array(([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]))\nbatch_size = 2\n\n@qml.qnode(dev)\ndef circuit(var, x):\n   AmplitudeEmbedding(features=x, wires=range(2), normalize=True)\n   qml.RX(var[0], wires=0)\n   qml.RX(var[1], wires=0)\n   return qml.expval(qml.PauliZ(0))\n\n# To avoid the pickler complaining even more than it would otherwise\ndef circuit_wrapper(var, x):\n   return circuit(var, x)\n\ndef cost_serial(var, X):\n   return np.sum([circuit(var,x) for x in X])\n\ndef cost_parallel(var, X, pool):\n   mapper = partial(circuit_wrapper, var)\n   return np.sum(pool.map(mapper, X))\n\n# First step serially\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   var = opt.step(lambda v : cost_serial(v, X), var)\n# This will work without error\n\n# Try and just do a standard evaluation in parallel using multiprocessing\npool = mp.Pool(4)\nmapper = partial(circuit_wrapper, var)\nresult = np.sum(pool.map(mapper, X))\nprint(\"The result of our dummy circuit using multiprocessing is {:.3f}\".format(result))\n# This will work without error\n\n# Now try and do the circuit evaluation in an optimiser\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   try:\n       var = opt.step(lambda v : cost_parallel(v, X, pool), var)\n   except Exception as e:\n       print(e)\n# This will fail\n\n# Now let's try and use pathos\npool = Pool(4)\nmapper = partial(circuit_wrapper, var)\nresult = np.sum(pool.map(mapper, X))\nprint(\"The result of our dummy circuit using pathos is {:.3f}\".format(result))\n# This will work without error\n\n# Now try and do the circuit evaluation in an optimiser\nfor i in range(0, len(xes), batch_size):\n   X = xes[i:i+batch_size]\n   try:\n       var = opt.step(lambda v : cost_parallel(v, X, pool), var)\n   except Exception as e:\n       print(e)\n# This will fail\n\nmultiprocessing will fail with Can't pickle local object 'VJPNode.initialize_root.<locals>.<lambda>' which is not that surprising given what I\u2019ve seen online.\nUsing pathos gives a far stranger error though\nTraceback (most recent call last):\n  File \"[redacted]/lib/python3.11/site-packages/autograd/tracer.py\", line 118, in new_box\n    return box_type_mappings[type(value)](value, trace, node)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\nKeyError: <class 'pennylane.numpy.tensor.tensor'>\n\nI think on a technical level I know what\u2019s going on here and why pathos won\u2019t work. The optimiser registers the parameters with autograd but the subprocesses are spawning off their own copies of the parameters which autograd doesn\u2019t know about\u2026I could be wrong about this but that\u2019s my guess after digging around for a bit.\nThat\u2019s a bit of an aside though. fundamentally I would like to be able to run these circuits in parallel during the optimisation by whatever means works. What are people\u2019s suggestions here on how best to achieve this?", "link": "https://discuss.pennylane.ai//t/parallel-circuit-execution-during-optimisation/2998/3"}}