{"0": {"author": "Javier", "date": "1601646946172", "content": "Hi,\nI\u2019m trying to do a QAOA optimization using the default_qubit_tf device. I have the following QAOA implementation\ndev = qml.device('default.qubit.tf', wires = len(G.nodes), shots = 5000)\n@qml.qnode(dev, interface = \"tf\", diff_method = \"backprop\")\ndef circuit(params, **kwargs):\n    params = params.numpy()\n    for i in range(len(G.nodes)):\n        qml.Hadamard(wires = i)\n    for j in range(p):\n        U_C(params[0][j])\n        qaoa.mixer_layer(params[1][j], mixer_h)\n    return qml.probs(wires = list(range(len(G.nodes))))\n\nwhere U_C implements the cost Hamiltonian unitary operations and qaoa.mixer_layer the mixer Hamiltonian ones. The parameters params are initially implemented as\nparams = tf.Variable(0.01*np.random.rand(2, p))\n\nso that\u2019s why I redefine them inside the circuit, so I can provide them to the gates as numpy variables (I don\u2019t know if this is appropriate). I provide the cost function as\ndef cost_function(params):\n    result = circuit(params).numpy()[0]\n\n    counts = {}\n    for i in range(len(result)):\n        counts[f\"{i:010b}\"] = result[i]\n\n    E = 0        \n    for bitstring in counts.keys():\n        x = string_to_list(bitstring)\n        energy = MaxCut_cost(G, x)\n        E += -energy*counts[bitstring]\n    return tf.convert_to_tensor(np.array([E]))\n\nwhich does the following\n\nTakes the probability outcomes coming from the circuit, and turns them into numpy variables.\nDefines a dictionary that saves the probabilities obtained in the circuit.\nFor each bitstring obtained, computes the energy with MaxCut_costwhich defines the MaxCut cost funtion for the graph G I\u2019ve defined.\nFinally, returns the energy as a tensor variable, exactly equal to the one I\u2019m introducing through the parameters.\n\nFirst of all, I\u2019m not very familiar with tensorflow so that\u2019s why (probably) my implementation is terrible. Secondly, I tried to do the optimization in a similar fashion to what I\u2019m used to do with the default_qubit device, i.e., I define an optimizer opt = Optimizer(lr = 0.1) and then calculate the new parameters as params = opt.step(cost_function, params), but it gives me the following error\nTypeError: Can't differentiate w.r.t. type <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nI would really appreciate your help. The reason why I\u2019m trying to use this tensorflow approach is because I want to optimize for large values of p, i.e., the number of repetitions defined in QAOA, and in the documentation it\u2019s specified that this device is the appropriate one when I want to work with a big number of parameters.\nIf you have further questions about my implementation, I\u2019ll be happy to specify them in more detail. Thanks in advance!\nCheers,\nJavier.", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/1"}, "1": {"author": "antalszava", "date": "1601659739200", "content": "Hi @Javier,\nThanks for reaching out! \nThat seems odd indeed. Which optimizer did you go for using, could you further include the parts of the code where the optimization is done and the optimizer is being used? (Also a notebook/complete snippet close to the original could be helpful for reproducing the error.)\nThank you!\nAntal", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/2"}, "2": {"author": "josh", "date": "1601707581859", "content": "Hi @Javier!\n\nI would really appreciate your help. The reason why I\u2019m trying to use this tensorflow approach is because I want to optimize for large values of p , i.e., the number of repetitions defined in QAOA, and in the documentation it\u2019s specified that this device is the appropriate one when I want to work with a big number of parameters.\n\nThe reason we suggest default.qubit.tf for a large number of parameters is (as you show in your code snippet), since it is written in TensorFlow, it allows us to use backprop for differentiation (diff_method=\"backprop\").\nWhile backprop mode only works on specific simulators, it is much more efficient for training a large number of parameters compared to other simulator methods (such as the parameter-shift rule\u2014for more details, see Quantum Gradients with Backpropagation 1).\nIn fact, we now have two devices that support backpropagation!\n\n\ndefault.qubit.tf 1, which supports diff_method=\"backprop\" when used with the tf interface, and\n\n\ndefault.qubit.autograd, which supports diff_method=\"backprop\" when used with the default autograd interface.\n\n\nSo if you would like, you can swap to default.qubit.autograd, which uses the Autograd-enabled version of NumPy that comes with PennyLane.\n\nSecondly, I tried to do the optimization in a similar fashion to what I\u2019m used to do with the default_qubit device, i.e., I define an optimizer opt = Optimizer(lr = 0.1) and then calculate the new parameters as params = opt.step(cost_function, params)\n\nNote that the optimizers you must use depend entirely on your interface.\n\n\nIf using the autograd interface (the default one in PennyLane), you can use the provided PennyLane optimizers 1.\n\n\nHowever, if you are using the tf interface, then you will need to leverage one of the TensorFlow optimizers (found in tf.keras.optimizers ). Only the TensorFlow optimizers know how to optimize TensorFlow code.\n\n\nWe have a small guide on our TensorFlow interface page 4, showing how to optimize in TensorFlow:\nphi = tf.Variable([0.5, 0.1], dtype=tf.float64)\ntheta = tf.Variable(0.2, dtype=tf.float64)\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\nsteps = 200\n\nfor i in range(steps):\n    with tf.GradientTape() as tape:\n        loss = tf.abs(circuit4(phi, theta) - 0.5)**2\n\n    gradients = tape.gradient(loss, [phi, theta])\n    opt.apply_gradients(zip(gradients, [phi, theta]))\n\nHope that helps! Let us know if you have any other questions.1 Reply", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/3"}, "3": {"author": "Javier", "date": "1601734552383", "content": "Hi,\nthank you very much for your response @antalszava and @josh. I tried to change now to the default.qubit.autograd  option to deal with numpy, and now I get the following error\nTypeError: loop of ufunc does not support argument 0 of type ArrayBox which has no callable exp me\nIt appears when I perform the optimization step, using the adaptive gradient descent optimizer qml.AdagradOptimizer(lr). I provide you a notebook with all the details that you can find in my github (I couldn\u2019t upload it here), if you want to take a look to it (https://github.com/javierrd/Pennylane-questions 3). Otherwise, I don\u2019t have any problem in specifying further details of the problem here.\nThank you very much for your time and efforts!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/4"}, "4": {"author": "Javier", "date": "1601889373422", "content": "Hi @Josh,\nI tried this implementation and seems to work pretty well whenever I add my \u201chomemade\u201d mixer Hamiltonian, i.e., if I define the mixer Hamiltonian by myself implementing the gates \u201cby hand\u201d. However, when I use the qml.qaoa option, the following error arises: TypeError: Cannot convert (-0-1j) to EagerTensor of dtype double. Do you know why? I don\u2019t know if the qml.qaoaoption implements the gates in such a way that that tensorflowdoesn\u2019t know how to interpret them.\nThank you very much!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/5"}, "5": {"author": "josh", "date": "1601910197945", "content": "Hi @Javier, could you post a minimal (non)-working code example that produces that error? That will help me track down this potential bug  Thanks!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/6"}, "6": {"author": "Javier", "date": "1601911272620", "content": "Hi @Josh,\nsure, I will write it here part by part (each item corresponds in my notebook to a separate cell):\n\n\nPackages that I\u2019m introducing\n     import pennylane as qml\n     import cirq\n     from pennylane_cirq import ops as cirq_ops\n     from pennylane import qaoa\n     import tensorflow as tf\n     import autograd.numpy as np\n     import networkx as nx\n     import random\n     import torch\n\n\n\nGraph definition\n def dregular_graph(n,d,mu,sigma):\n     G= nx.generators.random_graphs.random_regular_graph(d,n)\n     for e in list(G.edges):\n         G[e[0]][e[1]][\"weight\"] = round(random.gauss(mu,sigma),2)\n     return G\n\n G = dregular_graph(15,8,0.1, 1)\n\n\n\nCircuit definition\n  def U_C(gamma):\n      for e in list(G.edges):\n         wire1 = int(e[0])\n         wire2 = int(e[1])\n         qml.CNOT(wires=[wire1, wire2])\n         qml.RZ(G[wire1][wire2][\"weight\"]*gamma, wires=wire2)\n         qml.CNOT(wires=[wire1, wire2])\n\n  cost_h, mixer_h = qaoa.maxcut(G)\n\n  def qaoa_mixer(alpha):\n      qaoa.mixer_layer(alpha, mixer_h)\n\n  p = 4\n  \n  dev = qml.device('default.qubit.tf', wires = len(G.nodes))\n  @qml.qnode(dev, interface = \"tf\", diff_method = \"backprop\")\n  def circuit(gamma, beta, **kwargs):\n      for i in range(len(G.nodes)):\n          qml.Hadamard(wires = i)\n      for j in range(p):\n          U_C(gamma[j])\n          qaoa_mixer(beta[j])\n      return qml.probs(wires = list(range(len(G.nodes))))\n\n\n\nMinimization\n  gamma = tf.Variable([2*random.random() for _ in range(p)], dtype=tf.float64)\n  beta = tf.Variable([2*random.random() for _ in range(p)], dtype=tf.float64)\n  opt = tf.keras.optimizers.Adagrad(learning_rate=0.1)\n\n  for _ in range(50):\n      with tf.GradientTape() as tape:\n          cost = sum(cost_function(gamma, beta))\n     \n      gradients = tape.gradient(cost, [gamma, beta])\n      opt.apply_gradients(zip(gradients, [gamma, beta]))\n\n\n\nOn the other hand, if I forget about the mixer_h defined by the qaoa function and write it myself introducing the rotations explicitly, then it works perfectly.\nBy the way, when running the code double-check that the identation is correct because I had to modify it a little bit here so that I could write it as a preformatted text.\nThank you very much!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/7"}, "7": {"author": "antalszava", "date": "1601936052269", "content": "Hi @Javier! \nThanks so much for your code! It helped spot a small issue when using default.qubit.autograd. We\u2019ve had a go with patching it 1. Until this change makes it into the master version of PennyLane, you could checkout the branch with the update.\nAnother small modification that was needed in your snippet was related to how the gradient function is defined for the optimization. The gradient function had to be passed explicitly as grad_fn=qml.jacobian(cost_function) making the line for optimization:\nparams = opt.step(cost_function, params, grad_fn=qml.jacobian(cost_function))\n\nHope this helps! Otherwise, let us know if there are further things that we could help out with 2", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/8"}, "8": {"author": "antalszava", "date": "1602008117687", "content": "Hi @Javier,\nThis is now fixed, if you install PennyLane from the master branch then it should work nicely. 2", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/9"}, "9": {"author": "Javier", "date": "1602244090284", "content": "Thank you very much for your solutions and efforts!\nI wanted to ask you another thing that might not be very related with what I was asking here because probably we can\u2019t handle it with backpropagation. I want to define a non-analytic circuit with some shots that at the end of the day samples the PauliZ operation over all the qubits, so I get an array full of 1 and -1. This I know it can be done with the qml.sample measurement. With that, I want to compute the cost function as C(x) = 1/N \\sum_x E(x), with E(x) the energy of configuration x (sampled from the circuit) and N the number of shots.\nThe question is then, can I define a circuit with some device that is able to optimize with this definition of the measurement and the cost function? I know that qml.sample cannot be used for analytic optimization because it\u2019s stochastic, but is there any other form I can implement?\nI was trying to implement this with the lightning.qubit (I want to go to a big number of qubits) and computing the cost function as np.sum(np.dot(x*,np.dot(adj,x))) with x* the transpose of the matrix given by the circuit and adj the adjacency matrix defined by my graph, but during the opt.step with AdagradOptimizer I get an error saying that Python cannot differentiate w.r.t. type <class 'numpy.int64'>.\nOnce again, thank you very much for your efforts and your answers!2 Replies", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/10"}, "10": {"author": "theodor", "date": "1602275986600", "content": "Hi @Javier,\nI suspect that the error you\u2019re getting might be from returning qml.sample(...) from your QNode. Would you be able to share a minimal code example of this, so that I can have a look at what might be causing this error?\n\nThe question is then, can I define a circuit with some device that is able to optimize with this definition of the measurement and the cost function?\n\nAs you mentioned, it doesn\u2019t really make sense to use qml.sample with analytic optimizations, hence it doesn\u2019t support gradient calculations. You could try using a gradient-free optimizer, which would be able to run; just keep in mind that the sample return is stochastic, and it might thus not make much sense.\nFeel free to share parts of your code here and we could have a closer look at what you\u2019re doing and what\u2019s not currently working!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/11"}, "11": {"author": "antalszava", "date": "1602287953284", "content": "Hi @Javier,\nOne side note about lightning.qubit is that it will default to using numpy when used for more than 16 qubits (and hence its performance will be identical to default.qubit in such cases).", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/12"}, "12": {"author": "Javier", "date": "1602316889189", "content": "Hi @theodor and @antalszava, thank you very much for your response.\nWith respect to @antalszava comment, I wanted to go to sizes between 16 and 20 qubits. In fact with the tensorflow option I can run in a reasonable amount of time 16 qubits, but for bigger systems, as the Hilbert space scales exponentially, I thought that one of the main bottlenecks was the computation of the cost function, since I was outputting qml.probs, calculate the energy for each possible bitstring, multiply both tensors and sum. Thus, I thought that approximating the energy as I specified in my previous message will enhance the operations. Is there any difference in using default.qubit.tf with parameter shift instead of default.qubit for these system sizes?\nWith respect to @theodor comment, here I attach you the code I was using. I\u2019ll split it in several steps as before\n\n\nThe graph definition is the same as the one shown in the code I sent in one of my previous messages to Josh.\n\n\nCircuit definition\nG = dregular_graph(4,2,0.1, 1)\np = 1\ndev = qml.device('lightning.qubit', analytic = False, shots = 100, \nwires = len(G.nodes))\n@qml.qnode(dev)\ndef circuit(params, **kwargs):\n    for i in range(len(G.nodes)):\n        qml.Hadamard(wires = i)\n    for j in range(p):\n        U_C(params[0][j])\n        U_M(params[1][j])\n    return [qml.sample(qml.PauliZ(k)) for k in G.nodes]\n\n\n\nAdjacency matrix and cost function definition\n def adjacency_matrix_np(G):\n     adj = np.zeros((len(G.nodes),len(G.nodes)))\n     for edge in G.edges:\n         i = edge[0]\n         j = edge[1]\n         adj[i,j] = G[i][j][\"weight\"]\n         adj[j,i] = G[j][i][\"weight\"]\n     return adj\n\n adj = adjacency_matrix_np(G)\n def cost_function_light(params):\n     result = circuit(params)\n     prod = np.dot(adj, result)\n     final_prod = np.dot(np.transpose(result), prod)\n     return np.sum(final_prod)/100\n\n\n\nOptimization\n opt = qml.AdagradOptimizer(0.1)\n steps = 20\n params = 0.01*np.random.rand(2, p)\n for _ in range(steps):\n     params = opt.step(cost_function_light, params)\n\n\n\nWith the packages I sent in the previous message, in principle should reproduce the error I\u2019m getting.\nThank you very much for your efforts and answers!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/13"}, "13": {"author": "Javier", "date": "1602576125265", "content": "There is a mistake in the definition of the cost function (cost_function_light in the code). The second operation, i.e. the one defined by final prod should be defined as result*prod. As you mentioned, with gradient free optimizers like qml.RotosolveOptimizer I don\u2019t get any error since no gradients are being calculated. However, the problem doesn\u2019t get optimized but this is something I\u2019ll try to figure out.\nSorry for the mistake and thank you very much for your time and efforts!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/14"}, "14": {"author": "theodor", "date": "1602612098585", "content": "HI @Javier. The problem is that your cost-function seems to be highly stochastic, i.e. it outputs completely different values even when using the same input, due to the use of qml.sample. Even though the gradient cannot be calculated when returning samples, it would most likely not help with the optimization. If I may ask; why do you want to return the samples and optimize over them? It might make more sense to use the expected value in this case. ", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/15"}, "15": {"author": "Javier", "date": "1602614727393", "content": "Hi @theodor, thank you very much for your response!\nI wanted to use this method of  obtaining samples because I wanted to approximate the energy as C = 1/N \\sum_{i=0}^N E(x_i) with N the number of samples. The reason for doing this is because I want to go to a big number of qubits (between 20 and 18). Initially I was doing these operations exactly with qml.probs, calculating the cost function as the product between the probability vector and another vector containing the energies of each possible sample (a 2^N size vector). For 16 qubits this works relatively well, i.e., in a reasonable amount of time (using tensorflow and backprop), but for 18 qubits already takes a a lot of time to optimize a QAOA with p = 1 (more than 4 hours), and apart form the circuit computation, one of the main bottlenecks I could see is the energy computation I was doing. Thus, with the previous approximation I expected to do these computations quicker.\nThank you very much!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/16"}, "16": {"author": "Maria_Schuld", "date": "1602652117284", "content": "Hey Javier,\nWhile, as @theodor said, the gradient of a stochastic output is somewhat ill-defined, there is still a way to do it in practice. The idea is to return the expectation of the PauliZ operator expval(PauliZ(wires=...), but setting dev.shots=1. This, in turn, is theoretically well defined, because it is the gradient of an expectation value, estimated using a single measurement. In other words, the output of the quantum function is now deterministic, but its evaluation is a stochastic estimate.\nJust stating this here for completeness, I don\u2019t know how feasible it is in terms of runtime, where I assume this will use a lot more resources than returning an expectation (especially on a simulator, where you have access to the exact expectation, and sampling is less natural). We are planning to make the shots API a bit more flexible so that your use case may be supported better in future.1", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/17"}, "17": {"author": "Viro", "date": "1644317034830", "content": "Hi @Maria_Schuld\nI wanted to implement something similar to what Javier has done here, and tried to use the same approach as you mentioned. I am curious as to what is actually measured in the backend when using the dev.shots = 1 approach? Is it that the parameter shift rule (or something similar) is applied to all the pauli_Z expectation values over all the available qubits? I.e if one had a 2 qubit system, one would perform parameter-shift on <z_1> and <z_2> to calculate the gradient with respect to some parameters gamma?\nAlso, has the shots API been edited to allow for something similar to what javier was requesting?", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/18"}, "18": {"author": "CatalinaAlbornoz", "date": "1644345577230", "content": "Hi @Viro,\nIf you\u2019re working on hardware then you will be using the parameter-shift rule. If you\u2019re working on a simulator you will need to specify the differentiation method when you define your qnode. If you don\u2019t define the differentiation method then PennyLane will pick the default method for that particular simulator. This is how you can specify that you want to use the parameter-shift rule to get the gradient.\n@qml.qnode(dev, diff_method=\"parameter-shift\")\nThe parameter-shift rule is then applied to the entire circuit. If you want to measure the expectation value for more than one qubit you can use the tensor product of the PauliZ on different wires, and get the gradient with respect to your parameters. Here\u2019s an example:\ndev = qml.device(\"default.qubit\", wires = 2, shots=1)\n\n@qml.qnode(dev, diff_method=\"parameter-shift\")\ndef my_circuit(data,params):\n    # Encode the input data as an RX rotation\n    qml.RX(data[0], wires=0)\n    qml.RX(data[1], wires=1)\n    qml.CNOT(wires=[0,1])\n    # Create a rotation based on the angles in \"params\"\n    qml.Rot(params[0], params[1], params[2], wires=0)\n    qml.Rot(params[3], params[4], params[5], wires=1)\n    # We return the expected value of a measurement along the Z axis \n    return qml.expval(qml.PauliZ(0)@qml.PauliZ(1))\n\ndata = np.array([0.1,0.1],requires_grad=False)\nparams = np.array([0.1,0.1,0.1,0.1,0.1,0.1],requires_grad=True)\n\ngrad_function = qml.grad(my_circuit)\nprint(grad_function(data,params))\n\nSince you only use 1 shot then your answer will change every time you run it.\nAbout the shots API, what functionality are you looking for?\nPlease let me know if this helps!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/19"}, "19": {"author": "Javier", "date": "1602244090284", "content": "Thank you very much for your solutions and efforts!\nI wanted to ask you another thing that might not be very related with what I was asking here because probably we can\u2019t handle it with backpropagation. I want to define a non-analytic circuit with some shots that at the end of the day samples the PauliZ operation over all the qubits, so I get an array full of 1 and -1. This I know it can be done with the qml.sample measurement. With that, I want to compute the cost function as C(x) = 1/N \\sum_x E(x)C(x)=1/N\u2211xE(x), with E(x)E(x) the energy of configuration xx (sampled from the circuit) and NN the number of shots.\nThe question is then, can I define a circuit with some device that is able to optimize with this definition of the measurement and the cost function? I know that qml.sample cannot be used for analytic optimization because it\u2019s stochastic, but is there any other form I can implement?\nI was trying to implement this with the lightning.qubit (I want to go to a big number of qubits) and computing the cost function as np.sum(np.dot(x*,np.dot(adj,x))) with x* the transpose of the matrix given by the circuit and adj the adjacency matrix defined by my graph, but during the opt.step with AdagradOptimizer I get an error saying that Python cannot differentiate w.r.t. type <class 'numpy.int64'>.\nOnce again, thank you very much for your efforts and your answers!2 Replies", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/20"}, "20": {"author": "Javier", "date": "1602614727393", "content": "Hi @theodor, thank you very much for your response!\nI wanted to use this method of  obtaining samples because I wanted to approximate the energy as C = 1/N \\sum_{i=0}^N E(x_i)C=1/N\u2211Ni=0E(xi) with NN the number of samples. The reason for doing this is because I want to go to a big number of qubits (between 20 and 18). Initially I was doing these operations exactly with qml.probs, calculating the cost function as the product between the probability vector and another vector containing the energies of each possible sample (a 2^N2N size vector). For 16 qubits this works relatively well, i.e., in a reasonable amount of time (using tensorflow and backprop), but for 18 qubits already takes a a lot of time to optimize a QAOA with p = 1 (more than 4 hours), and apart form the circuit computation, one of the main bottlenecks I could see is the energy computation I was doing. Thus, with the previous approximation I expected to do these computations quicker.\nThank you very much!", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/21"}, "21": {"author": "Viro", "date": "1644317034830", "content": "Hi @Maria_Schuld\nI wanted to implement something similar to what Javier has done here, and tried to use the same approach as you mentioned. I am curious as to what is actually measured in the backend when using the dev.shots = 1 approach? Is it that the parameter shift rule (or something similar) is applied to all the pauli_Z expectation values over all the available qubits? I.e if one had a 2 qubit system, one would perform parameter-shift on <z_1> and <z_2> to calculate the gradient with respect to some parameters gammagamma?\nAlso, has the shots API been edited to allow for something similar to what javier was requesting?", "link": "https://discuss.pennylane.ai//t/qaoa-optimization-with-default-qubit-tf-device/603/22"}}