{"0": {"author": "TM_MEME", "date": "1675823045415", "content": "Hi, I\u2019m TMMeme.\nI want to parallelize the circuit executions for local simulators.\nI want to parallelize over data \u2014 ie different inputs to the same circuits.\nWhat is currently the simplest way to achieve this? Parallel computations are planned to be performed at an accelerated rate by NVIDIA GPU.\nI saw the doc qml.QNodeCollection 5\u00b6. But this doc mainly introduces the way to run multiple different quantum circuits in parallel for a single input.\nThanks for reading so far", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/1"}, "1": {"author": "mlxd", "date": "1675880818398", "content": "Hi @TM_MEME thanks for reaching out.\nCurrently, support for batching with lightning.gpu works natively for observable calculations using the adjoint differentiation pipeline, but not for data-parallelism over input parameters.\nWe have had good success using both Dask (e.g see dask-cuda) and Ray for this, with the latter having allowed us to perform data-parallelism over 128 GPUs as part of our circuit-cutting work 4.\nIs there a specific use-case you are aiming to build here? As qml.QNodeCollection is a deprecated PennyLane feature, it may not be ideal for your purpose.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/2"}, "2": {"author": "TM_MEME", "date": "1675992885206", "content": "Thank you for kind comment, @mlxd\nI want to parallelize the quantum convolution. Currently I am doing sequential quantum convolutional operations, which takes time.\n#device\ndev = qml.device(\"default.qubit\", wires=10)\n\n@qml.qnode(dev, interface='torch')\ndef qconv(inputs, weights): \n    N_phi = len(inputs)\n\n    # Encoding of 'N_phi' classical input values\n    for j in range(N_phi):\n        qml.RY(np.pi * inputs[j], wires=j)\n\n    # Random quantum circuit\n    RandomLayers(weights, wires=range(N_phi))\n\n    # Measurement producing 'N_phi' classical output values\n    result = [qml.expval(qml.PauliZ(j)) for j in range(N_phi)]\n    return result\n\n#weight_shapes\nN_layers = 1 \nN_rand_params = 10 \nweight_shapes = {\"weights\": (N_layers, N_rand_params)}\n\n#TorchLayer\nqlayer = qml.qnn.TorchLayer(qconv, weight_shapes)\n\n#definition of Qonvlayer\nclass QonvLayer(nn.Module):\n    def __init__(self):\n        super(QonvLayer, self).__init__()\n        self.qonv = qlayer\n\n    def forward(self, data): \n        # Constants\n        N = len(data[0][0]) \n        K = 10\n        slide = 1\n        N_base = 4\n\n        # Output Size\n        out_x = 3\n        out_y = N_base * K \n        out_c = N-K+1 \n        out = np.zeros((out_x, out_y, out_c))\n        out_temp_temp = np.zeros((K*N_base, out_c))\n\n        for x in range(out_x):\n            batch_x = data[x:x+1][0][:]\n\n            for y in range(N_base):\n\n                out_temp = np.zeros((K, out_c))\n                print(\"out_temp\", out_temp, out_temp.shape)\n                batch_xy = batch_x[y] \n\n                for c in range(0, out_c, slide):\n                    batch_k = batch_xy[c:c+K]\n                    q_results = self.qonv(batch_k) \n                    print(\"q_results\", q_results)\n\n                    for i in range(K): \n                        out_temp[i][c]= q_results[i] \n                    print(\"out_temp:\" , out_temp) \n                \n                \n                for i in range(K): \n                    for j in range(out_c): \n                        out_temp_temp[i+y*K][j] = out_temp[i][j]\n\n            for i in range(K*N_base):\n                for j in range(out_c):\n                    out[x][i][j]=out_temp_temp[i][j]\n        \n        return out    \n\ndata = torch.randn([10, 4, 20])\n \n\n# forward\nmodel = QonvLayer()\nprint(model)\n\ny = model(data)\nprint(y)\n\nprint(\"done\")", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/3"}, "3": {"author": "mlxd", "date": "1676040080383", "content": "Hi @TM_MEME thank you for the input. Since this uses the Torch interface, it may be possible to explore Torch\u2019s support for multi-GPU and distributed training (link) 1. For a more complex case of GPUs being on multiple nodes, I\u2019d suggest reading up on DistributedDataParallel (link) 3 framework to gain data-parallelism over your inputs (in this case, replacing the input for loop with an offload of batches per GPU). If this matches your needs, I think the ideas should help get the batching done in parallel over different GPU resources.\nFeel free to let us know if this doesn\u2019t suit your needs, and we can offer some additional suggestions.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/4"}, "4": {"author": "TM_MEME", "date": "1677217384194", "content": "Sorry for the delay in replying.\nI\u2019m sorry but this may not fit my needs.\nIs there any way to copy a quantum circuit to multiple GPUs in one node and compute the quantum circuit in parallel?", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/5"}, "5": {"author": "mlxd", "date": "1677513909654", "content": "Hi @TM_MEME\nDo you mean to split a given state-vector computation across multiple GPUs (i), or to run separate state-vector computations with the same circuit, but different inputs (ii)?\nIf (i) this is not something we currently support with PennyLane LightningPU, but are planning this for a later release. This will likely however not result in any speedup, but will allow running larger circuits that do not fit onto a single GPU.\nIf (ii) you can enable batch-mode for LightningGPU, which will allow a given circuit, using adjoint differentiation mode to calculate each part of the computation across multiple GPUs on a single node. This can be enabled by using @qml.qnode(dev, interface='torch', diff_method=\"adjoint\") and enabling the batch mode as documented in the  Parallel adjoint differentiation support section 4 of the documentation.\nNote that this is not currently supporting multi-node communication, not does it provide any advantage to circuits less than 20 qubits.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/6"}, "6": {"author": "Abhishek", "date": "1678446898802", "content": "Hi,\nI am struggling with same problem. I tried the following with dask,\n\n\nNow, if we are parallelising the expectation values of 5 different inputs for same circuit as below,\n\n\nimage982\u00d7205 13.6 KB\n\nBut this is only until computing the expectation values in parallel. But machine learning requires computing the gradients of parameters for optimization. I want to speed up the simulations and got stuck with the gradients computations as follows,\nGradients : qml.grad(expval)(params)\nERROR :\n\n\nFailed to deserialize\n\n\nCannot pickle \u2018generator\u2019 object\n\n\nCan someone please help in this regard or show some directions on the alternatives.\nThanks a lot!!!", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/7"}, "7": {"author": "CatalinaAlbornoz", "date": "1678887003296", "content": "Hi @Abhishek,\nI\u2019m not being able to replicate your error.\nI get a different error though because you\u2019re trying to find the gradient of expval, however this isn\u2019t a real scalar-output function so you run into an error.\nFrom what I can tell your error seems more related to dask or your environment and not something related to PennyLane. If you look at the full error traceback then maybe you can see where the error is originating. If you share some more details here then maybe someone can help you.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/8"}, "8": {"author": "Abhishek", "date": "1678890489892", "content": "Hi @CatalinaAlbornoz ,\nThanks for your help. Following is the code.\ndev = qml.device('lightning.qubit', wires=1)\n@qml.qnode(dev, diff_method='adjoint')\ndef circuit(x, y):\n    qml.RX(x, wires=0)\n    qml.RY(y, wires=0)\n    return qml.expval(qml.PauliZ(wires=0))\n\nIn the circuit below, consider (RX) has many inputs which computations are parallelized and (RY) is a parameter.\n\ndef dask_cost(x, y):\n    temp = []\n    for i in range(5):\n        val = dask.delayed(circuit)(x[i], y)\n        temp.append(val)\n    expvals = dask.compute(*temp)\n    expvals = np.sum(np.array(expvals))\n    return expvals\n\ndef normal_cost(x, y):\n    temp = []\n    for i in range(5):\n        val = circuit(x[i], y)\n        temp.append(val)\n    expvals = np.sum(np.array(temp))\n    return expvals\n\nx = np.array(np.random.random(5))\ny = 2.0\ndask_cost(x, y)\n>>> tensor(-1.89933823, requires_grad=True)\n\nnormal_cost(x, y)\n>>> tensor(-1.89933823, requires_grad=True)\n\nGradient of cost function with respect to parameters are needed in machine learning task to update the parameters.\nqml.grad(normal_cost, argnum=1)(x, y)\n>>> -4.1501297391067\n\nqml.grad(dask_cost, argnum=1)(x, y)\nError :\n\nCould not serialize object of type ArrayBox.\nFailed to deserialize,\nCannot pickle \u2018generator\u2019 object\n\nI guess that this problem needs handling objects of type ArrayBox with serialization and deserializations of operations from dask. If anyone knows how to do that, please share your views here.\nAlso does this usage of parallel computing help to speed up the simulations ? Please share your views\nThanks !!!", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/9"}, "9": {"author": "CatalinaAlbornoz", "date": "1679093688753", "content": "Hi @Abhishek,\nThank you for sharing your code. I don\u2019t get any errors when I run it.\nI suspect that you may have an old version of Dask, PennyLane or Python.\nUsually having the latest Python and PennyLane versions is best. Using Python 3.8-3.11 and PennyLane 0.29.1 should work for you. I would recommend that you follow these steps. Note that where it says \u201cname_of_your_environment\" you can choose any name that you want:\nCreate a new conda environment with:conda create --name name_of_your_environment python=3.10\nActivate the environment:conda activate name_of_your_environment\nAfter this you can install the needed packages:python -m pip install pennylane dask\nPlease let me know if this works for you! If this doesn\u2019t solve your issue please post the output of qml.about() and your full error traceback.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/10"}, "10": {"author": "Abhishek", "date": "1679097468173", "content": "Hi @CatalinaAlbornoz , I followed your steps and got the same pickle error. And i traced the error back to autograd library.\nqml.about()\nName: PennyLane\nVersion: 0.29.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: C:\\Users\\xxxx\\Anaconda3\\envs\\dummy\\Lib\\site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, retworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning\n\nPlatform info:           Windows-10-10.0.19045-SP0\nPython version:          3.11.0\nNumpy version:           1.23.5\nScipy version:           1.10.1\nInstalled devices:\n- default.gaussian (PennyLane-0.29.1)\n- default.mixed (PennyLane-0.29.1)\n- default.qubit (PennyLane-0.29.1)\n- default.qubit.autograd (PennyLane-0.29.1)\n- default.qubit.jax (PennyLane-0.29.1)\n- default.qubit.tf (PennyLane-0.29.1)\n- default.qubit.torch (PennyLane-0.29.1)\n- default.qutrit (PennyLane-0.29.1)\n- null.qubit (PennyLane-0.29.1)\n- lightning.qubit (PennyLane-Lightning-0.29.0)\n\nI am wondering how you are getting output without error. Can you please post your code and the output. I would really like to see it.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/11"}, "11": {"author": "CatalinaAlbornoz", "date": "1679356131086", "content": "Hi @Abhishek,\nI\u2019m using your exact same code. I\u2019m wondering if maybe you\u2019re importing numpy properly. Remember that if you\u2019re using the autograd interface you need to import numpy from PennyLane:\nimport pennylane as qml\nfrom pennylane import numpy as np\n\nThere may also be an issue with the Python and/or numpy versions that you\u2019re using, although this is less likely. I used Python 3.9 and numpy 1.22.\nIf the numpy import isn\u2019t the issue then I would suggest that you follow the steps I shared again but with Python 3.10 (just in case there\u2019s some strange unknown behaviour with Python 3.11) and please copy and paste here your full error traceback here.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/12"}, "12": {"author": "Abhishek", "date": "1679395060455", "content": "Hi @CatalinaAlbornoz ,\nI use only pennylane numpy (from pennylane import numpy as np) everytime like you showed. When I change the device from lightning.qubit to default.qubit and diff_method from adjoint to best, surprisingly it is giving some output although different ones.\nqml.grad(normal_cost, argnum=1)(x, y)\n>>-4.315619113490985\n\nqml.grad(dask_cost, argnum=1)(x, y)\n>>-0.8970631311618367\n\nHowever, I followed your suggestion with python 3.10 version. And I have not changed numpy version as pennylane installs its own numpy. The error occured to me when i use lightning qubit and adjoint diff method.\nFollowing is the qml.about()\nName: PennyLane\nVersion: 0.29.1\nSummary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\nHome-page: https://github.com/XanaduAI/pennylane\nAuthor: \nAuthor-email: \nLicense: Apache License 2.0\nLocation: c:\\users\\setty\\anaconda3\\envs\\temp\\lib\\site-packages\nRequires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, retworkx, scipy, semantic-version, toml\nRequired-by: PennyLane-Lightning\n\nPlatform info:           Windows-10-10.0.19042-SP0\nPython version:          3.10.9\nNumpy version:           1.23.5\nScipy version:           1.10.1\nInstalled devices:\n- default.gaussian (PennyLane-0.29.1)\n- default.mixed (PennyLane-0.29.1)\n- default.qubit (PennyLane-0.29.1)\n- default.qubit.autograd (PennyLane-0.29.1)\n- default.qubit.jax (PennyLane-0.29.1)\n- default.qubit.tf (PennyLane-0.29.1)\n- default.qubit.torch (PennyLane-0.29.1)\n- default.qutrit (PennyLane-0.29.1)\n- null.qubit (PennyLane-0.29.1)\n- lightning.qubit (PennyLane-Lightning-0.29.0)\n\nError traceback,\nqml.grad(dask_cost, argnum=1)(x, y)\n2023-03-21 11:32:27,376 - distributed.protocol.core - CRITICAL - Failed to deserialize\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\core.py\", line 158, in loads\n    return msgpack.loads(\n  File \"msgpack\\_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\core.py\", line 138, in _decode_default\n    return merge_and_deserialize(\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 497, in merge_and_deserialize\n    return deserialize(header, merged_frames, deserializers=deserializers)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 426, in deserialize\n    return loads(header, frames)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 180, in serialization_error_loads\n    raise TypeError(msg)\nTypeError: Could not serialize object of type ArrayBox\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 63, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nAttributeError: Can't pickle local object 'VJPNode.initialize_root.<locals>.<lambda>'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 68, in dumps\n    pickler.dump(x)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 24, in reducer_override\n    if _always_use_pickle_for(obj):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 45, in _always_use_pickle_for\n    return isinstance(x, (str, bytes))\nRecursionError: maximum recursion depth exceeded in __instancecheck__\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 81, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'generator' object\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[47], line 1\n----> 1 qml.grad(dask_cost, argnum=1)(x, y)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\pennylane\\_grad.py:115, in grad.__call__(self, *args, **kwargs)\n    112     self._forward = self._fun(*args, **kwargs)\n    113     return ()\n--> 115 grad_value, ans = grad_fn(*args, **kwargs)\n    116 self._forward = ans\n    118 return grad_value\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\autograd\\wrap_util.py:20, in unary_to_nary.<locals>.nary_operator.<locals>.nary_f(*args, **kwargs)\n     18 else:\n     19     x = tuple(args[i] for i in argnum)\n---> 20 return unary_operator(unary_f, x, *nary_op_args, **nary_op_kwargs)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\pennylane\\_grad.py:133, in grad._grad_with_forward(fun, x)\n    127 @staticmethod\n    128 @unary_to_nary\n    129 def _grad_with_forward(fun, x):\n    130     \"\"\"This function is a replica of ``autograd.grad``, with the only\n    131     difference being that it returns both the gradient *and* the forward pass\n    132     value.\"\"\"\n--> 133     vjp, ans = _make_vjp(fun, x)\n    135     if not vspace(ans).size == 1:\n    136         raise TypeError(\n    137             \"Grad only applies to real scalar-output functions. \"\n    138             \"Try jacobian, elementwise_grad or holomorphic_grad.\"\n    139         )\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\autograd\\core.py:10, in make_vjp(fun, x)\n      8 def make_vjp(fun, x):\n      9     start_node = VJPNode.new_root()\n---> 10     end_value, end_node =  trace(start_node, fun, x)\n     11     if end_node is None:\n     12         def vjp(g): return vspace(x).zeros()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\autograd\\tracer.py:10, in trace(start_node, fun, x)\n      8 with trace_stack.new_trace() as t:\n      9     start_box = new_box(x, t, start_node)\n---> 10     end_box = fun(start_box)\n     11     if isbox(end_box) and end_box._trace == start_box._trace:\n     12         return end_box._value, end_box._node\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\autograd\\wrap_util.py:15, in unary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f(x)\n     13 else:\n     14     subargs = subvals(args, zip(argnum, x))\n---> 15 return fun(*subargs, **kwargs)\n\nCell In[43], line 6, in dask_cost(x, y)\n      4     val = dask.delayed(circuit)(x[i], y)\n      5     temp.append(val)\n----> 6 expvals = dask.compute(*temp)\n      7 expvals = np.sum(np.array(expvals))\n      8 return expvals\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\dask\\base.py:599, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    596     keys.append(x.__dask_keys__())\n    597     postcomputes.append(x.__dask_postcompute__())\n--> 599 results = schedule(dsk, keys, **kwargs)\n    600 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\client.py:3168, in Client.get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\n   3166         should_rejoin = False\n   3167 try:\n-> 3168     results = self.gather(packed, asynchronous=asynchronous, direct=direct)\n   3169 finally:\n   3170     for f in futures.values():\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\client.py:2328, in Client.gather(self, futures, errors, direct, asynchronous)\n   2326 else:\n   2327     local_worker = None\n-> 2328 return self.sync(\n   2329     self._gather,\n   2330     futures,\n   2331     errors=errors,\n   2332     direct=direct,\n   2333     local_worker=local_worker,\n   2334     asynchronous=asynchronous,\n   2335 )\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\utils.py:345, in SyncMethodMixin.sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\n    343     return future\n    344 else:\n--> 345     return sync(\n    346         self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n    347     )\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\utils.py:412, in sync(loop, func, callback_timeout, *args, **kwargs)\n    410 if error:\n    411     typ, exc, tb = error\n--> 412     raise exc.with_traceback(tb)\n    413 else:\n    414     return result\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\utils.py:385, in sync.<locals>.f()\n    383         future = wait_for(future, callback_timeout)\n    384     future = asyncio.ensure_future(future)\n--> 385     result = yield future\n    386 except Exception:\n    387     error = sys.exc_info()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tornado\\gen.py:769, in Runner.run(self)\n    766 exc_info = None\n    768 try:\n--> 769     value = future.result()\n    770 except Exception:\n    771     exc_info = sys.exc_info()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\client.py:2220, in Client._gather(self, futures, errors, direct, local_worker)\n   2218     else:\n   2219         self._gather_future = future\n-> 2220     response = await future\n   2222 if response[\"status\"] == \"error\":\n   2223     log = logger.warning if errors == \"raise\" else logger.debug\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\client.py:2271, in Client._gather_remote(self, direct, local_worker)\n   2268                 response[\"data\"].update(data2)\n   2270     else:  # ask scheduler to gather data for us\n-> 2271         response = await retry_operation(self.scheduler.gather, keys=keys)\n   2273 return response\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\utils_comm.py:434, in retry_operation(coro, operation, *args, **kwargs)\n    428 retry_delay_min = parse_timedelta(\n    429     dask.config.get(\"distributed.comm.retry.delay.min\"), default=\"s\"\n    430 )\n    431 retry_delay_max = parse_timedelta(\n    432     dask.config.get(\"distributed.comm.retry.delay.max\"), default=\"s\"\n    433 )\n--> 434 return await retry(\n    435     partial(coro, *args, **kwargs),\n    436     count=retry_count,\n    437     delay_min=retry_delay_min,\n    438     delay_max=retry_delay_max,\n    439     operation=operation,\n    440 )\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\utils_comm.py:413, in retry(coro, count, delay_min, delay_max, jitter_fraction, retry_on_exceptions, operation)\n    411             delay *= 1 + random.random() * jitter_fraction\n    412         await asyncio.sleep(delay)\n--> 413 return await coro()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\core.py:1234, in PooledRPCCall.__getattr__.<locals>.send_recv_from_rpc(**kwargs)\n   1232 prev_name, comm.name = comm.name, \"ConnectionPool.\" + key\n   1233 try:\n-> 1234     return await send_recv(comm=comm, op=key, **kwargs)\n   1235 finally:\n   1236     self.pool.reuse(self.addr, comm)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\core.py:993, in send_recv(comm, reply, serializers, deserializers, **kwargs)\n    991 await comm.write(msg, serializers=serializers, on_error=\"raise\")\n    992 if reply:\n--> 993     response = await comm.read(deserializers=deserializers)\n    994 else:\n    995     response = None\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\comm\\tcp.py:254, in TCP.read(self, deserializers)\n    251 try:\n    252     frames = unpack_frames(frames)\n--> 254     msg = await from_frames(\n    255         frames,\n    256         deserialize=self.deserialize,\n    257         deserializers=deserializers,\n    258         allow_offload=self.allow_offload,\n    259     )\n    260 except EOFError:\n    261     # Frames possibly garbled or truncated by communication error\n    262     self.abort()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\comm\\utils.py:100, in from_frames(frames, deserialize, deserializers, allow_offload)\n     98     res = await offload(_from_frames)\n     99 else:\n--> 100     res = _from_frames()\n    102 return res\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\comm\\utils.py:83, in from_frames.<locals>._from_frames()\n     81 def _from_frames():\n     82     try:\n---> 83         return protocol.loads(\n     84             frames, deserialize=deserialize, deserializers=deserializers\n     85         )\n     86     except EOFError:\n     87         if size > 1000:\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\core.py:158, in loads(frames, deserialize, deserializers)\n    152                 raise ValueError(\n    153                     \"Unpickle on the Scheduler isn't allowed, set `distributed.scheduler.pickle=true`\"\n    154                 )\n    156         return msgpack_decode_default(obj)\n--> 158     return msgpack.loads(\n    159         frames[0], object_hook=_decode_default, use_list=False, **msgpack_opts\n    160     )\n    162 except Exception:\n    163     logger.critical(\"Failed to deserialize\", exc_info=True)\n\nFile msgpack\\_unpacker.pyx:194, in msgpack._cmsgpack.unpackb()\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\core.py:138, in loads.<locals>._decode_default(obj)\n    136     if \"compression\" in sub_header:\n    137         sub_frames = decompress(sub_header, sub_frames)\n--> 138     return merge_and_deserialize(\n    139         sub_header, sub_frames, deserializers=deserializers\n    140     )\n    141 else:\n    142     return Serialized(sub_header, sub_frames)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py:497, in merge_and_deserialize(header, frames, deserializers)\n    493             merged = bytearray().join(subframes)\n    495         merged_frames.append(merged)\n--> 497 return deserialize(header, merged_frames, deserializers=deserializers)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py:426, in deserialize(header, frames, deserializers)\n    421     raise TypeError(\n    422         \"Data serialized with %s but only able to deserialize \"\n    423         \"data with %s\" % (name, str(list(deserializers)))\n    424     )\n    425 dumps, loads, wants_context = families[name]\n--> 426 return loads(header, frames)\n\nFile ~\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py:180, in serialization_error_loads(header, frames)\n    178 def serialization_error_loads(header, frames):\n    179     msg = \"\\n\".join([codecs.decode(frame, \"utf8\") for frame in frames])\n--> 180     raise TypeError(msg)\n\nTypeError: Could not serialize object of type ArrayBox\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 63, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nAttributeError: Can't pickle local object 'VJPNode.initialize_root.<locals>.<lambda>'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 68, in dumps\n    pickler.dump(x)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 24, in reducer_override\n    if _always_use_pickle_for(obj):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 45, in _always_use_pickle_for\n    return isinstance(x, (str, bytes))\nRecursionError: maximum recursion depth exceeded in __instancecheck__\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\distributed\\protocol\\pickle.py\", line 81, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"C:\\Users\\setty\\Anaconda3\\envs\\temp\\lib\\site-packages\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'generator' object\n\nI think you are right. If its working for you then it should work somehow, I am just missing something here.\nThanks for the help ", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/13"}, "13": {"author": "CatalinaAlbornoz", "date": "1679685086775", "content": "Hi @Abhishek,\nThank you for sharing your full error traceback. From what I can tell there\u2019s probably a mismatch in your Python versions between your Dask client and the workers. (Eg, see the troubleshooting guide here 1.)\nI recommend that you create a new environment and make sure that everything uses Python 3.8 or greater.\nYou can also try using Google Colab, where you will be able to run your code with no issues.\nPlease let me know if this solves your issue!", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/14"}, "14": {"author": "Abhishek", "date": "1679910145771", "content": "Hi @CatalinaAlbornoz ,\nThank you for sharing the trouble shooting link.\nVersions for client and the 4 workers which were created seems to be matching as follows,\nclient.get_version()\n{'scheduler': {'host': {'python': '3.11.0.final.0',\n   'python-bits': 64,\n   'OS': 'Windows',\n   'OS-release': '10',\n   'machine': 'AMD64',\n   'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n   'byteorder': 'little',\n   'LC_ALL': 'None',\n   'LANG': 'None'},\n  'packages': {'python': '3.11.0.final.0',\n   'dask': '2023.3.1',\n   'distributed': '2023.3.1',\n   'msgpack': '1.0.5',\n   'cloudpickle': '2.2.1',\n   'tornado': '6.2',\n   'toolz': '0.12.0',\n   'numpy': '1.23.5',\n   'pandas': '1.5.3',\n   'lz4': None}},\n 'workers': {'tcp://127.0.0.1:60486': {'host': {'python': '3.11.0.final.0',\n    'python-bits': 64,\n    'OS': 'Windows',\n    'OS-release': '10',\n    'machine': 'AMD64',\n    'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n    'byteorder': 'little',\n    'LC_ALL': 'None',\n    'LANG': 'None'},\n   'packages': {'python': '3.11.0.final.0',\n    'dask': '2023.3.1',\n    'distributed': '2023.3.1',\n    'msgpack': '1.0.5',\n    'cloudpickle': '2.2.1',\n    'tornado': '6.2',\n    'toolz': '0.12.0',\n    'numpy': '1.23.5',\n    'pandas': '1.5.3',\n    'lz4': None}},\n  'tcp://127.0.0.1:60493': {'host': {'python': '3.11.0.final.0',\n    'python-bits': 64,\n    'OS': 'Windows',\n    'OS-release': '10',\n    'machine': 'AMD64',\n    'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n    'byteorder': 'little',\n    'LC_ALL': 'None',\n    'LANG': 'None'},\n   'packages': {'python': '3.11.0.final.0',\n    'dask': '2023.3.1',\n    'distributed': '2023.3.1',\n    'msgpack': '1.0.5',\n    'cloudpickle': '2.2.1',\n    'tornado': '6.2',\n    'toolz': '0.12.0',\n    'numpy': '1.23.5',\n    'pandas': '1.5.3',\n    'lz4': None}},\n  'tcp://127.0.0.1:60496': {'host': {'python': '3.11.0.final.0',\n    'python-bits': 64,\n    'OS': 'Windows',\n    'OS-release': '10',\n    'machine': 'AMD64',\n    'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n    'byteorder': 'little',\n    'LC_ALL': 'None',\n    'LANG': 'None'},\n   'packages': {'python': '3.11.0.final.0',\n    'dask': '2023.3.1',\n    'distributed': '2023.3.1',\n    'msgpack': '1.0.5',\n    'cloudpickle': '2.2.1',\n    'tornado': '6.2',\n    'toolz': '0.12.0',\n    'numpy': '1.23.5',\n    'pandas': '1.5.3',\n    'lz4': None}},\n  'tcp://127.0.0.1:60499': {'host': {'python': '3.11.0.final.0',\n    'python-bits': 64,\n    'OS': 'Windows',\n    'OS-release': '10',\n    'machine': 'AMD64',\n    'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n    'byteorder': 'little',\n    'LC_ALL': 'None',\n    'LANG': 'None'},\n   'packages': {'python': '3.11.0.final.0',\n    'dask': '2023.3.1',\n    'distributed': '2023.3.1',\n    'msgpack': '1.0.5',\n    'cloudpickle': '2.2.1',\n    'tornado': '6.2',\n    'toolz': '0.12.0',\n    'numpy': '1.23.5',\n    'pandas': '1.5.3',\n    'lz4': None}}},\n 'client': {'host': {'python': '3.11.0.final.0',\n   'python-bits': 64,\n   'OS': 'Windows',\n   'OS-release': '10',\n   'machine': 'AMD64',\n   'processor': 'Intel64 Family 6 Model 165 Stepping 5, GenuineIntel',\n   'byteorder': 'little',\n   'LC_ALL': 'None',\n   'LANG': 'None'},\n  'packages': {'python': '3.11.0.final.0',\n   'dask': '2023.3.1',\n   'distributed': '2023.3.1',\n   'msgpack': '1.0.5',\n   'cloudpickle': '2.2.1',\n   'tornado': '6.2',\n   'toolz': '0.12.0',\n   'numpy': '1.23.5',\n   'pandas': '1.5.3',\n   'lz4': None}}}\n\nI created new environment following your steps in the previous posts and posted the above error tracebacks and qml.about() etc. In each new environment, python versions were above 3.8. I am not able to understand the version problem.\nAlso, now I tried with google colab and saw the same error traceback. Google colab has its own python version of 3.9. I have not changed anything. If its working for you, can you please share your solution code with results. That would be great so that I can use the same one to bypass the errors.\nThanks", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/15"}, "15": {"author": "Abhishek", "date": "1679947875056", "content": "Hi @CatalinaAlbornoz ,\nSorry, I found my mistake. I missed to add the line in the above code.\nimport pennylane as qml\nimport matplotlib.pyplot as plt\nfrom pennylane import numpy as np\nimport dask\nfrom dask.distributed import Client, progress\nclient = Client(threads_per_worker=1, n_workers=4)\nclient\n\n\nimage2236\u00d7568 43 KB\n\nI tried without client and got the output without error. But, without the Client initialization, I think the parallelization in computation is not actually happening. So running dask was just like normal computation, and there is no error. Can you please add these lines into your code in the beginning and rerun the code. Also while running the dask lines, you can check the status of parallelism in the link generated by client. Please let me know if this shows up the error.\nThanks\u2026", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/16"}, "16": {"author": "CatalinaAlbornoz", "date": "1680121025738", "content": "Hi @Abhishek!\nThank you for sharing these additional details. I can now reproduce your problem.\nIt seems that you will need to import pickle and use it within your code.\nYou might need to create a dask.distributed client, with pickle serializers, and use processes instead of threads.\nYou might also need to override the device\u2019s native batch_execute function by creating a batch_execute method that uses pickle, and then replace the device\u2019s batch_execute method with the one you create.\nOnce you have this you can use PennyLane as usual.\nMe or my colleague @mlxd might be able to provide a code example in the next few days.", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/17"}, "17": {"author": "mlxd", "date": "1680182345079", "content": "Hi @Abhishek\nIt is be likely that, depending on your intentions, Dask (or Ray), may not be straightforward \u2014 or it could be a single line change. Handling the serialization of internal datastructures with tracked gradients can be problematic, so as @CatalinaAlbornoz suggested, we often need to override in-built behaviours of Dask or Pennylane, depending on the example workload. In this case, it may be easier (and computationally cheaper) to use the in-built batching functionality of default.qubit over input parameters to circuits, as this will be immediately compatible with ML interfaces, potentially allowing you to avoid using a distributed execution framework altogether.\nAlso, if you are using 1 qubit per simulation, I\u2019d suggest stick with default.qubit overall, as lightning.qubit will have some initial overheads, and does not support such batching workloads. As another note, the adjoint differentiation pipeline relies on parallelisation in C++ to handle gradients over multiple observables, but will not benefit directly from distribution, unless the task is packaged appropriately.\nHere is an example of using Dask to handle parameter-shift gradients, where a new circuit is created for each shifted parameter, and immediately can take advantage of distribution using Dask. We redefine the batch_execute method, as mentioned by @CatalinaAlbornoz , change the serializer functionality to ensure the gradients are tracked correctly, and farm the work out to Dask workers. Hopefully this helps you with your workload:\nimport pennylane as qml\nimport dask\nfrom dask.distributed.protocol import serialize, deserialize\nfrom dask.distributed import Client\nimport pickle\n\ndef batch_execute_dask(circuits):\n    # Override the device's native batch_execute function\n    def run_circ(circ):\n        \"Create remote device and run the circuit\"\n        c = pickle.loads(circ)\n        dev = qml.device(\"lightning.qubit\", wires=wires)\n        return qml.execute([c], dev, gradient_fn=None)\n\n    results = []\n    for circuit in circuits:\n        print(f\"Submitting circuit: {circuit}\")\n        results.append(client.submit(run_circ, pickle.dumps(circuit)))\n\n    return client.gather(results)\n\nif __name__== \"__main__\":\n\n    # create dask.distributed client, with pickle serializers, and use processes instead of threads\n    client = Client(serializers=['pickle'], deserializers=['pickle'], processes=True)\n\n    wires=11\n    dev = qml.device(\"lightning.qubit\", wires=wires)\n\n    params = qml.numpy.random.rand(wires)\n\n    # replace the batch_execute method with the above\n    dev.batch_execute = batch_execute_dask\n\n    # The rest as normal\n    @qml.qnode(dev, diff_method=\"parameter-shift\")\n    def circuit(x):\n        for i in range(wires):\n            qml.RX(x[i], i)\n        for i in range(1, wires):\n            qml.CNOT(wires=[i-1,i])\n        return [qml.expval(qml.PauliZ(i)) for i in range(wires)]\n\n    print(qml.jacobian(circuit)(params))\n2", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/18"}, "18": {"author": "Abhishek", "date": "1680512764176", "content": "Hi @mlxd and @CatalinaAlbornoz ,\nThanks a lot for your effort to solve this problem. This idea was to see if there is a speed up in the simulation time. Above example with single qubit was to just make the problem simple to understand. However, the real use cases are much larger and time consuming ones. I will try with this solution and once again thank you so much for the solution code 1", "link": "https://discuss.pennylane.ai//t/is-there-a-way-to-parallelize-the-same-circuit-for-multiple-input-data/2516/19"}}