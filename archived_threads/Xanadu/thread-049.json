{"0": {"author": "akatief", "date": "1688462265353", "content": "Hello! I\u2019m trying to write a TorchLayer capable of handling multidimensional batches. I\u2019d like to split the batch in the second dimension and perform some separate computations on the two halves. A toy example of a similar computation in PyTorch looks like this:\nimport torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n\n    def forward(self, x):\n        # Split the batch along second dimension\n        x1 = x[:, 0]\n        x2 = x[:, 1]\n        # Do some computation separately\n        y1 = self.linear(x1)\n        y2 = self.linear(x2)\n        return y1 + y2\n\nmm = MyModule()\nx = torch.zeros((8, 2, 2))\n\noutput = mm(x)\n\nWhen I try to pass batched input to a TorchLayer, it gets flattened so that I end up having only two dimensions to work with.\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef qnode(inputs, weights_0, weight_1):\n    print(f'This is {inputs.shape} but should be (8, 2, 2)')\n\n    # Some computation\n    qml.RX(inputs[:, 0], wires=0)\n    qml.RX(inputs[:, 1], wires=1)\n    qml.Rot(*weights_0, wires=0)\n    qml.RY(weight_1, wires=1)\n    qml.CNOT(wires=[0, 1])\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\nweight_shapes = {\"weights_0\": 3, \"weight_1\": 1}\nx = torch.zeros((8, 2, 2))\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\noutput = qlayer(x)\n\n# Output:\n# This is torch.Size([16, 2]) but should be (8, 2, 2)\n\nMy question therefore is: how can I have the circuit not flatten the first two dimensions of my input? I\u2019d like to avoid workarounds like reshaping the tensor inside qnode.\nThanks a lot!", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/1"}, "1": {"author": "isaacdevlugt", "date": "1688498747051", "content": "Hey @akatief! Welcome to the forum \nI tried running your quantum circuit, and I got\nThis is torch.Size([2]) but should be (8, 2, 2)\n\nI\u2019m using the most up-to-date versions of PennyLane and PyTorch  (at the time of writing, v0.31 for PennyLane and 2.0.1 for Torch).\nThat said, inputs[:, 0] has shape (8, 2), which RX can\u2019t broadcast over. It needs to see a one-dimensional vector of parameters. E.g., qml.RX([0.1, 0.2, 0.3], wires=0). You may just need to cleverly access the elements of inputs that you want to broadcast over .\nLet me know if this helps!", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/2"}, "2": {"author": "akatief", "date": "1688590962584", "content": "Thanks a lot for your answer!\nQuite worringly, I get a different output than yours despite having the same version of PyTorch and PennyLane, I wonder why that is.\nI understand your point about broadcasting though, I also found some lines in TorchLayer.forward that explicitly reshape the input I give.\nI guess a workaround could be to reshape the input vector outside the qnode and access specific elements inside of it. I wonder, is that the intended way?", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/3"}, "3": {"author": "isaacdevlugt", "date": "1688592451200", "content": "\nI also found some lines in TorchLayer.forward that explicitly reshape the input I give.\n\nHmm \u2026 If you\u2019re creating a hybrid model that inherits from nn.Module, then forward is user-defined. In other words, most likely there\u2019s something that you\u2019re doing that is causing things to be reshaped.\n\nI guess a workaround could be to reshape the input vector outside the qnode and access specific elements inside of it. I wonder, is that the intended way?\n\nIt really depends on your application. Sometimes the forward pass of a model can be non-trivial and may involve some clever intermediary data processing between layers. Do you have a small code example that replicates what you\u2019re seeing / having to do?", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/4"}, "4": {"author": "akatief", "date": "1688632371951", "content": "I\u2019m still talking about the toy example of before, same code. Using debug I can see that output = qlayer(x) calls TorchLayer.forward inside the file torch.py and runs line 398:\n# inputs is our (8,2,2) tensor\ninputs = torch.reshape(inputs, (-1, inputs.shape[-1]))\n\nFrom this, I understand that PennyLane really doesn\u2019t want inputs with more than one batch dimension. Is this the same behavior you get?\nAgain, to make sure we\u2019re running the same version I restarted the kernel, pip freez-ed and got\ntorch==2.0.1\ntorchsummary==1.5.1\ntorchvision==0.15.2\nPennyLane==0.31.0\nPennyLane-Lightning==0.31.0\nPennyLane-qiskit==0.31.0\n\nI feel like my complete use case has some other issues that may require a different thread, for now I\u2019d just be happy with sorting this one out ", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/5"}, "5": {"author": "isaacdevlugt", "date": "1688653120137", "content": "Ah! Apologies. You\u2019re right \u2014 if you\u2019re passing in a batch of inputs, it will be flattened such that len(inputs.shape) == 2 based on here 3.\n\nmost likely there\u2019s something that you\u2019re doing that is causing things to be reshaped.\n\nJust clarifying that this is misleading at best .\nSo, in your case, if you must have inputs that have len(inputs.shape) > 2, you should be aware of how TorchLayer will reshape it, and adjust things if desired.", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/6"}, "6": {"author": "akatief", "date": "1688462265353", "content": "Hello! I\u2019m trying to write a TorchLayer capable of handling multidimensional batches. I\u2019d like to split the batch in the second dimension and perform some separate computations on the two halves. A toy example of a similar computation in PyTorch looks like this:\nimport torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n\n    def forward(self, x):\n        # Split the batch along second dimension\n        x1 = x[:, 0]\n        x2 = x[:, 1]\n        # Do some computation separately\n        y1 = self.linear(x1)\n        y2 = self.linear(x2)\n        return y1 + y2\n\nmm = MyModule()\nx = torch.zeros((8, 2, 2))\n\noutput = mm(x)\n\nWhen I try to pass batched input to a TorchLayer, it gets flattened so that I end up having only two dimensions to work with.\nimport pennylane as qml\nimport torch\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev, interface=\"torch\")\ndef qnode(inputs, weights_0, weight_1):\n    print(f'This is {inputs.shape} but should be (8, 2, 2)')\n\n    # Some computation\n    qml.RX(inputs[:, 0], wires=0)\n    qml.RX(inputs[:, 1], wires=1)\n    qml.Rot(*weights_0, wires=0)\n    qml.RY(weight_1, wires=1)\n    qml.CNOT(wires=[0, 1])\n    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n\nweight_shapes = {\"weights_0\": 3, \"weight_1\": 1}\nx = torch.zeros((8, 2, 2))\n\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\noutput = qlayer(x)\n\n# Output:\n# This is torch.Size([16, 2]) but should be (8, 2, 2)\n\nMy question therefore is: how can I have the circuit not flatten the first two dimensions of my input? I\u2019d like to avoid workarounds like reshaping the tensor inside qnode.\nThanks a lot!", "link": "https://discuss.pennylane.ai//t/2d-batching-of-torchlayer-input/3136/7"}}