{"0": {"author": "Qranos", "date": "1615821816863", "content": "Hello I\u2019m currently working on a project where I\u2019m trying to classify arrays with shape (8,60).\nMy basic plan was to use 8 qubits and 60 layers to embed the data (approximately 1Gb). But I run into an issue while doing the backpropagation with pytorch my RAM seems to fill up pretty quickly and that leads to a crash of my processes.\nMy computer is a OMEN (16Gb of RAM, GTX2060, 6Gb VRAM, intel i7 9th gen) It seems like it\u2019s a performance issue of either my computer or the QPU wich is the basic pennylane simulator for now. It seems to be working if I reduce the numbers of layers to 1 but that ruins my model.\nFrom what I understand it seems that when doing the backpropagation the QPU uses the RAM for calculation instead of the GPU wich leads to a crash. Is there a way to run a simulator on the GPU maybe ?\nI\u2019m really lost and my deadline is coming pretty fast. Is there a way to speed up the calculation maybe? Or the technology is not quite there yet ?", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/1"}, "1": {"author": "christina", "date": "1615825705361", "content": "Welcome @Qranos ! Thanks for the question.\nIf memory is an issue, try using diff_method=\"adjoint\" or \"reversible\", see qnode doc 6.  You have a lot of parameters, so this may still take a long time, but these methods won\u2019t take more space as you add layers.1", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/2"}, "2": {"author": "Qranos", "date": "1615880737810", "content": "Thanks @christina, it works fine now but It really does take a long time to train.\nDo you have any idea on how I could accelerate the trainning ?\nI\u2019ve compared different devices already and the pennylane default qubit seems to be the most performant and the only one that accepts the diff_method = \"adjoint\".", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/3"}, "3": {"author": "josh", "date": "1615892328267", "content": "Hi @Qranos!  8 qubits and 60 layers is quite a bit, so I can imagine it does take a while to train. How many parameters do you have in your circuit?\n\nI\u2019ve compared different devices already and the pennylane default qubit seems to be the most performant and the only one that accepts the diff_method = \"adjoint\" .\n\nWhen using default.qubit with the adjoint method, does it lead to a significant speedup in training your model?", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/4"}, "4": {"author": "Qranos", "date": "1615894057632", "content": "Hello @josh!\nThe shape of my parameters was [60, 2 , 8, 3] but I managed tu cut that off to [60, 1, 8] (480 parameters) by going from the Stronglyenteglinglayer to the Basic one and from 2 embedding layers to 1 and that lead to a speed up of ~ x5.\n\nWhen using default.qubit with the adjoint method, does it lead to a significant speedup in training your model?\n\nWhen I did my bench marking to try to see which simulator was the most performant by running my model with a single input, it did not show any kind of improvement.\nHowever when I\u2019m trying to run my training the diff_method = \"adjoint\" is the only thing that\u2019s seems to work without crashing my computer by filling the RAM.\nWhen I tried with diff_method = \"reversible\" it didn\u2019t crashed but nothing happened for 5 minutes so I killed it.\nHere are the results of my benchmarking:\nAs you can see qulacs[\u201ccpu\u201d] seems to perform a little better on my model but does not run when I\u2019m training the model it did nothing for 5min so I killed it too. Also the forest virtual devices seem to freeze and never get an answer maybe I did something wrong during the installation.\n    Encoding size: [10, 8, 60]\n    Number of qubits: 8\n\n    Device: pyQVM NumpyWavefunction Simulator Device\n    Time = 1.753637929999968\n\n    Device: Forest Wavefunction Simulator Device\n    Time = 2.221205955998812\n\n    Device: Default qubit PennyLane plugin\n    Time = 1.4565937029983616\n\n    Device: Qiskit PennyLane plugin\n    Time = 4.764246687998821\n\n    Device: Qulacs device\n    Time = 1.3797734300005686\n\n    Device: Qulacs device\n    Time = 1.0685497309987113\n\n    Device: Cirq Simulator device for PennyLane\n    Time = 2.8849340850010776\n\n    Device: ProjectQ PennyLane plugin\n    Time = 6.023803063999367\n\n    Device: Qiskit PennyLane plugin\n    Time = 9.381965378999666\n", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/5"}, "5": {"author": "josh", "date": "1615895527352", "content": "Wow that is some thorough benchmarking! Thanks for that, it is very useful.\nAre all the benchmarks done using diff_method=\"adjoint\"?\n\nHowever when I\u2019m trying to run my training the diff_method = \"adjoint\" is the only thing that\u2019s seems to work without crashing my computer by filling the RAM.\n\nThis is not surprising \u2014 the adjoint method is a form of backpropagation that takes into account the reversibility of quantum computing, to reduce the amount of memory required ", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/6"}, "6": {"author": "Qranos", "date": "1615901966926", "content": "No the `diff_method=\u201cadjoint\u201d was only used on the default qubit from penny lane it seems to be the only one that supports it.\nDo you think that I\u2019m doing something wrong here ? Because I\u2019m only able to launch my training on the default.qubit  with  diff_method=\"adjoint\" everything else results in a crash.\nOr my model is just too big ?", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/7"}, "7": {"author": "josh", "date": "1615907647153", "content": "I see!\n\nOr my model is just too big ?\n\nThis would be my best guess, especially based on the information that adjoint (the best differentiation method in terms of memory usage) is the only approach where a crash doesn\u2019t occur1", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/8"}, "8": {"author": "Qranos", "date": "1615908094439", "content": "Ok I guess I will just work on reducing my model for now. Thanks for your help it has been a pleasure!\nI might update this thread with my results later on, or If I find tricks to improve it more.", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/9"}, "9": {"author": "josh", "date": "1615914467765", "content": "No worries @Qranos, sorry I couldn\u2019t be of more help!\n\nI might update this thread with my results later on, or If I find tricks to improve it more.\n\nPlease do, I would be interested to hear if you have any interesting tips or results.1", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/10"}, "10": {"author": "Qranos", "date": "1615821816863", "content": "Hello I\u2019m currently working on a project where I\u2019m trying to classify arrays with shape (8,60).\nMy basic plan was to use 8 qubits and 60 layers to embed the data (approximately 1Gb). But I run into an issue while doing the backpropagation with pytorch my RAM seems to fill up pretty quickly and that leads to a crash of my processes.\nMy computer is a OMEN (16Gb of RAM, GTX2060, 6Gb VRAM, intel i7 9th gen) It seems like it\u2019s a performance issue of either my computer or the QPU wich is the basic pennylane simulator for now. It seems to be working if I reduce the numbers of layers to 1 but that ruins my model.\nFrom what I understand it seems that when doing the backpropagation the QPU uses the RAM for calculation instead of the GPU wich leads to a crash. Is there a way to run a simulator on the GPU maybe ?\nI\u2019m really lost and my deadline is coming pretty fast. Is there a way to speed up the calculation maybe? Or the technology is not quite there yet ?", "link": "https://discuss.pennylane.ai//t/pytorch-backpropagation-ram-issue/913/11"}}