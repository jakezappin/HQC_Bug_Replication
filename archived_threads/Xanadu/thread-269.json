{"0": {"author": "RX1", "date": "1669130198898", "content": "def layer(W):\n    for i in range(6):\n        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n    for i in range(4):\n        qml.CNOT(wires=[i, i + 1])\n    qml.CNOT(wires=[5, 0])\n    \ndef statepreparation(x):\n    qml.MottonenStatePreparation(Norm1DArray(x), wires=[i for i in range(6)])\n    \n@qml.qnode(dev1)   \ndef conv_net(weights, x):\n\n    statepreparation(x)\n\n    \n    for W in weights:\n        layer(W)\n\n    return qml.expval(qml.PauliZ(0))\n\n\nopt = qml.NesterovMomentumOptimizer(0.05)\nbatch_size = 5\n\n# train the variational classifier\nweights = para_init\nbias = bias_init\n\nfor it in range(60):\n\n    # Update the weights by one optimizer step\n    batch_index = np.random.randint(0, num_train, (batch_size,))\n    x_train_batch = x_train[batch_index]\n    y_train_batch = y_train[batch_index]\n    weights, bias, _, _ = opt.step(cost, weights, bias, x_train_batch, y_train_batch)\n    print(weights)\n\n    # Compute predictions on train and validation set\n    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in x_train]\n    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in x_test]\n\n    # Compute accuracy on train and validation set\n    acc_train = accuracy(y_train, predictions_train)\n    acc_val = accuracy(y_test, predictions_val)\n\n    print(\n        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n        \"\".format(it + 1, cost(weights, bias, x_train, y_train), acc_train, acc_val)\n    )# cost(para, bias, features, labels):\n\n\nValueError                                Traceback (most recent call last)\nInput In [28], in <cell line: 8>()\n12 x_train_batch = x_train[batch_index]\n13 y_train_batch = y_train[batch_index]\n\u2014> 14 weights, bias, _, _ = opt.step(cost, weights, bias, x_train_batch, y_train_batch)\n15 print(weights)\n17 # Compute predictions on train and validation set\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:88, in GradientDescentOptimizer.step(self, objective_fn, grad_fn, *args, **kwargs)\n70 def step(self, objective_fn, *args, grad_fn=None, **kwargs):\n71     \u201c\u201d\u201cUpdate trainable arguments with one step of the optimizer.\n72\n73     Args:\n(\u2026)\n85         If single arg is provided, list [array] is replaced by array.\n86     \u201c\u201d\u201d\n\u2014> 88     g, _ = self.compute_grad(objective_fn, args, kwargs, grad_fn=grad_fn)\n89     new_args = self.apply_grad(g, args)\n91     # unwrap from list if one argument, cleaner return\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\optimize\\nesterov_momentum.py:71, in NesterovMomentumOptimizer.compute_grad(self, objective_fn, args, kwargs, grad_fn)\n68         shifted_args[index] = args[index] - self.momentum * self.accumulation[index]\n70 g = get_gradient(objective_fn) if grad_fn is None else grad_fn\n\u2014> 71 grad = g(*shifted_args, **kwargs)\n72 forward = getattr(g, \u201cforward\u201d, None)\n74 grad = (grad,) if len(trainable_indices) == 1 else grad\nFile d:\\miniconda3\\lib\\site-packages\\pennylane_grad.py:115, in grad.call(self, *args, **kwargs)\n112     self._forward = self._fun(*args, **kwargs)\n113     return ()\n \u2192 115 grad_value, ans = grad_fn(*args, **kwargs)\n116 self._forward = ans\n118 return grad_value\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\wrap_util.py:20, in unary_to_nary..nary_operator..nary_f(*args, **kwargs)\n18 else:\n19     x = tuple(args[i] for i in argnum)\n\u2014> 20 return unary_operator(unary_f, x, *nary_op_args, **nary_op_kwargs)\nFile d:\\miniconda3\\lib\\site-packages\\pennylane_grad.py:133, in grad._grad_with_forward(fun, x)\n127 @staticmethod\n128 @unary_to_nary\n129 def _grad_with_forward(fun, x):\n130     \u201c\u201d\u201cThis function is a replica of autograd.grad, with the only\n131     difference being that it returns both the gradient and the forward pass\n132     value.\u201d\u201c\u201d\n \u2192 133     vjp, ans = _make_vjp(fun, x)\n135     if not vspace(ans).size == 1:\n136         raise TypeError(\n137             \"Grad only applies to real scalar-output functions. \"\n138             \u201cTry jacobian, elementwise_grad or holomorphic_grad.\u201d\n139         )\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\core.py:10, in make_vjp(fun, x)\n8 def make_vjp(fun, x):\n9     start_node = VJPNode.new_root()\n\u2014> 10     end_value, end_node =  trace(start_node, fun, x)\n11     if end_node is None:\n12         def vjp(g): return vspace(x).zeros()\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\tracer.py:10, in trace(start_node, fun, x)\n8 with trace_stack.new_trace() as t:\n9     start_box = new_box(x, t, start_node)\n\u2014> 10     end_box = fun(start_box)\n11     if isbox(end_box) and end_box._trace == start_box._trace:\n12         return end_box._value, end_box._node\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\wrap_util.py:15, in unary_to_nary..nary_operator..nary_f..unary_f(x)\n13 else:\n14     subargs = subvals(args, zip(argnum, x))\n\u2014> 15 return fun(*subargs, **kwargs)\nInput In [24], in cost(para, bias, features, labels)\n202 def cost(para, bias, features, labels):\n \u2192 203     predictions = [variational_classifier(para, bias, f) for f in features]\n204     return square_loss(labels, predictions)\nInput In [24], in (.0)\n202 def cost(para, bias, features, labels):\n \u2192 203     predictions = [variational_classifier(para, bias, f) for f in features]\n204     return square_loss(labels, predictions)\nInput In [24], in variational_classifier(para, bias, features)\n199 def variational_classifier(para, bias, features):\n \u2192 200     return conv_net(para, features) + bias\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\qnode.py:611, in QNode.call(self, *args, **kwargs)\n608         set_shots(self._original_device, override_shots)(self._update_gradient_fn)()\n610 # construct the tape\n \u2192 611 self.construct(args, kwargs)\n613 cache = self.execute_kwargs.get(\u201ccache\u201d, False)\n614 using_custom_cache = (\n615     hasattr(cache, \u201cgetitem\u201d)\n616     and hasattr(cache, \u201csetitem\u201d)\n617     and hasattr(cache, \u201cdelitem\u201d)\n618 )\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\qnode.py:526, in QNode.construct(self, args, kwargs)\n523 self._tape = qml.tape.QuantumTape()\n525 with self.tape:\n \u2192 526     self._qfunc_output = self.func(*args, **kwargs)\n527 self._tape._qfunc_output = self._qfunc_output\n529 params = self.tape.get_parameters(trainable_only=False)\nInput In [24], in conv_net(weights, x)\n172 @qml.qnode(dev1)\n173 def conv_net(weights, x):\n \u2192 175     statepreparation(x)\n178     for W in weights:\n179         layer(W)\nInput In [24], in statepreparation(x)\n169 def statepreparation(x):\n \u2192 170     qml.MottonenStatePreparation(Norm1DArray(x), wires=[i for i in range(6)])\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py:314, in MottonenStatePreparation.init(self, state_vector, wires, do_queue, id)\n312     norm = qml.math.sum(qml.math.abs(state) ** 2)\n313     if not qml.math.allclose(norm, 1.0, atol=1e-3):\n \u2192 314         raise ValueError(\n315             f\"State vectors have to be of norm 1.0, vector {i} has norm {norm}\"\n316         )\n318 super().init(state_vector, wires=wires, do_queue=do_queue, id=id)\nValueError: State vectors have to be of norm 1.0, vector 0 has norm Autograd ArrayBox with value nan\n@Maria_Schuld\n@isaacdevlugt", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/1"}, "1": {"author": "CatalinaAlbornoz", "date": "1669223350774", "content": "Hi @RX1 ,\nUnfortunately due to non-trivial classical processing of the state vector, the MottonenStatePreparation template is not always fully differentiable.\nI\u2019m not sure if this is the cause of the problem though because I wasn\u2019t able to reproduce your problem since the code you have shared doesn\u2019t include everything that I would need to run it. If you share the rest of your code including the creation of the device, para_init, and other variables I can try to see if I can reproduce your problem.\nI hope this is helpful.\nPlease let me know if you have any additional questions!1", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/2"}, "2": {"author": "RX1", "date": "1669247163573", "content": "So if I convert MottonenStatePreparation to controlled RY and CNOT gates via compute_decomposition and re-express MottonenStatePreparation in terms of basic quantum gates, is this fully differentiable? Like this:\n\n\u56fe\u724711449\u00d7577 202 KB\n\n\nimage1092\u00d7628 82.2 KB\n\n@CatalinaAlbornoz\n@isaacdevlugt", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/3"}, "3": {"author": "CatalinaAlbornoz", "date": "1671832898853", "content": "Hi @RX1 , I still don\u2019t think this can be fully differentiable but you can try.", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/4"}, "4": {"author": "RX1", "date": "1669130198898", "content": "def layer(W):\n    for i in range(6):\n        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n    for i in range(4):\n        qml.CNOT(wires=[i, i + 1])\n    qml.CNOT(wires=[5, 0])\n    \ndef statepreparation(x):\n    qml.MottonenStatePreparation(Norm1DArray(x), wires=[i for i in range(6)])\n    \n@qml.qnode(dev1)   \ndef conv_net(weights, x):\n\n    statepreparation(x)\n\n    \n    for W in weights:\n        layer(W)\n\n    return qml.expval(qml.PauliZ(0))\n\n\nopt = qml.NesterovMomentumOptimizer(0.05)\nbatch_size = 5\n\n# train the variational classifier\nweights = para_init\nbias = bias_init\n\nfor it in range(60):\n\n    # Update the weights by one optimizer step\n    batch_index = np.random.randint(0, num_train, (batch_size,))\n    x_train_batch = x_train[batch_index]\n    y_train_batch = y_train[batch_index]\n    weights, bias, _, _ = opt.step(cost, weights, bias, x_train_batch, y_train_batch)\n    print(weights)\n\n    # Compute predictions on train and validation set\n    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in x_train]\n    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in x_test]\n\n    # Compute accuracy on train and validation set\n    acc_train = accuracy(y_train, predictions_train)\n    acc_val = accuracy(y_test, predictions_val)\n\n    print(\n        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n        \"\".format(it + 1, cost(weights, bias, x_train, y_train), acc_train, acc_val)\n    )# cost(para, bias, features, labels):\n\n\nValueError                                Traceback (most recent call last)\nInput In [28], in <cell line: 8>()\n12 x_train_batch = x_train[batch_index]\n13 y_train_batch = y_train[batch_index]\n\u2014> 14 weights, bias, _, _ = opt.step(cost, weights, bias, x_train_batch, y_train_batch)\n15 print(weights)\n17 # Compute predictions on train and validation set\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:88, in GradientDescentOptimizer.step(self, objective_fn, grad_fn, *args, **kwargs)\n70 def step(self, objective_fn, *args, grad_fn=None, **kwargs):\n71     \u201c\u201d\u201cUpdate trainable arguments with one step of the optimizer.\n72\n73     Args:\n(\u2026)\n85         If single arg is provided, list [array] is replaced by array.\n86     \u201c\u201d\u201d\n\u2014> 88     g, _ = self.compute_grad(objective_fn, args, kwargs, grad_fn=grad_fn)\n89     new_args = self.apply_grad(g, args)\n91     # unwrap from list if one argument, cleaner return\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\optimize\\nesterov_momentum.py:71, in NesterovMomentumOptimizer.compute_grad(self, objective_fn, args, kwargs, grad_fn)\n68         shifted_args[index] = args[index] - self.momentum * self.accumulation[index]\n70 g = get_gradient(objective_fn) if grad_fn is None else grad_fn\n\u2014> 71 grad = g(*shifted_args, **kwargs)\n72 forward = getattr(g, \u201cforward\u201d, None)\n74 grad = (grad,) if len(trainable_indices) == 1 else grad\nFile d:\\miniconda3\\lib\\site-packages\\pennylane_grad.py:115, in grad.call(self, *args, **kwargs)\n112     self._forward = self._fun(*args, **kwargs)\n113     return ()\n \u2192 115 grad_value, ans = grad_fn(*args, **kwargs)\n116 self._forward = ans\n118 return grad_value\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\wrap_util.py:20, in unary_to_nary..nary_operator..nary_f(*args, **kwargs)\n18 else:\n19     x = tuple(args[i] for i in argnum)\n\u2014> 20 return unary_operator(unary_f, x, *nary_op_args, **nary_op_kwargs)\nFile d:\\miniconda3\\lib\\site-packages\\pennylane_grad.py:133, in grad._grad_with_forward(fun, x)\n127 @staticmethod\n128 @unary_to_nary\n129 def _grad_with_forward(fun, x):\n130     \u201c\u201d\u201cThis function is a replica of autograd.grad, with the only\n131     difference being that it returns both the gradient and the forward pass\n132     value.\u201d\u201c\u201d\n \u2192 133     vjp, ans = _make_vjp(fun, x)\n135     if not vspace(ans).size == 1:\n136         raise TypeError(\n137             \"Grad only applies to real scalar-output functions. \"\n138             \u201cTry jacobian, elementwise_grad or holomorphic_grad.\u201d\n139         )\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\core.py:10, in make_vjp(fun, x)\n8 def make_vjp(fun, x):\n9     start_node = VJPNode.new_root()\n\u2014> 10     end_value, end_node =  trace(start_node, fun, x)\n11     if end_node is None:\n12         def vjp(g): return vspace(x).zeros()\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\tracer.py:10, in trace(start_node, fun, x)\n8 with trace_stack.new_trace() as t:\n9     start_box = new_box(x, t, start_node)\n\u2014> 10     end_box = fun(start_box)\n11     if isbox(end_box) and end_box._trace == start_box._trace:\n12         return end_box._value, end_box._node\nFile d:\\miniconda3\\lib\\site-packages\\autograd\\wrap_util.py:15, in unary_to_nary..nary_operator..nary_f..unary_f(x)\n13 else:\n14     subargs = subvals(args, zip(argnum, x))\n\u2014> 15 return fun(*subargs, **kwargs)\nInput In [24], in cost(para, bias, features, labels)\n202 def cost(para, bias, features, labels):\n \u2192 203     predictions = [variational_classifier(para, bias, f) for f in features]\n204     return square_loss(labels, predictions)\nInput In [24], in (.0)\n202 def cost(para, bias, features, labels):\n \u2192 203     predictions = [variational_classifier(para, bias, f) for f in features]\n204     return square_loss(labels, predictions)\nInput In [24], in variational_classifier(para, bias, features)\n199 def variational_classifier(para, bias, features):\n \u2192 200     return conv_net(para, features) + bias\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\qnode.py:611, in QNode.call(self, *args, **kwargs)\n608         set_shots(self._original_device, override_shots)(self._update_gradient_fn)()\n610 # construct the tape\n \u2192 611 self.construct(args, kwargs)\n613 cache = self.execute_kwargs.get(\u201ccache\u201d, False)\n614 using_custom_cache = (\n615     hasattr(cache, \u201cgetitem\u201d)\n616     and hasattr(cache, \u201csetitem\u201d)\n617     and hasattr(cache, \u201cdelitem\u201d)\n618 )\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\qnode.py:526, in QNode.construct(self, args, kwargs)\n523 self._tape = qml.tape.QuantumTape()\n525 with self.tape:\n \u2192 526     self._qfunc_output = self.func(*args, **kwargs)\n527 self._tape._qfunc_output = self._qfunc_output\n529 params = self.tape.get_parameters(trainable_only=False)\nInput In [24], in conv_net(weights, x)\n172 @qml.qnode(dev1)\n173 def conv_net(weights, x):\n \u2192 175     statepreparation(x)\n178     for W in weights:\n179         layer(W)\nInput In [24], in statepreparation(x)\n169 def statepreparation(x):\n \u2192 170     qml.MottonenStatePreparation(Norm1DArray(x), wires=[i for i in range(6)])\nFile d:\\miniconda3\\lib\\site-packages\\pennylane\\templates\\state_preparations\\mottonen.py:314, in MottonenStatePreparation.init(self, state_vector, wires, do_queue, id)\n312     norm = qml.math.sum(qml.math.abs(state) ** 2)\n313     if not qml.math.allclose(norm, 1.0, atol=1e-3):\n \u2192 314         raise ValueError(\n315             f\"State vectors have to be of norm 1.0, vector {i} has norm {norm}\"\n316         )\n318 super().init(state_vector, wires=wires, do_queue=do_queue, id=id)\nValueError: State vectors have to be of norm 1.0, vector 0 has norm Autograd ArrayBox with value nan\n@Maria_Schuld\n@isaacdevlugt", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/5"}, "5": {"author": "RX1", "date": "1669247163573", "content": "So if I convert MottonenStatePreparation to controlled RY and CNOT gates via compute_decomposition and re-express MottonenStatePreparation in terms of basic quantum gates, is this fully differentiable? Like this:\n\n\u56fe\u724711449\u00d7577 202 KB\n\n\nimage1092\u00d7628 82.2 KB\n\n@CatalinaAlbornoz\n@isaacdevlugt", "link": "https://discuss.pennylane.ai//t/optimizing-the-parameters-of-mottonenstatepreparation-produces-the-following-error/2307/6"}}