{"0": {"author": "martin", "date": "1599156019172", "content": "Hi there,\nI am observing a systematic increase in my loss function (square loss) when the parameter shift is big (1.0 or 0.1) but not for smaller parameter shifts. Is there any physical/numerical reason for this?\nHere is a minimal working example, where the only parameterized gate is a displacement gate and the target is the approximation of a single function value f(0.5)=1.0\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10, shots=100) \n\n@qml.qnode(dev)\ndef quantum_neural_net(parameter, x=None):\n# Encode input x into quantum state\nqml.Displacement(x, 0.0, wires=0)\n\nqml.Displacement(parameter, 0.0, wires=0)\n\nreturn qml.expval(qml.X(0))\n\nlr = 0.001\n\ninput = 0.5\ngoal = 1.0\n\n# starting value of displacement parameter\nparameter = -0.1\ncosts = []\nsteps = 500\nfor it in range(steps):\n    # feed forward with the parameter value and calculate loss\n    output = quantum_neural_net(parameter, x=input)\n    loss=(goal - output) ** 2\n    costs.append(loss)  \n\n    # feedforward at shifts of the parameter and calculate partial derivative\n    output_plus = output = quantum_neural_net(parameter+s, x=input)\n    output_minus = output = quantum_neural_net(parameter-s, x=input)\n\n    output_gradient = 1./(2.*s)*(output_plus-output_minus)\n    \n    # calculate gradient of loss with respect to parameter using chain rule\n    gradient = -2*(goal-output)*output_gradient\n    \n    # update parameter with simple gradient descent \n    parameter -= gradient*lr\n\nThis code leads to the following figures for different values of s (loss as a function of steps):\n\n\n\nDoes anyone have an idea of why a big s, leads to this error? Any help would be greatly appreciated!\nKind regards,\nMartin Knudsen", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/1"}, "1": {"author": "theodor", "date": "1599159158456", "content": "Hi @martin,\nIt seems like you\u2019re using finite-differences in your code to calculate the gradient (not to be confused with the parameter shift rule). Since the gradient will be less exact the larger the value of s, the parameters might update incorrectly and thus cause the loss to go up. Using a more exact gradient (i.e. a smaller s) will most likely give more precise gradient-descent steps, thus seemingly working well for s=0.01.\nI hope this helps. ", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/2"}, "2": {"author": "martin", "date": "1599160620584", "content": "Hi @theodor,\nThanks for your answer \nWell, I can see it has exactly the same form as the centered finite difference scheme, but it looks like the parameter shift rule for that gate has that form as well (Schuld). If this is not the case, what is the difference between my version and the parameter shift for the displacement gate?", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/3"}, "3": {"author": "theodor", "date": "1599169573647", "content": "Hi @martin,\nYou are correct that the parameter shift rule for the displacement gate looks exactly like the finite differences rule stated in the paper you linked. I mistakenly thought that you were using finite differences in your code.  After looking into it a bit more, the issue seems to be that you\u2019re using the strawberryfields.fock backend with a too small cutoff_dim value, causing large parameter shifts to fall out of scope, so to say. The higher the value of s, the bigger the cutoff needs to be. You could solve this by either using a larger cutoff or by switching to e.g. the strawberryfields.gaussian backend instead.\nI updated this specific reply since what I wrote before wasn\u2019t strictly correct. Sorry about any confusion I might have caused. I hope this makes sense, though; otherwise, feel free to keep on asking questions.1", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/4"}, "4": {"author": "martin", "date": "1599220710878", "content": "Hi @theodor,\nThanks for the reply, I think I found my mistake and it was a pretty stupid one. I wrote this\noutput_plus = output = quantum_neural_net(parameter+s, x=input)\noutput_minus = output = quantum_neural_net(parameter-s, x=input)\n\nwhich certainly does something fishy, so correcting it to this, I don\u2019t get that weird increase\noutput_plus = quantum_neural_net(parameter+s, x=input)\noutput_minus = quantum_neural_net(parameter-s, x=input)\n\nSorry about that. However, just for my understanding: Could you elaborate on how an increased shift falls out of scope? I imagine, that the parameter shift of the displacement gate has the effect of moving the Wigner distribution in x,p phase space without changing it\u2019s shape. But the cutoff dimensions only controls how many Fock states are allowed in the basis to describe the current CV state. How are these related?", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/5"}, "5": {"author": "Nicolas_Quesada", "date": "1599229660477", "content": "Hi Martin \u2014 A very restricted rule of thumb in Fock space is that if you apply a displacement by alpha on vacuum then your Fock distribution will be Poisson with mean equal to |alpha|^2. This means that you will need a Fock cutoff of at least |\\alpha|^2 + k |\\alpha| where k~2,3 to account for most of the photon number probability.", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/6"}, "6": {"author": "martin", "date": "1599156019172", "content": "Hi there,\nI am observing a systematic increase in my loss function (square loss) when the parameter shift is big (1.0 or 0.1) but not for smaller parameter shifts. Is there any physical/numerical reason for this?\nHere is a minimal working example, where the only parameterized gate is a displacement gate and the target is the approximation of a single function value f(0.5)=1.0\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10, shots=100) \n\n@qml.qnode(dev)\ndef quantum_neural_net(parameter, x=None):\n# Encode input x into quantum state\nqml.Displacement(x, 0.0, wires=0)\n\nqml.Displacement(parameter, 0.0, wires=0)\n\nreturn qml.expval(qml.X(0))\n\nlr = 0.001\n\ninput = 0.5\ngoal = 1.0\n\n# starting value of displacement parameter\nparameter = -0.1\ncosts = []\nsteps = 500\nfor it in range(steps):\n    # feed forward with the parameter value and calculate loss\n    output = quantum_neural_net(parameter, x=input)\n    loss=(goal - output) ** 2\n    costs.append(loss)  \n\n    # feedforward at shifts of the parameter and calculate partial derivative\n    output_plus = output = quantum_neural_net(parameter+s, x=input)\n    output_minus = output = quantum_neural_net(parameter-s, x=input)\n\n    output_gradient = 1./(2.*s)*(output_plus-output_minus)\n    \n    # calculate gradient of loss with respect to parameter using chain rule\n    gradient = -2*(goal-output)*output_gradient\n    \n    # update parameter with simple gradient descent \n    parameter -= gradient*lr\n\nThis code leads to the following figures for different values of s (loss as a function of steps):\n\n\n\nDoes anyone have an idea of why a big s, leads to this error? Any help would be greatly appreciated!\nKind regards,\nMartin Knudsen", "link": "https://discuss.pennylane.ai//t/weird-loss-increase-with-big-parameter-shift/548/7"}}