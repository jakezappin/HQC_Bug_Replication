{"0": {"author": "NikSchet", "date": "1603982855375", "content": "Hello all i am using this code with very nice results\n     nqubits=2\n    device = qml.device('default.qubit', wires=nqubits)\n\n    # Define QNode\n    @qml.qnode(device)\n    def qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(nqubits))\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(nqubits))\n        return [qml.expval(qml.PauliZ(i)) for i in range(nqubits)]\n\n\n    # define weight_shapes\n    weight_shapes = {\"weights\": (n_layers, nqubits, 3)}\n    # Define inputs and qnode trainable weights\n    qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=nqubits)\n    clayer = tf.keras.layers.Dense(output_size)\n    model = tf.keras.Sequential([qlayer, clayer])\n\n    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n    model.compile(opt, loss='binary_crossentropy',metrics=['accuracy','mse', 'mae',\"binary_accuracy\"])\n\nI am wondering if i can just alter the Classical layer so i can have a more options. So can i use something like this for clayer? (note dropout)\nNN = Sequential()\nNN.add(Dense(5, input_dim=d, kernel_initializer='uniform', activation='relu'))\nNN.add(Dropout(rate=dropout_rate))\nNN.add(Dense(2, kernel_initializer='uniform', activation='relu'))\nNN.add(Dropout(rate=dropout_rate))\nNN.add(Dense(2, activation='sigmoid'))\nclayer = NN\n\nmodel = tf.keras.Sequential([qlayer, clayer])\n\nUpon implementation of the code above i get some results but the sketch is having difficulties in converging, maybe it is not running as it was suppose to run?\nIf it is not possible maybe for backprogatation issues how can i implement such a classical layer?\nThank you very much for the support", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/1"}, "1": {"author": "antalszava", "date": "1603998399752", "content": "Hi @NikSchet,\nThanks so much for your question! \nThat sounds odd indeed and it might need further investigation to uncover what exactly is happening here. The new classical layer would not be expected to cause a difference here when using backpropagation.\nOne approach could be trying to interpolate between the clayer defined using NN (including dropout) and the single layer. E.g., tying 2 simple layers and see how it trains, maybe that way it can be uncovered what is causing the difficulty at least (maybe dropout?).\nHope this helps with a bit of direction!", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/2"}, "2": {"author": "NikSchet", "date": "1604061479259", "content": "Thank you very much for your answer. So problem is that classical layer needs too many epochs to converge (1000 epochs,) as a result it makes no sense to try this specific approach because in a quantum processor unit that would require too many processing time. And i guess this is the reason why you have the tranfer learning demo in which you pre-train the classical model and then you apply a quantum node. (any ideas what transformations should i do to that demo to apply my classical neural network?)\nSo what i am trying to do is similar to the transfer learning method.\n\nI pre train only the classical neural network and save it\n\u0399 use the hybrid code  with the saved neural network\n\nThis is like a jump start for the code to converge faster.", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/3"}, "3": {"author": "Tom_Bromley", "date": "1604094975827", "content": "Hi @NikSchet,\nTransfer learning does sound like an exciting approach, and should be promising.\nOne thing I noticed is that your model is a quantum layer followed by a classical layer. This is slightly different to the transfer learning demo 3 we have where we have a classical layer feeding into a quantum one. I don\u2019t think is should be a problem though, although the link to \u201ctransfer learning\u201d as a concept may not be as strong.\nIn terms of getting it to work, it should be a case of:\n\nCreating a purely classical model and training it.\nCreating a second hybrid model composed of the quantum layer and then the second half of the previous model.\nEnsuring that the parameters of the classical part of the hybrid are initialized to match the classical trained model (you could just reuse the layers).\nEnsuring that the classical parameters are not trained, which I believe should be a case of setting the trainable property for each layer.\n3", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/4"}, "4": {"author": "NikSchet", "date": "1604136364996", "content": "\u03a4hank you very much. Actually you did a very good observation the order of layers play a significant role in my code.\nSo if use first the classical pretrain model and then the quantum node you get much much higher accuracy.", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/5"}, "5": {"author": "NikSchet", "date": "1604341418119", "content": "I have one more weird question , so for my hybrid network i use\n`modelh = tf.keras.Sequential([saved_model,qlayer,])\nWhat if i use something repeatedly like this\n`modelh = tf.keras.Sequential([qlayer,qlayer,qlayer,qlayer,])\nWouldn\u2019t that be equivalent to the data-reuploading classifier?!?", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/6"}, "6": {"author": "antalszava", "date": "1604419309684", "content": "Hi @NikSchet,\nOne thing to keep in mind is that both the input and the output of qml.qnn.KerasLayer,  is classical information (e.g., for a vector of data to be encoded we obtain the expectation value of an Hermitian operator, measurement outcomes, etc.). Contrary to this we can consider quantum layers as unitary transformations making up a quantum circuit where the input of the layer is a quantum state and the output is a quantum state as well. In PennyLane such layers are defined in the QNode.\nThe data-reuploading classifier could be expressed using a QNode which is later used with a single qml.qnn.KerasLayer. We have a cost function that uses the output of the QNode (the expectation value of an Hermitian operator that corresponds to the fidelity of two states). In particular note, that the layers mentioned for the data-reuploading classifier are unitaries of the same quantum circuit (and QNode).1", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/7"}, "7": {"author": "kevinkawchak", "date": "1694391815110", "content": "Hello,\n\n\nIs it possible now to implement quantum specific dropout variants in PennyLane on A) layers or B) qubits of parameterized quantum circuits?\n\n\nWill there be a new PQC model to include customizations directly to quantum circuits analogous to advanced deep learning techniques?\n\n\nReferences:\na) Overfitting in quantum machine learning and entangling dropout | SpringerLink 3\nb) [1804.00633] Circuit-centric quantum classifiers 21 Reply", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/8"}, "8": {"author": "CatalinaAlbornoz", "date": "1694477921490", "content": "Hi @kevinkawchak , thank you for your question. Let me check with the team and get back to you.", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/9"}, "9": {"author": "Guillermo_Alonso", "date": "1694532230917", "content": "Hey @kevinkawchak !\nRegarding to the first question: In PennyLane, the first thing you do is define a device with a number of shots. Even if you could randomly choose whether to set certain CNOTs gates, all shots would be executed with respect to the same configuration. If you really want to sometimes place them and sometimes don\u2019t, you would have to run several circuits of one shot and post process the results. In the short term I would say this is the only way.\nCould you give more details about 2?", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/10"}, "10": {"author": "kevinkawchak", "date": "1694555601639", "content": "Hello @Guillermo_Alonso,\nSuch as freezing earlier layers of quantum circuits as training progresses.", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/11"}, "11": {"author": "Guillermo_Alonso", "date": "1694556669679", "content": "In that case, I would take a look at this optimizer 2.\nInstead of freezing the initial layers, you step by step increase the depth of the circuit. I would say the results should be equivalent 2 Replies1", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/12"}, "12": {"author": "kevinkawchak", "date": "1694754561407", "content": "Thank you for the info.", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/13"}, "13": {"author": "kevinkawchak", "date": "1695579043155", "content": "Hello,\nFrom Huynh, L., et al. 2023 \u201cQuantum-Inspired Machine Learning: a Survey\u201d, the authors state:\n\u201cIn the case of QVAS models, comparable performance is often only observed when classical models are deliberately scaled back in terms of architecture size, the number of parameters and/or number of input features.\u201d\na) Is there an anticipated break-even point for reaching a number of quantum algorithm parameters to outperform classical deep neural networks?\nb) Are there current studies where quantum variational algorithms at lower numbers of parameters are the exclusive way to assist deep learning networks for speedups or other improvements? Thank you.\nReference:\n\narxiv.org\n\n\n\n2308.11269.pdf 4\n2038.46 KB\n\n\n\n\n\n", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/14"}, "14": {"author": "CatalinaAlbornoz", "date": "1695758213711", "content": "Hi @kevinkawchak ,\nGreat questions. At the moment classical machine learning is more powerful than quantum machine learning for almost everything. There are some specific cases where quantum computers are more powerful but this is usually only the case for toy problems at the moment.\nThis blog post by Maria Schuld can give you an excellent perspective on this topic.\nAt the moment researchers over the world are looking into understanding quantum machine learning better in order to find potential for advantage. As far as I know there\u2019s no anticipated break-even point. The question itself is not the best because in quantum algorithms don\u2019t necessarily perform better with more parameters, and there are so many other aspects that change in a quantum algorithm that it\u2019s not necessarily a good way to compare the two.\nThat being said, the demo on quantum advantage in learning from experiments 1, based on a paper cited at the beginning of the demo, shows one of these specific examples where quantum algorithms can have an advantage over classical ones.1", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/15"}, "15": {"author": "kevinkawchak", "date": "1695843277225", "content": "Hello,\nDo any of the 6 Numpy quantum specific optimizers act in ways to change the circuit over the course of the run?\nReference:\nhttps://docs.pennylane.ai/en/stable/introduction/interfaces.html 1", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/16"}, "16": {"author": "Guillermo_Alonso", "date": "1696002795273", "content": "Hi @kevinkawchak ! The Adaptive Optimizer 3 modify the circuit during the training.\nIn the case of this optimizer, if you do step_and_cost, 3 values are returned instead of the usual 2. The first one refers to the circuit in that particular iteration.\nI hope that helps!1", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/17"}, "17": {"author": "kevinkawchak", "date": "1696628166955", "content": "Hello @Guillermo_Alonso, is there a ML demo that AdaptiveOptimizer would work good in?", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/18"}, "18": {"author": "Guillermo_Alonso", "date": "1696643930346", "content": "At the moment we only show this feature in this chemistry demo.\nHowever, you could adjust it to another problem of interest in QML ", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/19"}, "19": {"author": "kevinkawchak", "date": "1696741979115", "content": "Hello Guillermo,\nI\u2019m looking to use a specific quantum circuit for one mini-batch, a different quantum circuit for the next mini-batch, and other quantum circuits for the remainder of the ml workflow. The demo and documentation appear to be for finding the best algorithm, but I\u2019m not seeing the tools I would need to use several different circuits systematically throughout a run.1 Reply", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/20"}, "20": {"author": "Ivana_at_Xanadu", "date": "1696926380586", "content": "Can you help me understand what you mean?\nYou want to use different circuits sequentially in the same run? That should be fine. You want to use different circuits for different runs? I\u2019m not sure that would make sense for training.  You want to compare the results for different circuits? Then just have separate runs and that should do it.\nDid I understand what you were asking? ", "link": "https://discuss.pennylane.ai//t/hybrid-quantum-classical-network/647/21"}}